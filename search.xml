<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>辉夜大小姐想让我告白~</title>
    <url>/2021/11/13/2021-11-13-%E8%BE%89-%E5%A4%9C-%E5%A4%A7-%E5%B0%8F-%E5%A7%90-%E6%83%B3-%E8%AE%A9-%E6%88%91-%E5%91%8A-%E7%99%BD/</url>
    <content><![CDATA[<h3 id="总体感受">总体感受</h3>
<h3 id="读漫画过程中的小触动">读漫画过程中的小触动</h3>
<h4 id="大佛">大佛</h4>
<ul>
<li>"会喜欢上做正确的事的人，那是件很理所当然的事。世界上有许多因为时机和状况的原因，而没能把[喜欢]变成恋爱感情的例子。请让我祝你幸福。毕竟并不是只有爱情才是[喜欢]。"</li>
<li><img src="../assets/img/posts/huiye/image-20211117105156460.png" alt="image-20211117105156460" style="zoom:33%;" /></li>
<li><img src="../assets/img/posts/huiye/image-20211117105334372.png" alt="image-20211117105334372" style="zoom:33%;" /></li>
</ul>
<h4 id="辉夜">辉夜</h4>
<ul>
<li>有的人，亲手为喜欢的人做了巧克力，最终，最终她真的送了出去！</li>
</ul>
<h4 id="白银御行">白银御行</h4>
<h4 id="石上优">石上优</h4>
<ul>
<li>有的人，嘴上喊着现充去死去死，最终自己活成了现充的模样</li>
</ul>
<h4 id="藤原千花">藤原千花</h4>
<ul>
<li>有的人，嘴上说着要减肥减肥，最终还是把巧克力当爆米花吃</li>
</ul>
<h4 id="早坂爱">早坂爱</h4>
<h4 id="小弥">小弥</h4>
<ul>
<li>有的人，嘴上强调着送义理巧克力，最终还是送了情人节特别款</li>
</ul>
<h4 id="子安燕">子安燕</h4>
<ul>
<li>为什么喜爱之情跟恋爱之情是不一样的呢?</li>
</ul>
]]></content>
      <tags>
        <tag>二次元desi</tag>
        <tag>辉夜大小姐想让我告白~</tag>
      </tags>
  </entry>
  <entry>
    <title>Rambling Digression</title>
    <url>/2021/11/13/2021-11-13-By-talk/</url>
    <content><![CDATA[<h5 id="section">!!!!</h5>
]]></content>
      <tags>
        <tag>by-talk</tag>
      </tags>
  </entry>
  <entry>
    <title>Math Test</title>
    <url>/2021/11/13/2021-11-17-test-math/</url>
    <content><![CDATA[<p><span class="math inline">\(\frac{3}{4}\)</span></p>
<p><span class="math inline">\(\mathcal{F}\)</span></p>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>Jekyll theme Adam Blog 2.0</title>
    <url>/2021/12/22/2021-11-17-Adam-theme/</url>
    <content><![CDATA[<p><strong>本文的所有内容均搬运自<a href="https://github.com/the-mvm/the-mvm.github.io">the-mvm/the-mvm.github.io: The Minimum Viable Model website and Jekyll theme.</a></strong></p>
<p>demo site now <a href="https://weathered-bread-8229.on.fleek.co/">mirrored</a> in <a href="https://github.com/ipfs/ipfs#quick-summary">IPFS</a>!</p>
<h1 id="jekyll-theme-adam-blog-2.0">Jekyll theme: Adam Blog 2.0</h1>
<p>by <a href="https://github.com/amaynez">Armando Maynez</a> based on <a href="https://github.com/artemsheludko/adam-blog">V1.0</a> by <a href="https://github.com/artemsheludko">Artem Sheludko</a>.</p>
<p>Adam Blog 2.0 is a Jekyll theme that was built to be 100% compatible with <a href="https://pages.github.com/">GitHub Pages</a>. If you are unfamiliar with GitHub Pages, you can check out <a href="https://help.github.com/categories/github-pages-basics/">their documentation</a> for more information. <a href="http://jmcglone.com/guides/github-pages/">Jonathan McGlone's guide</a> on creating and hosting a personal site on GitHub is also a good resource.</p>
<h3 id="what-is-jekyll">What is Jekyll?</h3>
<p>Jekyll is a simple, blog-aware, static site generator for personal, project, or organization sites. Basically, Jekyll takes your page content along with template files and produces a complete website. For more information, visit the <a href="https://jekyllrb.com/docs/home/">official Jekyll site</a> for their documentation. Codecademy also offers a great course on <a href="https://www.codecademy.com/learn/deploy-a-website">how to deploy a Jekyll site</a> for complete beginners.</p>
<span id="more"></span>
<h3 id="never-used-jekyll-before">Never Used Jekyll Before?</h3>
<p>The beauty of hosting your website on GitHub is that you don't have to actually have Jekyll installed on your computer. Everything can be done through the GitHub code editor, with minimal knowledge of how to use Jekyll or the command line. All you have to do is add your posts to the <code>_posts</code> directory and edit the <code>_config.yml</code> file to change the site settings. With some rudimentary knowledge of HTML and CSS, you can even modify the site to your liking. This can all be done through the GitHub code editor, which acts like a content management system (CMS).</p>
<h2 id="features-of-v2.0">Features of v2.0:</h2>
<ul>
<li>SEO meta tags</li>
<li>Dark mode ([configurable in _config.yml file](https://github.com/the-mvm/the-mvm.github.io/blob/a8d4f781bfbc4107b4842433701d28f5bbf1c520/_config.yml#L10))</li>
<li>automatic <a href="http://the-mvm.github.io/sitemap.xml">sitemap.xml</a></li>
<li>automatic <a href="http://the-mvm.github.io/archive/">archive page</a> with infinite scrolling capability</li>
<li><a href="https://the-mvm.github.io/tag/?tag=Coding">new page</a> of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling</li>
<li>click to tweet functionality (just add a <code>&lt;tweet&gt; &lt;/tweet&gt;</code> tag in your markdown.</li>
<li>custom and responsive <a href="https://the-mvm.github.io/404.html">404 page</a></li>
<li>responsive and automatic Table of Contents (optional per post)</li>
<li>read time per post automatically calculated</li>
<li>responsive post tags and social share icons (sticky or inline)</li>
<li>included linkedin, reddit and bandcamp icons</li>
<li><em>copy link to clipboard</em> sharing option (and icon)</li>
<li>view on github link button (optional per post)</li>
<li>MathJax support (optional per post)</li>
<li>tag cloud in the home page</li>
<li>'back to top' button</li>
<li>comments 'courtain' to mask the disqus interface until the user clicks on it ([configurable in _config.yml](https://github.com/the-mvm/the-mvm.github.io/blob/d4a67258912e411b639bf5acd470441c4c219544/_config.yml#L13))</li>
<li><a href="https://github.com/the-mvm/the-mvm.github.io/blob/d4a67258912e411b639bf5acd470441c4c219544/assets/css/main.css#L8">CSS variables</a> to make it easy to customize all colors and fonts</li>
<li>added several themes for code syntax highlight [configurable from the _config.yml file](https://github.com/the-mvm/the-mvm.github.io/blob/e146070e9348c2e8f46cb90e3f0c6eb7b59c041a/_config.yml#L44).</li>
<li>responsive footer menu and footer logo (<a href="https://github.com/the-mvm/the-mvm.github.io/blob/d4a67258912e411b639bf5acd470441c4c219544/_config.yml#L7">if setup in the config file</a>)</li>
<li>search shows results based on full post content, not just the description</li>
<li>smoother menu animations</li>
</ul>
<h2 id="features-preserved-from-v1.0">Features preserved from v1.0</h2>
<ul>
<li><a href="https://fonts.google.com/">Google Fonts</a></li>
<li><a href="http://fontawesome.io/">Font Awesome icons</a></li>
<li><a href="https://disqus.com/">Disqus</a></li>
<li><a href="https://mailchimp.com/">MailChimp</a></li>
<li><a href="https://analytics.google.com/analytics/web/">Analytics</a></li>
<li><a href="https://github.com/christian-fei/Simple-Jekyll-Search">Search</a></li>
</ul>
<h2 id="demo">Demo</h2>
<p><a href="https://the-mvm.github.io/">Check the theme in action</a></p>
<p>The main page looks like this:</p>
<p><img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/homepage-responsive.jpg?raw=true"></p>
<p>Dark mode selector in main menu:</p>
<p><img width="560px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/light-toggle.png?raw=true"></p>
<p>The post page looks like:</p>
<p><img width="540px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/post.jpg?raw=true"> <img width="540px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/post_bottom.jpg?raw=true"></p>
<p>Custom responsive 404:</p>
<p><img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/404-responsive.jpg?raw=true"></p>
<p>Dark mode looks like this:</p>
<p><img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/homepage-dark.png?raw=true"></p>
<p><img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/post-dark.png?raw=true"> <img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/post_bottom-dark.png?raw=true"></p>
<h1 id="installation">Installation</h1>
<h2 id="local-installation">Local Installation</h2>
<p>For a full local installation of Adam Blog 2.0, <a href="https://github.com/the-mvm/the-mvm.github.io/archive/refs/heads/main.zip">download your own copy of Adam Blog 2.0</a> and unzip it into it's own directory. From there, open up your favorite command line tool, enter <code>bundle install</code>, and then enter <code>jekyll serve</code>. Your site should be up and running locally at <a href="http://localhost:4000">http://localhost:4000</a>.</p>
<p>If you're completely new to Jekyll, I recommend checking out the documentation at <a href="https://jekyllrb.com/" class="uri">https://jekyllrb.com/</a> or there's a tutorial by <a href="https://www.smashingmagazine.com/2014/08/build-blog-jekyll-github-pages/">Smashing Magazine</a>.</p>
<p>If you are hosting your site on GitHub Pages, then committing a change to the <code>_config.yml</code> file (or any other file) will force a rebuild of your site with Jekyll. Any changes made should be viewable soon after. If you are hosting your site locally, then you must run <code>jekyll serve</code> again for the changes to take place.</p>
<p>Head over to the <code>_posts</code> directory to view all the posts that are currently on the website, and to see examples of what post files generally look like. You can simply just duplicate the template post and start adding your own content.</p>
<h2 id="github-pages-installation">GitHub Pages Installation</h2>
<h3 id="step-1."><strong>STEP 1.</strong></h3>
<p><a href="https://github.com/the-mvm/the-mvm.github.io/fork/">Fork this repository</a> into your own account.</p>
<h4 id="using-github-pages">Using Github Pages</h4>
<p>You can host your Jekyll site for free with Github Pages. <a href="https://pages.github.com/">Click here</a> for more information.</p>
<p>When forking, if you use as destination a repository named <code>USERNAME.github.io</code> then your url will be <code>https://USERNAME.github.io/</code>, else <code>https://USERNAME.github.io/REPONAME/</code>) and your site will be published to the gh-pages branch. Note: if you are hosting several sites under the same GitHub username, then you will have to use <a href="https://help.github.com/articles/user-organization-and-project-pages/">Project Pages instead of User Pages</a> - just change the repository name to something other than 'http://USERNAME.github.io'.</p>
<h5 id="a-configuration-tweak-if-youre-using-a-gh-pages-branch">A configuration tweak if you're using a gh-pages branch</h5>
<p>In addition to your github-username.github.io repo that maps to the root url, you can serve up sites by using a gh-pages branch for other repos so they're available at github-username.github.io/repo-name.</p>
<p>This will require you to modify the <code>_config.yml</code> like so:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Site settings</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">Repo</span> <span class="string">Name</span></span><br><span class="line"><span class="attr">email:</span> <span class="string">your_email@example.com</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Your</span> <span class="string">Name</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;Repo description&quot;</span></span><br><span class="line"><span class="attr">baseurl:</span> <span class="string">&quot;/repo-name&quot;</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">&quot;https://github-username.github.io&quot;</span></span><br></pre></td></tr></table></figure>
<p>This will ensure that the the correct relative path is constructed for your assets and posts.</p>
<h3 id="step-2."><strong>STEP 2.</strong></h3>
<p>Modify <code>_config.yml</code> file, located in the root directory, with your data.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Site settings</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">The</span> <span class="string">Title</span> <span class="string">for</span> <span class="string">Your</span> <span class="string">Website</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;A description of your blog&#x27;</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">&#x27;:title:output_ext&#x27;</span> <span class="comment"># how the permalinks will behave</span></span><br><span class="line"><span class="attr">baseurl:</span> <span class="string">&quot;/&quot;</span> <span class="comment"># the subpath of your site, e.g. /blog</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">&quot;&quot;</span> <span class="comment"># the base hostname &amp; protocol for your site, e.g. http://example.com</span></span><br><span class="line"><span class="attr">logo:</span> <span class="string">&quot;&quot;</span> <span class="comment"># the logo for your site</span></span><br><span class="line"><span class="attr">logo-icon:</span> <span class="string">&quot;&quot;</span> <span class="comment"># a smaller logo, typically squared</span></span><br><span class="line"><span class="attr">logo-icon-SEO:</span> <span class="string">&quot;&quot;</span> <span class="comment"># must be a non SVG file, could be the same as the logo-icon</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Night/Dark mode default mode is &quot;auto&quot;, &quot;auto&quot; is for auto nightshift (19:00 - 07:00), &quot;manual&quot; is for manual toggle, and &quot;on/off&quot; is for default on/off. Whatever the user&#x27;s choice is, it will supersede the default setting of the site and be kept during the visit (session). Only the dark mode setting is &quot;manual&quot;, it will be always kept on every visit (i.e. no matter the browser is closed or not)</span></span><br><span class="line"><span class="attr">night_mode:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line"><span class="attr">logo-dark:</span> <span class="string">&quot;/assets/img/branding/MVM-logo-full-dark.svg&quot;</span> <span class="comment">#if you want to display a different logo when in dark mode</span></span><br><span class="line"><span class="attr">highlight_theme:</span> <span class="string">syntax-base16.monokai.dark</span> <span class="comment"># select a dark theme for the code highlighter if needed</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Author settings</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Your</span> <span class="string">Name</span> <span class="comment"># add your name</span></span><br><span class="line"><span class="attr">author-pic:</span> <span class="string">&#x27;&#x27;</span> <span class="comment"># a picture of you</span></span><br><span class="line"><span class="attr">about-author:</span> <span class="string">&#x27;&#x27;</span> <span class="comment"># a brief description of you</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Contact links</span></span><br><span class="line"><span class="attr">email:</span> <span class="string">your@email.com</span> <span class="comment"># Add your Email address</span></span><br><span class="line"><span class="attr">phone:</span> <span class="comment"># Add your Phone number</span></span><br><span class="line"><span class="attr">website:</span>  <span class="comment"># Add your website</span></span><br><span class="line"><span class="attr">linkedin:</span>  <span class="comment"># Add your Linkedin handle</span></span><br><span class="line"><span class="attr">github:</span>  <span class="comment"># Add your Github handle</span></span><br><span class="line"><span class="attr">twitter:</span>  <span class="comment"># Add your Twitter handle</span></span><br><span class="line"><span class="attr">bandcamp:</span>  <span class="comment"># Add your Bandcamp username</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tracker</span></span><br><span class="line"><span class="attr">analytics:</span> <span class="comment"># Google Analytics tag ID</span></span><br><span class="line"><span class="attr">fbadmin:</span> <span class="comment"># Facebook ID admin</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Paginate</span></span><br><span class="line"><span class="attr">paginate:</span> <span class="number">6</span> <span class="comment"># number of items to show in the main page</span></span><br><span class="line"><span class="attr">paginate_path:</span> <span class="string">&#x27;page:num&#x27;</span></span><br><span class="line"><span class="attr">words_per_minute:</span> <span class="number">200</span> <span class="comment"># default words per minute to be considered when calculating the read time of the blog posts</span></span><br></pre></td></tr></table></figure>
<h3 id="step-3."><strong>STEP 3.</strong></h3>
<p>To configure the newsletter, please create an account in https://mailchimp.com, set up a web signup form and paste the link from the embed signup form in the <code>config.yml</code> file:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Newsletter</span></span><br><span class="line"><span class="attr">mailchimp:</span> <span class="string">&quot;https://github.us1.list-manage.com/subscribe/post?u=8ece198b3eb260e6838461a60&amp;amp;id=397d90b5f4&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="step-4."><strong>STEP 4.</strong></h3>
<p>To configure Disqus, set up a <a href="https://disqus.com/admin/create/">Disqus site</a> with the same name as your site. Then, in <code>_config.yml</code>, edit the <code>disqus_identifier</code> value to enable.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Disqus</span></span><br><span class="line"><span class="attr">discus_identifier:</span>  <span class="comment"># Add your discus identifier</span></span><br><span class="line"><span class="attr">comments_curtain:</span> <span class="literal">yes</span> <span class="comment"># leave empty to show the disqus embed directly</span></span><br></pre></td></tr></table></figure>
<p>More information on <a href="http://www.perfectlyrandom.org/2014/06/29/adding-disqus-to-your-jekyll-powered-github-pages/">how to set up Disqus</a>.</p>
<h3 id="step-5."><strong>STEP 5.</strong></h3>
<p>Customize the site colors. Modify <code>/assets/css/main.css</code> as follows:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">html</span> &#123;</span><br><span class="line">  --shadow:       <span class="built_in">rgba</span>(<span class="number">32</span>,<span class="number">30</span>,<span class="number">30</span>,.<span class="number">3</span>);</span><br><span class="line">  --accent:       <span class="number">#DB504A</span>;    <span class="comment">/* accent */</span></span><br><span class="line">  --accent-dark:  <span class="number">#4e3e51</span>;    <span class="comment">/* accent 2 (dark) */</span></span><br><span class="line">  --<span class="selector-tag">main</span>:         <span class="number">#326273</span>;    <span class="comment">/* main color */</span></span><br><span class="line">  --<span class="selector-tag">main</span>-dim:     <span class="number">#879dab</span>;    <span class="comment">/* dimmed version of main color */</span></span><br><span class="line">  --text:         <span class="number">#201E1E</span>;</span><br><span class="line">  --grey1:        <span class="number">#5F5E58</span>;</span><br><span class="line">  --grey2:        <span class="number">#8D897C</span>;</span><br><span class="line">  --grey3:        <span class="number">#B4B3A7</span>;</span><br><span class="line">  --grey4:        <span class="number">#DAD7D2</span>;</span><br><span class="line">  --grey5:        <span class="number">#F0EFED</span>;</span><br><span class="line">  --<span class="attribute">background</span>:   <span class="number">#ffffff</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">html</span><span class="selector-attr">[data-theme=<span class="string">&quot;dark&quot;</span>]</span>  &#123;</span><br><span class="line">  --accent:       <span class="number">#d14c47</span>;    <span class="comment">/* accent */</span></span><br><span class="line">  --accent-dark:  <span class="number">#CD8A7A</span>;    <span class="comment">/* accent 2 (dark) */</span></span><br><span class="line">  --<span class="selector-tag">main</span>:         <span class="number">#4C6567</span>;    <span class="comment">/* main color */</span></span><br><span class="line">  --<span class="selector-tag">main</span>-dim:     <span class="number">#273335</span>;    <span class="comment">/* dimmed version of main color */</span></span><br><span class="line">  --text:         <span class="number">#B4B3A7</span>;</span><br><span class="line">  --grey1:        <span class="number">#8D897C</span>;</span><br><span class="line">  --grey2:        <span class="number">#827F73</span>;</span><br><span class="line">  --grey3:        <span class="number">#76746A</span>;</span><br><span class="line">  --grey4:        <span class="number">#66645D</span>;</span><br><span class="line">  --grey5:        <span class="number">#4A4945</span>;</span><br><span class="line">  --<span class="attribute">background</span>:   <span class="number">#201E1E</span>;</span><br><span class="line">  --shadow:       <span class="built_in">rgba</span>(<span class="number">180</span>,<span class="number">179</span>,<span class="number">167</span>,.<span class="number">3</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="step-6."><strong>STEP 6.</strong></h3>
<p>Customize the site fonts. Modify <code>/assets/css/main.css</code> as follows:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">  --font1: <span class="string">&#x27;Lora&#x27;</span>, charter, Georgia, Cambria, <span class="string">&#x27;Times New Roman&#x27;</span>, Times, serif;<span class="comment">/* body text */</span></span><br><span class="line">  --font2: <span class="string">&#x27;Source Sans Pro&#x27;</span>, <span class="string">&#x27;Helvetica Neue&#x27;</span>, Helvetica, Arial, sans-serif; <span class="comment">/* headers and titles   */</span></span><br><span class="line">  --font1-light:      <span class="number">400</span>;</span><br><span class="line">  --font1-regular:    <span class="number">400</span>;</span><br><span class="line">  --font1-bold:       <span class="number">600</span>;</span><br><span class="line">  --font2-light:      <span class="number">200</span>;</span><br><span class="line">  --font2-regular:    <span class="number">400</span>;</span><br><span class="line">  --font2-bold:       <span class="number">700</span>;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>If you change the fonts, you need to also modify <code>/_includes/head.html</code> as follows: Uncomment and change the following line with your new fonts and font weights:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;https://fonts.googleapis.com/css?family=Lora:400,600|Source+Sans+Pro:200,400,700&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Delete everything within <code>&lt;style&gt;&lt;/style&gt;</code> just before the line above:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="css"></span></span><br><span class="line"><span class="css"><span class="comment">/* latin */</span></span></span><br><span class="line"><span class="css"><span class="keyword">@font-face</span> &#123;</span></span><br><span class="line"><span class="css">  <span class="attribute">font-family</span>: <span class="string">&#x27;Lora&#x27;</span>;</span></span><br><span class="line"><span class="css">  ...</span></span><br><span class="line"><span class="css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="step-7."><strong>STEP 7.</strong></h3>
<p>You will find example posts in your <code>/_posts/</code> directory. Go ahead and edit any post and re-build the site to see your changes, for github pages, this happens automatically with every commit. You can rebuild the site in many different ways, but the most common way is to run <code>jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>
<p>To add new posts, simply add a file in the <code>_posts</code> directory that follows the convention of <code>YYYY-MM-DD-name-of-post.md</code> and includes the necessary front matter. Take a look at any sample post to get an idea about how it works. If you already have a website built with Jekyll, simply copy over your posts to migrate to Adam Blog 2.0.</p>
<p>The front matter options for each post are:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">post</span> <span class="comment">#ensure this one stays like this</span></span><br><span class="line"><span class="attr">read_time:</span> <span class="literal">true</span> <span class="comment"># calculate and show read time based on number of words</span></span><br><span class="line"><span class="attr">show_date:</span> <span class="literal">true</span> <span class="comment"># show the date of the post</span></span><br><span class="line"><span class="attr">title:</span>  <span class="string">Your</span> <span class="string">Blog</span> <span class="string">Post</span> <span class="string">Title</span></span><br><span class="line"><span class="attr">date:</span>   <span class="string">XXXX-XX-XX</span> <span class="string">XX:XX:XX</span> <span class="string">XXXX</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;The description of your blog post&quot;</span></span><br><span class="line"><span class="attr">img:</span> <span class="comment"># the path for the hero image, from the image folder (if the image is directly on the image folder, just the filename is needed)</span></span><br><span class="line"><span class="attr">tags:</span> [<span class="string">tags</span>, <span class="string">of</span>, <span class="string">your</span>, <span class="string">post</span>]</span><br><span class="line"><span class="attr">author:</span> <span class="string">Your</span> <span class="string">Name</span></span><br><span class="line"><span class="attr">github:</span> <span class="string">username/reponame/</span> <span class="comment"># set this to show a github button on the post</span></span><br><span class="line"><span class="attr">toc:</span> <span class="literal">yes</span> <span class="comment"># leave empty or erase for no table of contents</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure>
<p>Edit your blogpost using markdown. <a href="https://www.markdownguide.org/">Here is a good guide about how to use it.</a></p>
<h3 id="step-7.-1"><strong>STEP 7.</strong></h3>
<p>Delete images inside of <code>/assets/img/posts/</code> and upload your own images for your posts.</p>
<h3 id="step-8."><strong>STEP 8.</strong></h3>
<p>Make sure Github Pages are turned on in the repository settings, and pointing to the main or master branch (where you cloned this repo).</p>
<h2 id="additional-documentation">Additional documentation</h2>
<h3 id="directory-structure">Directory Structure</h3>
<p>If you are familiar with Jekyll, then the Adam Blog 2.0 directory structure shouldn't be too difficult to navigate. The following some highlights of the differences you might notice between the default directory structure. More information on what these folders and files do can be found in the <a href="https://jekyllrb.com/docs/structure/">Jekyll documentation site</a>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Adam Blog 2.0/</span><br><span class="line">├── _includes                  <span class="comment"># Theme includes</span></span><br><span class="line">├── _layouts                   <span class="comment"># Theme layouts (see below for details)</span></span><br><span class="line">├── _posts                     <span class="comment"># Where all your posts will go</span></span><br><span class="line">├── assets                     <span class="comment"># Style sheets and images are found here</span></span><br><span class="line">|  ├── css                     <span class="comment"># Style sheets go here</span></span><br><span class="line">|  |  └── _sass                <span class="comment"># Folder containing SCSS files</span></span><br><span class="line">|  |  └── main.css             <span class="comment"># Main SCSS file</span></span><br><span class="line">|  |  └── highlighter          <span class="comment"># Style sheet for code syntax highlighting</span></span><br><span class="line">|  └── img                     <span class="comment"># </span></span><br><span class="line">|     └── posts                <span class="comment"># Images go here</span></span><br><span class="line">├── _pages                     <span class="comment"># Website pages (that are not posts)</span></span><br><span class="line">├── _config.yml                <span class="comment"># Site settings</span></span><br><span class="line">├── Gemfile                    <span class="comment"># Ruby Gemfile for managing Jekyll plugins</span></span><br><span class="line">├── index.html                 <span class="comment"># Home page</span></span><br><span class="line">├── LICENSE.md                 <span class="comment"># License for this theme</span></span><br><span class="line">├── README.md                  <span class="comment"># Includes all of the documentation for this theme</span></span><br><span class="line">├── feed.xml                   <span class="comment"># Generates atom file which Jekyll points to</span></span><br><span class="line">├── 404.html                   <span class="comment"># custom and responsive 404 page</span></span><br><span class="line">├── all-posts.json             <span class="comment"># database of all posts used for infinite scroll</span></span><br><span class="line">├── ipfs-404.html              <span class="comment"># 404 page for IPFS</span></span><br><span class="line">├── posts-by-tag.json          <span class="comment"># database of posts by tag</span></span><br><span class="line">├── robots.txt                 <span class="comment"># SEO crawlers exclusion file</span></span><br><span class="line">├── search.json                <span class="comment"># database of posts used for search</span></span><br><span class="line">└── sitemap.xml                <span class="comment"># automatically generated sitemap for search engines</span></span><br></pre></td></tr></table></figure>
<h3 id="starting-from-scratch">Starting From Scratch</h3>
<p>To completely start from scratch, simply delete all the files in the <code>_posts</code>, <code>assets/img/posts</code> folders, and add your own content. Everything in the <code>_config.yml</code> file can be edited to suit your needs. Also change the <code>favicon.ico</code> file to your own favicon.</p>
<h3 id="click-to-tweet">Click to tweet</h3>
<p>If you have a tweetable quote in your blog post and wish to feature it as a click to tweet block, you just have to use the <code>&lt;tweet&gt;&lt;/tweet&gt;</code> tags, everything between them will be converted in a click to tweet box.</p>
<p><img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/ctt-markdown.png?raw=true"></p>
<p><img width="640px" src="https://github.com/the-mvm/the-mvm.github.io/blob/main/assets/img/template_screenshots/ctt-render.png?raw=true"></p>
<h3 id="google-analytics">Google Analytics</h3>
<p>It is possible to track your site statistics through <a href="https://www.google.com/analytics/">Google Analytics</a>. Similar to Disqus, you will have to create an account for Google Analytics, and enter the correct Google ID for your site under <code>google-ID</code> in the <code>_config.yml</code> file. More information on <a href="https://michaelsoolee.com/google-analytics-jekyll/">how to set up Google Analytics</a>.</p>
<h3 id="atom-feed">Atom Feed</h3>
<p>Atom is supported by default through <a href="https://github.com/jekyll/jekyll-feed">jekyll-feed</a>. With jekyll-feed, you can set configuration variables such as 'title', 'description', and 'author', in the <code>_config.yml</code> file.</p>
<p>Your atom feed file will be live at <code>https://your.site/feed.xml</code> <a href="https://the-mvm.github.io/feed.xml">example</a>.</p>
<h3 id="social-media-icons">Social Media Icons</h3>
<p>All social media icons are courtesy of <a href="http://fontawesome.io/">Font Awesome</a>. You can change which icons appear, as well as the account that they link to, in the <code>_config.yml</code> file.</p>
<h3 id="mathjax">MathJax</h3>
<p>Adam Blog 2.0 comes out of the box with <a href="https://www.mathjax.org/">MathJax</a>, which allows you to display mathematical equations in your posts through the use of <a href="http://www.andy-roberts.net/writing/latex/mathematics_1">LaTeX</a>. Just add <code>Mathjax: yes</code> in the frontmatter of your post.</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;text-align:center&quot;</span>&gt;</span></span></span><br><span class="line">\(\theta<span class="emphasis">_&#123;t+1&#125; = \theta_</span>&#123;t&#125; - \dfrac&#123;\eta&#125;&#123;\sqrt&#123;\hat&#123;v&#125;<span class="emphasis">_t&#125; + \epsilon&#125; \hat&#123;m&#125;_</span>t\).</span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<figure>
<img src="../../../assets/img/template_screenshots/MathjaxRendered.jpg" alt="rendered mathjax" /><figcaption aria-hidden="true">rendered mathjax</figcaption>
</figure>
<h3 id="syntax-highlighting">Syntax Highlighting</h3>
<p>Adam Blog 2.0 provides syntax highlighting through <a href="https://help.github.com/articles/creating-and-highlighting-code-blocks/">fenced code blocks</a>. Syntax highlighting allows you to display source code in different colors and fonts depending on what programming language is being displayed. You can find the full list of supported programming languages <a href="https://github.com/jneen/rouge/wiki/List-of-supported-languages-and-lexers">here</a>. Another option is to embed your code through <a href="https://en.support.wordpress.com/gist/">Gist</a>.</p>
<p>You can choose the color theme for the syntax highlight in the <code>_config.yml</code> file:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">highlight_theme:</span> <span class="string">syntax-base16.monokai.dark</span> <span class="comment"># select a theme for the code highlighter</span></span><br></pre></td></tr></table></figure>
<p>See the <a href="https://github.com/the-mvm/the-mvm.github.io/tree/main/assets/css/highlighter">highlighter directory</a> for reference on the options.</p>
<h3 id="markdown">Markdown</h3>
<p>Jekyll offers support for GitHub Flavored Markdown, which allows you to format your posts using the <a href="https://guides.github.com/features/mastering-markdown/">Markdown syntax</a>.</p>
<h2 id="everything-else">Everything Else</h2>
<p>Check out the <a href="http://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll's GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>
<h2 id="contributing">Contributing</h2>
<p>If you would like to make a feature request, or report a bug or typo in the documentation, then please <a href="https://github.com/the-mvm/the-mvm.github.io/issues/new">submit a GitHub issue</a>. If you would like to make a contribution, then feel free to <a href="https://help.github.com/articles/about-pull-requests/">submit a pull request</a> - as a bonus, I will credit all contributors below! If this is your first pull request, it may be helpful to read up on the <a href="https://guides.github.com/introduction/flow/">GitHub Flow</a> first.</p>
<p>Adam Blog 2.0 has been designed as a base for users to customize and fit to their own unique needs. Please keep this in mind when requesting features and/or submitting pull requests. Some examples of changes that I would love to see are things that would make the site easier to use, or better ways of doing things. Please avoid changes that do not benefit the majority of users.</p>
<h2 id="questions">Questions?</h2>
<p>This theme is completely free and open source software. You may use it however you want, as it is distributed under the <a href="http://choosealicense.com/licenses/mit/">MIT License</a>. If you are having any problems, any questions or suggestions, feel free to <a href="https://github.com/the-mvm/the-mvm.github.io/issues/new">file a GitHub issue</a>.</p>
]]></content>
  </entry>
  <entry>
    <title>解决Mathjax与Markdown之间的冲突问题</title>
    <url>/2021/12/22/2021-11-19-Mathjax-Markdown/</url>
    <content><![CDATA[<p>由于最近刚刚开始搭建个人blog中遇到了“<strong>明明启用了Mathjax,但是markdown文档在上传之后仍然无法正确显示行间公式或者行内公式</strong>”的问题，因此，折腾了好一会之后，得到了一个个人的修复方案。</p>
<span id="more"></span>
<h2 id="blog搭建结构">Blog搭建结构</h2>
<p>Github自带的基于Jekyll的Github pages搭建方案。</p>
<h2 id="问题原因">问题原因</h2>
<p>举个例子来说，在 Markdown 中，下划线 _ 被保留，那么，当Markdown 在 MathJax 之前处理文档的时候，就会将下划线 _ 转为一个HTML tag&lt;i&gt;，导致MathJax无法正确识别公式。</p>
<h2 id="解决方案">解决方案</h2>
<ul>
<li><p>修改xx.github.io/**_config.yml** 中的markdown解析器为kramdown:</p>
<p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">markdown: kramdown</span><br></pre></td></tr></table></figure></p></li>
<li><p>在xx.github.io/**_include/header.html**中添加两行处理代码:</p>
<p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config(&#123;tex2jax: &#123;inlineMath: [[&#x27;$&#x27;,&#x27;$&#x27;], [&#x27;\\(&#x27;,&#x27;\\)&#x27;]]&#125;&#125;);&lt;/script&gt;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; async src=&quot;//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p>
<p>直接放在<code>&lt;header class="main-header"&gt;</code>后两行即可。</p></li>
<li><p>这样以后，用$...$写行内公式，用$$...$$写行间公式应该都没有问题了。</p></li>
</ul>
]]></content>
      <tags>
        <tag>Problem</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224n Assignment2</title>
    <url>/2021/12/22/2021-11-19-Record-CS224n-a2/</url>
    <content><![CDATA[<h3 id="前情提要-the-word2vec-skip-gram-prediction-model">前情提要 : The word2vec skip-gram prediction model</h3>
<p>这里对handout_Assignment2.Understanding word2vec进行一个个人梳理。</p>
<ul>
<li><p>word2vec诞生的idea在于，我们有一个“每个word都是通过它周围的words组成一个company，我们才能够理解the meaning”的想法而诞生的。</p></li>
<li><p>因此，在skip-gram word2vec的想法下，一个能表达词语意思的结构由一个中心词(center word) <span class="math inline">\(c\)</span>, 跟它周围的词语(限定在一个contextual window)组成。对于处在contextual window中的一个特定的词语(specific word)用符号<span class="math inline">\(o\)</span> 记。</p></li>
<li><figure>
<img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202111191258141.png" alt="image-20211119125840081" /><figcaption aria-hidden="true">image-20211119125840081</figcaption>
</figure>
<p>在The word2vec skip-gram prediction model中的contextual window 的一个例子如上，对于一个center word banking,其contextual window 中的词为turing,into,crises,as。</p></li>
</ul>
<span id="more"></span>
<ul>
<li><p>在word2vec中，我们想要计算的条件概率采用如下形式进行求解: <span class="math display">\[
  P(O = o | C = c) =\frac{\exp(\mathbf{u_o^T} \mathbf{v_c})}{\sum_{\omega \in Vocabulary Set} \exp(\mathbf{u_\omega^T} \mathbf{v_c})}
  \]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{u_o}\)</span> :表示的是一个outside vector,其代表了一个特定的outside word <span class="math inline">\(o\)</span></li>
<li><span class="math inline">\(\mathbf{v_c}\)</span> : 与<span class="math inline">\(\mathbf{u_o}\)</span>类似的，它表示的是一个center vector,代表了我们选定的一个中心词<span class="math inline">\(c\)</span></li>
<li>在实际应用中，通常都以矩阵形式<span class="math inline">\(\mathbf{U},\mathbf{V}\)</span>存储这两个参数。一般来说，这两个矩阵的每一列都记录了一个词<span class="math inline">\(w\)</span>在处于不同角色，即outside word或者center word下的不同向量表示。</li>
</ul></li>
<li><p>最naive的loss定义如下:对于一个特定的pair<span class="math inline">\((c,o)\)</span> : <span class="math display">\[
  \mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U}) = -\log P(O = o | C = c)
  \]</span></p>
<ul>
<li><p>注意到，在下面的书面题中，我们将会证明其与<span class="math inline">\(\mathbf{y}\)</span>与<span class="math inline">\(\mathbf{\hat{y}}\)</span>之间的交叉熵损失是一致的。</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y}\)</span>: the ground truth,真实的标签。即一个one-hot编码,除特定的outside word<span class="math inline">\(o\)</span>下标处为1之外，均0.</p></li>
<li><p><span class="math inline">\(\mathbf{\hat{y}}\)</span>: 由之前定义的概率分布<span class="math inline">\(P(O = o | C = c)\)</span>给出的prediction</p></li>
<li><p>交叉熵损失(Cross Entropy) : <span class="math display">\[
  -\sum_{i}\mathbf{y_i} \log(\mathbf{\hat{y_i}})
  \]</span></p></li>
</ul></li>
</ul></li>
</ul>
<h3 id="前情提要2-negative-sampling-loss">前情提要2 ： Negative Sampling Loss</h3>
<p>Negative Sampling的大概想法:</p>
<ul>
<li><p>从vocabulary中采样出<span class="math inline">\(K\)</span>个negative samples(即words),<span class="math inline">\(w_1,w_2,...,w_k\)</span>,其相对应地就有outside vectors<span class="math inline">\(\mathbf{u_1},\mathbf{u_2},...,\mathbf{u_K}\)</span> 。定义negative sampling loss function如下:</p>
<ul>
<li><p><span class="math display">\[
  \mathbf{J}_{neg-sample}(\mathbf{v_c},o,\mathbf{U}) = \\
  -\log(\sigma(\mathbf{u_o^T} \mathbf{v_c})) - \sum_{k=1}^{K} \log(\sigma(\mathbf{-u_k^T} \mathbf{v_c}))
  \]</span></p>
<ul>
<li><span class="math inline">\(\sigma(\cdot)\)</span>可以是sigmoid 函数</li>
<li><span class="math inline">\(o\)</span>不在<span class="math inline">\(K\)</span>个negative samples当中</li>
</ul></li>
</ul></li>
<li><p>Negative Sampling loss由于每次只要计算<span class="math inline">\(K\)</span>个negative samples的相关信息，效率会比naive-softmax高很多。</p></li>
</ul>
<h3 id="前情提要3-skip-gram-loss">前情提要3 ： Skip-gram Loss</h3>
<p>由之前的介绍，我们就可以整理出skip-gram的大致想法:</p>
<ul>
<li><p>对于一个中心词<span class="math inline">\(c = w_t\)</span> ,其context window在window size为<span class="math inline">\(m\)</span>的情况下是<span class="math inline">\([w_{t-m},...,w_{t-1},w_{t},w_{t+1},...,w_{t+m}]\)</span> 。定义context window的total loss：</p>
<ul>
<li><p><span class="math display">\[
  \mathbf{J}_{skip-gram}(\mathbf{v_c},w_{t-m},...,w_{t+m},\mathbf{U}) = \\
  \sum_{-m\le j\le m,j\not=0} \mathbf{J}(\mathbf{v_c},w_{t+j},\mathbf{U})
  \]</span></p></li>
<li><p><span class="math inline">\(\mathbf{J}(\mathbf{v_c},w_{t+j},\mathbf{U})\)</span>可以是任意的一个loss function。</p></li>
</ul></li>
</ul>
<h3 id="计算">计算</h3>
<h4 id="在此之前">在此之前</h4>
<p>对于所有的推导，我们都应该尽可能地遵守shape convention,具体而言，任意一个函数<span class="math inline">\(f(x)\)</span>的偏导的shape都应该跟<span class="math inline">\(x\)</span>的shape保持一致。</p>
<ol type="a">
<li>Show that the naive-softmax loss is the same as the cross-entropy loss between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span>; i.e., show that: <span class="math display">\[
-\sum_{\omega \in Vocab} y_{\omega} \log (\hat{y_{\omega}}) = -\log (\hat{y_{o}})
\]</span></li>
</ol>
<ul>
<li>这里注意到<span class="math inline">\(y_{\omega}\)</span>是one-hot编码形式的，因此容易得到: <span class="math display">\[
  -\sum_{\omega \in Vocab} y_{\omega} \log (\hat{y_{\omega}}) = y_{o} \log (\hat{y_{o}}) = \log (\hat{y_{o}})
  \]</span></li>
</ul>
<ol start="2" type="a">
<li>Compute the partial derivative of <span class="math inline">\(\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})\)</span> with respect to <span class="math inline">\(\mathbf{v_c}\)</span> :</li>
</ol>
<ul>
<li><span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{v_c}} = \\ \frac{\exp(\mathbf{u_o^T}\mathbf{v_c})\mathbf{u_o}\sum_{\omega \in VS} \exp(\mathbf{u_\omega^T} \mathbf{v_c}) - \exp(\mathbf{u_o^T}\mathbf{v_c})\sum_{\omega \in VS} \exp(\mathbf{u_\omega^T} \mathbf{v_c}) \mathbf{u_\omega}}{-P(O = o | C = c) (\sum_{\omega \in VS} \exp(\mathbf{u_\omega^T} \mathbf{v_c}))^2} \\
  = - \mathbf{u_o} + \sum_{\omega \in VS} \frac{\exp(\mathbf{u_\omega^T} \mathbf{v_c}) \mathbf{u_\omega}}{\sum_{\omega \in VS} \exp(\mathbf{u_\omega^T} \mathbf{v_c})} \\
  = - \mathbf{u_o} + \sum_{\omega \in VS} P(O = \omega | C = c) \mathbf{u_\omega} \\
  = -\mathbf{U}^T \mathbf{y}+\mathbf{U}^T\mathbf{\hat{y}} \\
  = \mathbf{U}^T (\mathbf{\hat{y}} - \mathbf{y})
  \]</span></li>
</ul>
<p>(c),(d)Compute the partial derivatives of <span class="math inline">\(\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})\)</span> with respect to each of the ‘outside’ word vectors, <span class="math inline">\(\mathbf{u_w}\)</span>’s <span class="math inline">\(\mathbf{U}\)</span>’s</p>
<ul>
<li><p>注意到原式<span class="math inline">\(P(O = o | C = c)\)</span>中<span class="math inline">\(\mathbf{u_w},\mathbf{v_c}\)</span>是存在着一定对称性的，对<span class="math inline">\(\mathbf{u_w}\)</span>求偏导的形式与对<span class="math inline">\(\mathbf{v_c}\)</span>求偏导的形式基本一致,因此可以简单交换符号后得到:</p>
<ul>
<li><p><span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{U}} = (\mathbf{\hat{y}} - \mathbf{y})^T \mathbf{v_c}
  \]</span></p></li>
<li><p><span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{u_w}} = P(O=w|C=c) \mathbf{v_c}
  \]</span></p></li>
<li><p><span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{u_w}} = (P(O=o|C=c)-1) \mathbf{v_c}
  \]</span></p></li>
</ul></li>
</ul>
<p>(e)Please compute the derivative of <span class="math inline">\(σ(x)\)</span> with respect to x, where x is a scalar: <span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<ul>
<li>sigmoid,softmax的导数均为<span class="math inline">\(\sigma(x)(1-\sigma(x))\)</span></li>
</ul>
<p>(f)for negtive sampling loss,repeat (b)(c):</p>
<ul>
<li><p>利用sigmoid函数的性质，容易得到: <span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{v_c}} = \\
  (\sigma(\mathbf{u_o^T} \mathbf{v_c}) - 1)\mathbf{u_o} + \sum_{k=1}^{K}(1-\sigma(-\mathbf{u_k^T} \mathbf{v_c}))\mathbf{u_k}
  \]</span></p>
<p><span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{u_o}} = (\sigma(\mathbf{u_o^T} \mathbf{v_c}) - 1)\mathbf{v_c}
  \]</span></p>
<p><span class="math display">\[
  \frac{\partial\mathbf{J}_{naive-softmax}(\mathbf{v_c},o,\mathbf{U})}{\partial \mathbf{u_k}} = (1-\sigma(\mathbf{-u_k^T} \mathbf{v_c}) )\mathbf{v_c}
  \]</span></p></li>
</ul>
<p>(h)for skip-gram loss,repeat (b)(c):</p>
<ul>
<li><p><span class="math display">\[
  \frac{\partial \mathbf{J}_{skip-gram}(\mathbf{v_c},w_{t-m},...,w_{t+m},\mathbf{U})}{\partial \mathbf{v_c}} = \\
  \sum_{-m\le j\le m,j\not=0} \frac{\partial \mathbf{J}(\mathbf{v_c},w_{t+j},\mathbf{U})}{\partial \mathbf{v_c}}
  \]</span></p></li>
<li><p><span class="math display">\[
  \frac{\partial \mathbf{J}_{skip-gram}(\mathbf{v_c},w_{t-m},...,w_{t+m},\mathbf{U})}{\partial \mathbf{v_w}} = 0\\
  \]</span></p></li>
<li><p><span class="math display">\[
  \frac{\partial \mathbf{J}_{skip-gram}(\mathbf{v_c},w_{t-m},...,w_{t+m},\mathbf{U})}{\partial \mathbf{U}} = \\
  \sum_{-m\le j\le m,j\not=0} \frac{\partial \mathbf{J}(\mathbf{v_c},w_{t+j},\mathbf{U})}{\partial \mathbf{U}}
  \]</span></p></li>
</ul>
<h3 id="实验中的新知">实验中的新知</h3>
<ul>
<li>np.allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False)
<ul>
<li>equal_nan控制是否判断相同位置的NaN是相等的。</li>
<li>根据以下式子判断是否return True。
<ul>
<li>absolute(a - b) &lt;= (atol+ rtol * absolute(b))</li>
</ul></li>
<li>a ,b 地位不同，因此np.allclose(a,b)与np.allclose(b,a)不一定相同。</li>
</ul></li>
</ul>
]]></content>
      <tags>
        <tag>CS224n</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>chap.16 强化学习</title>
    <url>/2021/12/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h4 id="强化学习">强化学习</h4>
<ul>
<li><p>强化学习的过程概括些说就是做一个动作action，环境给予你一个反馈r:</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221600231.png" alt="image-20211222160057193" style="zoom:50%;" /></p></li>
<li><p>强化学习任务通常用马尔可夫决策过程(MDP)来描述：</p>
<ul>
<li><p>对于一个环境<span class="math inline">\(E\)</span>,其由状态空间<span class="math inline">\(X\)</span>,动作空间<span class="math inline">\(A\)</span>,状态转移概率<span class="math inline">\(P\)</span>以及奖励函数<span class="math inline">\(R\)</span>组成，通常的P,R函数形式如下：<span class="math inline">\(P: X\times A\times X -&gt; \mathbb{R}\)</span> 指定一个概率,<span class="math inline">\(R: X\times A\times X -&gt; \mathbb{R}\)</span> 指定一个奖赏。</p></li>
<li><p>因此，强化学习的内容就是让机器通过在环境中不断尝试从而学到一个策略<span class="math inline">\(\pi\)</span> ,根据这个策略本<span class="math inline">\(\pi\)</span> ,我们可以确定在状态<span class="math inline">\(x\)</span>下要执行的动作<span class="math inline">\(a = \pi(x)\)</span> ,并且使得长期累积的奖赏可以最大化。</p></li>
<li><p>常用的长期累计奖赏的计算方式有：</p>
<ul>
<li><p>T步累计奖赏 ： <span class="math inline">\(\mathbb{E}[\frac{1}{T} \sum_{t=1}^T r_t]\)</span></p></li>
<li><p><span class="math inline">\(\gamma\)</span>折扣累计奖赏 ： <span class="math inline">\(\mathbb{E}[ \sum_{t=0}^{+\infty} \gamma^t r_{t}]\)</span></p></li>
<li><p>马尔可夫回报过程（MRP）计算的就是： <span class="math display">\[
  V(x) = \sum_{x^\prime\in X} P(x^\prime|x) (R(x^\prime) + V(x^\prime))
  \]</span> 描述的就是从一个状态<span class="math inline">\(x\)</span>转移到其他所有状态<span class="math inline">\(x^\prime\)</span>之后的累计回报。</p></li>
</ul></li>
</ul></li>
</ul>
<span id="more"></span>
<h5 id="k-摇臂赌博机">K-摇臂赌博机</h5>
<ul>
<li>强化学习任务需要在多步动作之后，才能看到最终奖赏，考虑简单情况，就是最大化单步奖赏。要最大化单步奖赏，我们不仅要知道每个动作带来的奖赏，还要执行奖赏最大的动作。但是动作的奖赏通常都是取自一个概率分布，无法通过单次尝试获得平均奖赏值。</li>
<li>K-摇臂赌博机描述了单步强化学习任务的过程：K-摇臂赌博机有k个摇臂，投入一个硬币可以按下一个摇臂，每个摇臂以一定概率（不可知）吐出硬币。</li>
<li>对于这种情况，一般有两种策略：
<ul>
<li>探索——估计摇臂的优劣：即平均给每个摇臂投硬币，得到概率的近似解</li>
<li>利用——选择当前最优的摇臂。</li>
<li>由于总尝试次数有限，因此，侧重于任何一方都会削弱另外一方。这也就是强化学习面临的“探索-利用窘境”。想要达到最大的累计奖赏，就必须在探索与利用之间达成较好的折中。</li>
</ul></li>
</ul>
<h5 id="epsilon贪心策略"><span class="math inline">\(\epsilon\)</span>贪心策略</h5>
<ul>
<li><p><span class="math inline">\(\epsilon\)</span>贪心法基于一个概率来对探索以及利用过程进行折中：</p>
<ul>
<li><p>每次尝试时，以<span class="math inline">\(\epsilon\)</span>概率进行探索，选择一个摇臂；以<span class="math inline">\(1-\epsilon\)</span> 的概率选择当前平均奖赏最高的摇臂（多个采取随机选择）。</p></li>
<li><p>则平均奖赏可以记录为 <span class="math display">\[
  Q(k) = \frac{1}{n} \sum_{i=1}^n v_i
  \]</span> <span class="math inline">\(v_i\)</span>为每次返回的奖赏,需要记录n个奖赏值。</p></li>
<li><p>一种更为高效的办法是对均值进行增量式计算，即每尝试一次就更新<span class="math inline">\(Q\)</span>,设定用下标记录尝试次数,<span class="math inline">\(Q_i\)</span>表示i次尝试之后的平均奖赏，更新公式如下: <span class="math display">\[
  Q_n(k) = \frac{1}{n} ((n-1)\times Q_{n-1}(k) + v_n)
  \]</span> 也就只需要记录每个摇臂的尝试次数n，以及平均奖赏<span class="math inline">\(Q_i(k)\)</span></p></li>
<li><p>其中，通过调整<span class="math inline">\(\epsilon\)</span>的大小，可以调整探索以及利用的比率。当迭代较多次之后，可以考虑增大利用的比率。</p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221700300.png" alt="image-20211222170052246" style="zoom:50%;" /></p></li>
</ul></li>
</ul>
<h5 id="softmax算法">Softmax算法</h5>
<ul>
<li><p>Softmax算法基于已知的摇臂平均奖赏来对探索以及利用进行折中，通过摇臂平均奖赏计算摇臂被选取的概率(基于Boltzmann分布)，且正相关： <span class="math display">\[
  P(k) = \frac{e^{\frac{Q(k)}{\tau}}}{\sum_{i=1}^K e^{\frac{Q(i)}{\tau}}}
  \]</span> 其中<span class="math inline">\(\tau\)</span>是可以自设定的参数，越高则平均奖赏越高的摇臂被选取的概率越高，也就是越趋向于“仅利用”。</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221700202.png" alt="image-20211222170036149" style="zoom:50%;" /></p></li>
<li><p>Softmax算法以及<span class="math inline">\(\epsilon\)</span>贪心法孰优孰劣看具体应用。</p></li>
</ul>
<h5 id="有模型学习">有模型学习</h5>
<ul>
<li>有模型学习描述的是指马尔可夫决策过程总环境<span class="math inline">\(E=&lt;X,A,P,R&gt;\)</span>已知的情况,也就是说一个状态<span class="math inline">\(x\)</span>经过动作<span class="math inline">\(a\)</span>转移到<span class="math inline">\(x^\prime\)</span>的概率<span class="math inline">\(P_a\)</span>,以及reward<span class="math inline">\(R_a\)</span>都是已知的。</li>
</ul>
<h6 id="策略评估">策略评估</h6>
<ul>
<li><p>在模型已知的情况下，对于任意的策略<span class="math inline">\(\pi\)</span>,都可以估计出该策略带来的期望累计奖赏。</p></li>
<li><p>定义一些函数如下：</p>
<ul>
<li><span class="math inline">\(V^{\pi}(x)\)</span>表示从状态<span class="math inline">\(x\)</span>出发采用策略<span class="math inline">\(\pi\)</span>带来的累计奖赏
<ul>
<li><span class="math inline">\(V(\cdot)\)</span>是状态值函数</li>
</ul></li>
<li><span class="math inline">\(Q^{\pi}(x,a)\)</span>表示从状态出发，执行动作a之后再使用策略<span class="math inline">\(\pi\)</span>的累计奖赏
<ul>
<li><span class="math inline">\(Q(\cdot)\)</span>是一个状态-动作值函数</li>
</ul></li>
<li>累计奖赏都用T步累计奖赏、<span class="math inline">\(\gamma\)</span>折扣累计奖赏进行计算。</li>
</ul></li>
<li><p>由于MDP具有马尔可夫性质，因此系统下一时刻的状态仅有当前的状态决定，不依赖任何以往的状态，也就是说，我们可以将上述函数写成递归等式的形式（这也被称为Bellman等式）:</p>
<ul>
<li>对于T步累计奖赏有</li>
</ul>
<p><span class="math display">\[
  V^{\pi}_T(x) = \sum_{a\in A} \pi(x,a)\sum_{x^\prime\in X}P^a_{x\rightarrow x^\prime} (\frac{1}{T} R^a_{x\rightarrow x^\prime} + \frac{T-1}{T}V^{\pi}_{T-1}(x^\prime)) (16.7)
  \]</span></p>
<ul>
<li>对于<span class="math inline">\(\gamma\)</span>折扣累计奖赏有 <span class="math display">\[
  V^{\pi}_{\gamma}(x) = \sum_{a\in A} \pi(x,a)\sum_{x^\prime\in X}P^a_{x\rightarrow x^\prime} ( R^a_{x\rightarrow x^\prime} + \gamma V^{\pi}_{\gamma}(x^\prime))   (16.8)
  \]</span></li>
</ul></li>
<li><p>那么对于策略评估算法有</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221835924.png" alt="image-20211222183501861" style="zoom:50%;" /></p></li>
<li><p>对于<span class="math inline">\(\gamma\)</span>折扣累计奖赏,可以作出更改如下:</p>
<p>需要设定阈值的原因是，基于T步累计奖赏的策略评估算法的动态规划过程是从T-&gt;0,而基于<span class="math inline">\(\gamma\)</span>折扣累计奖赏的策略评估算法的动态规划过程是从<span class="math inline">\(0-&gt;+\infty\)</span></p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221835754.png" alt="image-20211222183558680" style="zoom:50%;" /></p></li>
</ul>
<h5 id="策略改进">策略改进</h5>
<ul>
<li><p>评估完某一个策略的累计奖赏之后，我们希望对其进行改进，即求解下列优化问题 <span class="math display">\[
  \pi^* = \arg\max_{\pi} \sum_{x\in X} V^{\pi} (x)
  \]</span> 对于最优策略，定义最优值函数<span class="math inline">\(V^*(x) = V^{\pi^*}(x),\forall x\in X\)</span> ,不过对于策略空间含有约束的情况下，还需要考虑在最优值函数尽可能大的情况下，满足约束。</p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221843751.png" alt="image-20211222184325665" style="zoom:50%;" /></p></li>
<li><p>最优bellman等式的含义就是在每一次策略选择的时候，都选择当前最优的动作，其理论保证在于下图推导，不妨假设选择最优动作之后的策略是<span class="math inline">\(\pi^\prime\)</span> ,而改变动作的条件在于<span class="math inline">\(Q^\pi(x,\pi^i(x)) \ge V^\pi (x)\)</span>：</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221848055.png" alt="image-20211222184846009" style="zoom:50%;" /> <span class="math display">\[
   = V^{\pi^\prime}(x)
  \]</span> 也就是说，每一步动作的选择都保证了值函数的增大,因此可以已知选取当前的最优动作，直到<span class="math inline">\(\pi^\prime = \pi\)</span>。</p></li>
</ul>
<h5 id="策略迭代">策略迭代</h5>
<ul>
<li><p>将策略评估以及策略优化结合起来，就可以得到策略迭代过程。</p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221857066.png" alt="image-20211222185737994" style="zoom:50%;" /></p>
<p>上述的策略迭代过程由于每次改进策略之后，都要重新进行策略评估，因此比较耗时。</p>
<p>注意到策略改进的过程与值改善的过程是一致的，因此可以更改成为以下的值迭代算法。</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221859783.png" alt="image-20211222185928712" style="zoom:50%;" /></p></li>
</ul>
<h4 id="免模型学习">免模型学习</h4>
<h5 id="蒙特卡洛强化学习">蒙特卡洛强化学习</h5>
<ul>
<li><p>免模型学习是指环境并不完全知悉，导致无法进行有模型学习时的策略迭代过程。针对这样的环境，我们可以通过多次采样，求取平均累计奖赏来作为期望累积奖赏的近似，这也就是蒙特卡洛强化学习的过程。</p></li>
<li><p>由于采样有限次，因此适合使用<span class="math inline">\(T\)</span>步累计奖赏的强化学习。</p></li>
<li><p>另外，蒙特卡洛强化学习的估计对象从值函数<span class="math inline">\(V(\cdot)\)</span>转变为了状态动作函数<span class="math inline">\(Q(\cdot)\)</span></p></li>
<li><p>为了能够多次采样出不同的轨迹，可以引入<span class="math inline">\(\epsilon\)</span>-贪心策略进行策略选取，将原来确定性的策略<span class="math inline">\(\pi\)</span>转变为了<span class="math inline">\(\pi^{\epsilon}\)</span> 策略。</p></li>
<li><p>注意到引入<span class="math inline">\(\epsilon\)</span>-贪心策略并不会影响选取当前最优的动作构成的策略优化算法的正确性，因为，从已经最大化了值函数的原始策略<span class="math inline">\(\pi^\prime\)</span>中分出<span class="math inline">\(\epsilon\)</span>的概率给其他动作，仍然是原始策略<span class="math inline">\(\pi^\prime\)</span>中的最优动作可以最大化值函数。</p></li>
<li><p>如果评估与优化的都是同一个值函数，就可以得到同策略蒙特卡洛强化学习</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221917760.png" alt="image-20211222191729674" style="zoom:50%;" /></p></li>
<li><p>但是，由于同策略蒙特卡洛强化学习最终产生的是<span class="math inline">\(\epsilon\)</span>-贪心策略,我们更希望在策略评估的过程中引入<span class="math inline">\(\epsilon\)</span>-贪心策略，而在策略改进时只改进原始策略。</p></li>
<li><p>这里插入一个重要性采样(importance sampling)的概念：</p>
<p>对于另一个分布<span class="math inline">\(q\)</span>,函数<span class="math inline">\(f\)</span>在概率<span class="math inline">\(p\)</span>下的期望为： <span class="math display">\[
  \mathbb{E}[f] = \int_x q(x) \frac{p(x)}{q(x)} f(x)dx
  \]</span> 用采样离散化： <span class="math display">\[
  \hat{\mathbb{E}}[f] = \frac{1}{m} \sum_{i=1}^m \frac{p(x_i^\prime)}{q(x_i^\prime)} f(x_i^\prime)
  \]</span> 那么，对于贪心策略<span class="math inline">\(\pi^\prime\)</span>以及原始策略<span class="math inline">\(\pi\)</span>给出的两条轨迹<span class="math inline">\(i\)</span>的概率<span class="math inline">\(P_i^\pi,P_i^{\pi^\prime}\)</span>, <span class="math display">\[
  P^\pi = \prod_{i=0}^{T-1} \pi(x_i,a_i) P_{x_i \rightarrow x_{i+1}}^{a_i}\\
  P_i^{\pi^\prime} = \prod_{i=0}^{T-1} \pi^\prime(x_i,a_i) P_{x_i \rightarrow x_{i+1}}^{a_i}\\
  Q(x,a) = \frac{1}{m} \sum_{i=1}^m \frac{P^\pi}{P_i^{\pi^\prime}} r_i\\
      = \frac{1}{m} \sum_{i=1}^m \prod_{i=0}^{T-1} \frac{\pi(x_i,a_i)}{\pi^\prime(x_i,a_i)} r_i \\
      \pi(x_i,a_i) = 1\\
      \pi^\prime(x_i,a_i) =\frac{\epsilon}{|A|} \ or\ 1-\epsilon + \frac{\epsilon}{|A|}
  \]</span> <img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221930008.png" alt="image-20211222193032924" style="zoom:50%;" /></p></li>
</ul>
<h5 id="时序差分学习">时序差分学习</h5>
<ul>
<li><p>蒙特卡洛强化学习在进行完整的采样之后再进行状态-动作对的更新，可以将这个过程化为增量化的更新。</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221940826.png" alt="image-20211222194010746" style="zoom:50%;" /></p></li>
<li><p>Sarsa算法(同策略算法，评估执行的都是<span class="math inline">\(\epsilon\)</span>贪心策略):</p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221941491.png" alt="image-20211222194118419" style="zoom:50%;" /></p></li>
<li><p>Q-learning算法(异策略算法,评估<span class="math inline">\(\epsilon\)</span>贪心策略,执行原始策略):</p>
<ul>
<li><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221941783.png" alt="image-20211222194130693" style="zoom:50%;" /></li>
</ul></li>
</ul>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/12/21/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>chap.14 概率图模型</title>
    <url>/2021/12/23/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h4 id="概率图模型">概率图模型</h4>
<h5 id="隐马尔可夫模型">隐马尔可夫模型</h5>
<ul>
<li><p>inference 推断指的是利用已知变量推测未知变量的分布。</p></li>
<li><p>生成式模型建模联合分布<span class="math inline">\(P(Y,R,O)\)</span></p></li>
<li><p>判别式模型建模条件分布<span class="math inline">\(P(Y,R|O)\)</span></p>
<p><span id="more"></span></p></li>
<li><p>概率图模型是用图来表达变量相关关系的概率模型。</p>
<ul>
<li><p>有向无环图-&gt;贝叶斯网 (适合有显式的因果关系)</p></li>
<li><p>无向图-&gt;马尔可夫网 （适合有相关性，但不知道因果性）</p></li>
<li><p>($$)有向图转变为一个无向图,使用有向分离技术：</p>
<ul>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112231702012.png" alt="image-20211223170205953" style="zoom:50%;" /></p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112231703912.png" alt="image-20211223170319868" style="zoom:50%;" /></p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112231705108.png" alt="image-20211223170509065" style="zoom:50%;" /></p></li>
<li><p>有向分离技术指的是</p>
<ul>
<li>找出有向图中的所有<span class="math inline">\(V\)</span>型结构，在<span class="math inline">\(V\)</span>型结构的两个父节点之间加一个无向边(该过程为道德化)。</li>
<li>将所有的有向边改成无向边。</li>
<li>之后，该图称为道德图。</li>
<li>对于道德图中的变量<span class="math inline">\(x,y\)</span>和变量集合<span class="math inline">\(z = \{z_i\}\)</span>,如果变量<span class="math inline">\(x,y\)</span>能够被<span class="math inline">\(z\)</span>分开，也就是将<span class="math inline">\(z\)</span>去掉之后，<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>同属于两个连通分量，那么<span class="math inline">\(x,y\)</span>被<span class="math inline">\(z\)</span>有向分离，<span class="math inline">\(x\perp y|z\)</span> 。基于这个就可以找出所有的条件独立关系。</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>隐马尔可夫模型HMM是结构最简单的动态贝叶斯网。</p>
<ul>
<li><p>其有状态变量（又称隐变量）<span class="math inline">\(y\)</span>表示系统第i时刻的状态以及观测变量<span class="math inline">\(x_i\)</span></p></li>
<li><p>观测变量仅依赖于同时刻的状态变量，并且t时刻的状态变量<span class="math inline">\(y_t\)</span>仅依赖于t-1时刻的状态<span class="math inline">\(y_{t-1}\)</span>,这就是马尔可夫链，即系统的下一时刻的状态仅由当前状态决定，因此，所有变量的联合概率分布为: <span class="math display">\[
  P(x_1,y_1,..,x_n,y_n) = P(y_1)P(x_1|y_1)\prod_{i=2}^nP(y_i|y_{i+1})P(x_i|y_i)
  \]</span></p></li>
<li><p>除此之外，HMM还需要知道状态转移概率、输出观测概率、初始状态概率：</p>
<ul>
<li>状态转移概率：<span class="math inline">\(a_{ij} = P(y_{t+1}=s_j|y_t = s_i)\)</span></li>
<li>输出观测概率: $ b_{ij}=P(x_t=o_j|y_t=s_i)$</li>
<li>初始状态概率: <span class="math inline">\(\pi_i = P(y_1 = s_i)\)</span></li>
</ul></li>
<li><p>其产生观测序列<span class="math inline">\(\{x_1,x_2,...,x_n\}\)</span>的过程为：</p>
<ul>
<li><span class="math inline">\(t=1\)</span>时，根据初始状态概率<span class="math inline">\(\pi\)</span>选择初始状态<span class="math inline">\(y_1\)</span> 。</li>
<li>根据状态<span class="math inline">\(y_t\)</span>以及输出观测概率<span class="math inline">\(b_{ij}\)</span>选择观测变量取值<span class="math inline">\(x_t\)</span>。</li>
<li>根据状态<span class="math inline">\(y_t\)</span>以及状态转移概率<span class="math inline">\(a_{ij}\)</span>选择下一个状态<span class="math inline">\(y_{t+1}\)</span>。</li>
<li>如果<span class="math inline">\(t&lt;n\)</span>,<span class="math inline">\(t+=1\)</span> ,反之，stop</li>
</ul></li>
</ul></li>
</ul>
<h5 id="马尔可夫随机场">马尔可夫随机场</h5>
<ul>
<li><p>马尔可夫随机场（MRF）是典型的无向图模型，典型的马尔科夫网。</p></li>
<li><p>马尔可夫随机场定义了一些势函数（也称为因子）用来表达模型对于变量的偏爱。</p></li>
<li><p>对于任意一个子集，如果子集中的任意两点都有边连接，那么就称这个子集构成了一个团，如果这个团加入任意点都不再构成团，那么这个团就是一个极大团。</p></li>
<li><p>对于n个变量<span class="math inline">\(x_1,x_2,...,x_n\)</span>,假设所有的团构成的集合是<span class="math inline">\(Q\)</span>,马尔可夫随机场定义的联合概率为： <span class="math display">\[
  P(x) = \frac{1}{Z} \prod_{Q\in\mathcal{C}}\psi_{Q}(x_Q)
  \]</span> 其中<span class="math inline">\(\psi_Q(x)\)</span>是一个势函数，<span class="math inline">\(Z\)</span>是规范化因子，<span class="math inline">\(\mathcal{C}\)</span>为所有团构成的集合</p></li>
<li><p>但是，上述定义对于有很多变量的情况下，团数量较多，会有大量乘积项导致计算繁琐。因此，可以对于每个极大团定义一个势函数，假设极大团构成的集合为<span class="math inline">\(\mathcal{C}^*\)</span>,那么上式转换为： <span class="math display">\[
  P(x) = \frac{1}{Z^*} \prod_{Q\in\mathcal{C}^*}\psi_{Q}(x_Q)
  \]</span></p></li>
<li><p>跟有向分离类似的，马尔可夫随机场也可以获得变量之间的条件独立性：</p>
<ul>
<li>如果结点集A中的点想要连接到结点集B中的点都需要通过结点集C中的点，那么就称结点集A和结点集B被结点集C分离，结点集C为分离集，可以与有向分离类似表示为<span class="math inline">\(x_A \perp x_B | x_C\)</span> :
<ul>
<li>全局马尔可夫性：有分离集的两个结点集互相条件独立。</li>
<li>局部马尔可夫性：对于一个结点，给定其邻接变量，该点与其他非邻接变量的点条件独立。</li>
<li>成对马尔可夫性：任意两个非邻接的结点条件独立。</li>
</ul></li>
</ul></li>
<li><p>对于马尔可夫随机场中要求的势函数，其首先需要是一个非负实函数，并且对于其偏好的变量的取值较大。</p></li>
</ul>
<h5 id="条件随机场">条件随机场</h5>
<ul>
<li><p>条件随机场(CRF)是一种判别式无向图模型，HMM以及MRF都是生成式模型。</p>
<p>其目标是在给定一个观测序列<span class="math inline">\(x_1,x_2,...,x_n\)</span>,<span class="math inline">\(y = y_1,y_2,...,y_n\)</span>,构建出条件概率<span class="math inline">\(P(y|x)\)</span>,这里的<span class="math inline">\(y\)</span>可以含有结构型变量。</p></li>
<li><p>如果假设<span class="math inline">\(y_v\)</span>为与结点<span class="math inline">\(v\)</span>对应的标记变量，用<span class="math inline">\(n(v)\)</span>表示结点<span class="math inline">\(v\)</span>的邻接结点，如果无向图<span class="math inline">\(G(V,E)\)</span>满足马尔可夫性： <span class="math display">\[
  P(y_v|x,y_{V\backslash \{v\}}) = P(y_v|x,y_{n(v)})
  \]</span> 那么<span class="math inline">\((x,y)\)</span>构成了一个条件随机场。</p></li>
<li><p>最常用的条件随机场结构如下:</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112232148448.png" alt="image-20211223214829373" style="zoom:50%;" /></p>
<p>注意到上述结构中，构成团的标记变量为<span class="math inline">\(y_i,y_{i+1}\)</span>以及单个标记变量<span class="math inline">\(y_i\)</span>。</p>
<p>条件随机场也采用指数势函数以及团的结构来定义条件概率，并且额外增加了特征函数。 <span class="math display">\[
  P(y|x) = \frac{1}{Z} \exp(\sum_j \sum_{i=1}^{n-1}\lambda_jt_j(y_{i+1},y_i,x,i) + \sum_k \sum_{i=1}^{n}\mu_k s_k(y_i,x,i))
  \]</span> 其中<span class="math inline">\(t_j(y_{i+1},y_i,x,i)\)</span>是定义在两个相邻标记变量上的特征函数，用来刻画相邻变量之间的相关性以及观测序列对他们的影响；<span class="math inline">\(s_k(y_i,x,i)\)</span>定义在标记<span class="math inline">\(i\)</span>上的状态特征函数，用来刻画观测序列对于标记变量的影响。</p>
<ul>
<li><p>特征函数的作用是刻画一些数据很可能成立或者期望成立的经验特性。</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112232159701.png" alt="image-20211223215936642" style="zoom:50%;" /></p></li>
</ul></li>
<li><p>条件随机场以及马尔可夫随机场的区别主要就在于一个对条件概率建模，而另一个对联合概率建模。</p></li>
</ul>
<h5 id="学习与推断">学习与推断</h5>
<ul>
<li><p>基于概率图模型定义的联合概率分布，我们可以对目标变量的边际分布或者是以某些可观测变量为条件的条件分布进行推断。</p>
<ul>
<li>边际分布是指对联合分布中的其他无关变量进行积分或者求和的过程，也称该过程为边际化。</li>
</ul></li>
<li><p>并且，对于概率图模型，还需要确定一些具体分布的参数，该参数估计过程也可以被吸收到参数推断问题当中。</p>
<ul>
<li>具体而言，假设图模型有变量集<span class="math inline">\(x =\{x_1,x_2,...,x_N\}\)</span>能够分为<span class="math inline">\(x_E,x_F\)</span>两个不相交的变量集，推断问题的目标就是计算边际概率<span class="math inline">\(P(x_F)\)</span>或者条件概率<span class="math inline">\(P(x_F|x_E)\)</span>: <span class="math display">\[
  P(x_F|x_E) = \frac{P(x_E,x_F)}{P(x_E)} = \frac{P(x_E,x_F)}{\sum_{x_F}P(x_E,x_F)}
  \]</span> 而联合概率<span class="math inline">\(P(x_E,x_F)\)</span>可以通过概率图模型获得，因此重点就是计算边际分布: <span class="math display">\[
  P(x_E) = \sum_{x_F}P(x_E,x_F)
  \]</span> 对于该边际分布的计算，既可以采取精确计算的方式，但是其有指数时间复杂度，也可以采取近似推断的方式,其用较短的时间得到近似解。</li>
</ul></li>
<li><p>精确推断:</p>
<ul>
<li><p>变量消去:</p>
<ul>
<li><p>变量消去是通过先计算部分变量的加法来整合部分变量：</p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112232229774.png" alt="image-20211223222947706" style="zoom:50%;" /></p></li>
<li><p>不管是有向图还是无向图，该方法都是适用的。其缺点在于会造成大量冗杂运算。</p></li>
</ul></li>
<li><p>信念传播：</p>
<ul>
<li>主要用来解决变量消去算法中大量的重复计算过程。</li>
<li>信念传播基于的现实就是每次消息传播的过程都是在邻接结点之间进行的，因此，我们可以规定一个结点只有在接受完所有结点的信息之后才能发送消息给其他结点。
<ul>
<li>以上面图片中的例子来说，<span class="math inline">\(x_3\)</span>必须接收到<span class="math inline">\(m_{23}(x_3),m_{43}(x_3)\)</span>两个信息，才能发送信息给<span class="math inline">\(x_5\)</span>。</li>
</ul></li>
<li>对于无环的图，信念传播只需要经过两步就可以了:
<ul>
<li>指定一个根节点，让所有的叶节点传递信息给根节点，直到根节点的所有邻接结点都已经给根节点传了信息。</li>
<li>让根节点传递信息给所有的叶节点</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>近似推断</p>
<ul>
<li><p>近似推断有两种方案，一是通过采样(MCMC采样)来进行，使用随机化来完成近似,由大数定律保证近似可行；第二种确定性近似方法——变分推断。</p></li>
<li><p>MCMC采样:</p>
<ul>
<li><p>给定连续变量<span class="math inline">\(x\in X\)</span>的概率密度函数<span class="math inline">\(p(x)\)</span>,<span class="math inline">\(x\)</span>在某区间<span class="math inline">\(A\)</span>中的概率可以计算为: <span class="math display">\[
  P(A) = \int_A p(x) dx
  \]</span> 对于一个函数<span class="math inline">\(f(x)\)</span>: <span class="math display">\[
  p(f) = \mathbb{E}[f(X)] = \int_x f(x)p(x)dx
  \]</span> 通过采样构造出服从<span class="math inline">\(p\)</span>分布的独立同分布随机变量<span class="math inline">\(x_1,x_2,..,x_N\)</span>，得到一个无偏估计: <span class="math display">\[
  \hat{p}(f) = \frac{1}{N} \sum_{i=1}^N f(x_i)
  \]</span> 但是如果概率密度函数<span class="math inline">\(p(x)\)</span>很复杂，那么构造出<span class="math inline">\(p\)</span>分布的独立同分布采样也十分困难。MCMC采样的关键就是构造“平稳分布为p的马尔可夫立链",在假定马尔可夫链的状态转移概率为<span class="math inline">\(T(x^\prime|x)\)</span>,<span class="math inline">\(t\)</span>时刻的分布为<span class="math inline">\(p(x^t)\)</span> ,则马尔可夫链稳定的判定条件如下: <span class="math display">\[
  p(x^t) T(x^{t-1}|x^t) = p(x^{t-1}) T(x^t|x^{t-1})
  \]</span> 意为在t时刻的分布下状态转移到t-1时刻分布的概率与在t-1时刻的分布下状态转移到t时刻分布的概率相同。</p></li>
<li><p>MCMC采样的代表算法是Metropolis-Hastings:</p>
<ul>
<li><p>算法每次根据上一轮采样结果<span class="math inline">\(x^{t-1}\)</span>来采样获得候选状态样本<span class="math inline">\(x^*\)</span>,但是这个候选状态样本还会有一定概率被拒绝掉，假设状态<span class="math inline">\(x^{t-1}\)</span>到状态<span class="math inline">\(x^*\)</span>的转移概率为<span class="math inline">\(Q(x^*|x^{t-1})A(x^*|x^{t-1})\)</span>,其中<span class="math inline">\(Q(x^*|x^{t-1})\)</span>是用户给定的先验概率，<span class="math inline">\(A(x^*|x^{t-1})\)</span>是被拒绝的概率: <span class="math display">\[
  p(x^{t-1})Q(x^*|x^{t-1})A(x^*|x^{t-1})=p(x^*)Q(x^{t-1}|x^*)A(x^{t-1}|x^*)
  \]</span> <img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241032616.png" alt="image-20211224103211514" style="zoom:50%;" /></p></li>
<li><p>为了到达平稳状态，接受概率需要设置为: <span class="math display">\[
  A(x^*|x^{t-1}) = \min (1,\frac{p(x^*)Q(x^{t-1}|x^*)}{p(x^{t-1})Q(x^*|x^{t-1})})
  \]</span></p></li>
<li><p>另外，吉布斯采样也是MH算法的特例，它也用了马尔科夫链获取样本，而该马尔可夫链的平稳分布也是目标分布：</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241040128.png" alt="image-20211224104047071" style="zoom:50%;" /></p>
<p>具体而言:</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241056226.png" alt="image-20211224105633164" style="zoom:50%;" /></p>
<p>实质上，吉布斯采样是对贝叶斯网的所有变量的联合状态空间中与证据<span class="math inline">\(E=e\)</span>一致的子空间中进行"随机漫步"(random walk)。</p></li>
</ul></li>
</ul></li>
<li><p>变分推断:</p>
<ul>
<li><p>变分推断使用已知的简单分布逼近需要推断的复杂分布，生成一个局部最优但是具有确定解的近似后验分布。</p></li>
<li><p>盘式记法:</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241101773.png" alt="image-20211224110126700" style="zoom:50%;" /></p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241104149.png" alt="image-20211224110402065" style="zoom:50%;" /></p>
<p>在E步中为了解决z模型复杂的问题，可以借助变分推断，假设z服从分布: <span class="math display">\[
  q(z) = \prod_{i=1}^M q_i(z_i)
  \]</span> 也就是说，假设复杂变量z能够被拆分成一系列独立的多变量<span class="math inline">\(z_i\)</span>，并且采用一些具有良好性质的<span class="math inline">\(q_i(\cdot)\)</span>。这一步就是关键了，通过对隐变量<span class="math inline">\(z\)</span>进行拆分，假设各个变量子集<span class="math inline">\(z_j\)</span>服从的分布，套用其说能满足的最优分布<span class="math inline">\(q_j^*\)</span>公式,然后用EM算法求解即可: <span class="math display">\[
  q_j^* = \frac{\exp(\mathbb{E}_{i\not=j}[\ln p(x,z)])}{\int \exp(\mathbb{E}_{i\not=j}[\ln p(x,z)])dz_j}
  \]</span> <span class="math inline">\(\mathbb{E}_{i\not=j}[\ln p(x,z)]\)</span>往往有闭式解(解析解、数值解、可以由表达式求出),该式子也称为平均场办法。</p></li>
</ul></li>
</ul></li>
</ul>
<h5 id="话题模型">话题模型</h5>
<ul>
<li><p>话题模型是一族生成式有向图模型，LDA（隐狄利克雷分配模型）是其代表。</p></li>
<li><p>话题模型中的一些基本概念:</p>
<ul>
<li>词：待处理数据的离散单元。</li>
<li>文档：由一组词组成，不计顺序。</li>
<li>话题：表示一系列相关的词。</li>
</ul></li>
<li><p>LDA认为每篇文档(<span class="math inline">\(\mathbf{W}=\{w_1,w_2,...,w_n\}\)</span>)有多个话题(用k个N维向量表示话题<span class="math inline">\(\beta_k\)</span>),<span class="math inline">\(w_i,\beta_i\)</span>的分量都表示词频,<span class="math inline">\(\Theta_t\)</span>表示文档<span class="math inline">\(t\)</span>中包含的话题比例，<span class="math inline">\(\Theta_{t,k}\)</span>表示文档<span class="math inline">\(t\)</span>中包含话题<span class="math inline">\(k\)</span>的比例，其步骤如下:</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241117067.png" alt="image-20211224111739018" style="zoom:50%;" /></p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241117015.png" alt="image-20211224111751973" style="zoom:50%;" /></p></li>
<li><p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241122527.png" alt="image-20211224112202465" style="zoom:50%;" /></p></li>
<li><p>其对应的概率分布为:</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112241124078.png" alt="image-20211224112450019" style="zoom:50%;" /></p></li>
<li><p>给定训练数据<span class="math inline">\(\mathbf{W}\)</span>,LDA模型根据极大似然估计求解模型参数 <span class="math display">\[
  LL(\alpha,\eta)= \sum_{t=1}^T \ln p(w_t|\alpha,\eta)
  \]</span> 知道模型参数之后就可以根据<span class="math inline">\(\mathbf{W}\)</span>中的词频推断文档所对应的话题结构 <span class="math display">\[
  p(z,\beta,\Theta|\mathbf{W},\alpha,\eta) = \frac{p(\mathbf{W},z,\beta,\Theta|\alpha,\eta)}{p(\mathbf{W}|\alpha,\eta)} (*)
  \]</span> 其中，<span class="math inline">\(p(w_t|\alpha,\eta)\)</span>常常用变分法求解，<span class="math inline">\((*)\)</span>式子常用吉布斯采样或者变分法求解。</p></li>
</ul>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>chap.13 半监督学习</title>
    <url>/2021/12/22/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h4 id="半监督学习">半监督学习</h4>
<h5 id="未标记样本">未标记样本</h5>
<ul>
<li><p>显然，现实问题有很多没有label的数据，这些数据的数量明显大于有label的数据量。</p></li>
<li><p>为了将这些没有label的数据利用起来，有个平凡的想法，就是人工对所有的unlabel的数据激进行label，但是这是耗费大量人力物力的。</p></li>
<li><p>基于此，主动学习就被提出了。主动学习的流程是想使用有label的数据<span class="math inline">\(D_l\)</span>训练出一个学习器，然后让这个学习器对unlabel的数据进行预测，将预测结果人工检查好不好，再加入到有标记样本中进行训练。每次我们都挑选出对改善模型性能有帮助的瓜，那么，就只需要人工检查较少次数就可以构建出性能比较好的模型，从而降低成本。这也符合了它的目的：用尽量少的查询获得尽量好的性能。</p></li>
<li><p>但是主动学习仍然借助了专家的知识，如果能让学习器不依赖外界知识，自动利用<span class="math inline">\(D_u\)</span> 数据，那就是半监督学习(semi-supervised learning)的学习目标了。</p>
<p><span id="more"></span></p></li>
<li><p>想要做到这一点，一个问题就浮现了：如何揭示未标记样本所包含的数据分布信息并将其与label相联系？</p>
<ul>
<li>为了解决这个问题，提出了几个假设：
<ul>
<li>聚类假设（cluster assumption）:
<ul>
<li>假设数据有簇结构，同一个簇的样本属于同一个类别</li>
</ul></li>
<li>流形假设（manifold assumption）:
<ul>
<li>假设数据分布在一个流形结构上，邻近的样本拥有相似的输出值</li>
<li>它与聚类假设的本质都是相似的样本有相似的输出</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>半监督学习又可以进一步划分为纯半监督学习、直推学习。</p>
<ul>
<li>纯半监督学习认为未标记样本并不一定是待预测的数据。（open-world assumption）这个认定也就意味着纯半监督学习可以对测试样本进行预测。</li>
<li>而直推学习则认为未标记样本就是待预测的数据。（closed-world assumption）</li>
</ul></li>
</ul>
<h5 id="生成式方法">生成式方法</h5>
<ul>
<li><p>基于生成式模型，认为所有数据都是同一个潜在模型生成的，未标记样本的label是模型的缺失参数，可以基于EM算法进行极大似然法求解。</p></li>
<li><p>给定一个样本<span class="math inline">\(x\)</span>,其真实类别标记为<span class="math inline">\(y \in \mathcal{Y}\)</span> ,假设样本由高斯混合模型生成，并且每个类别对应一个高斯混合成分，也就是说： <span class="math display">\[
  p (x) = \sum_{i=1}^N \alpha_i \cdot p(x|\mu_i,\Sigma_i)
  \]</span> 其中<span class="math inline">\(\alpha_i \ge 0,\sum_{i=1}^k \alpha_i = 1\)</span>,<span class="math inline">\(p(x|\mu_i,\Sigma_i)\)</span>表示样本<span class="math inline">\(x\)</span>属于第<span class="math inline">\(i\)</span>个高斯混合成分。</p></li>
<li><p>如果接着假设<span class="math inline">\(f(x)\)</span>是模型<span class="math inline">\(f\)</span>对样本<span class="math inline">\(x\)</span>的预测标记，用<span class="math inline">\(\Theta\in\{1,2,...,N\}\)</span>表示样本隶属的高斯混合成分，按照最大化后验概率的想法，有： <span class="math display">\[
  f(x) = \arg\max_{j\in\mathcal{Y}} p(y=j|x) \\
   = \arg\max_{j\in\mathcal{Y}} \sum_{i=1}^N p(y=j,\Theta=i|x) \\
   = \arg\max_{j\in\mathcal{Y}} \sum_{i=1}^N p(y=j|\Theta=i,x) \cdot p(\Theta=i|x) \\
   p(\Theta=i|x) = \frac{\alpha_i \cdot p(x|\mu_i,\Sigma_i)}{\sum_{i=1}^N \alpha_i \cdot p(x|\mu_i,\Sigma_i)}
  \]</span> <span class="math inline">\(p(\Theta=i|x)\)</span>表示的是样本<span class="math inline">\(x\)</span>由第i个高斯混合成分生成的后验概率</p>
<p><span class="math inline">\(p(y=j|\Theta=i,x)\)</span>表示的是样本<span class="math inline">\(x\)</span>由第i个高斯混合成分生成且属于类别j的概率。</p>
<p>再假设类别与高斯混合成分相对应的情况下，只有当<span class="math inline">\(i=j\)</span>的时候，<span class="math inline">\(p(y=j|\Theta=i,x)=1\)</span> ,otherwise,为0.</p></li>
<li><p>对于上面的式子，由于<span class="math inline">\(p(y=j|\Theta=i,x)\)</span>与样本挂钩，<span class="math inline">\(p(\Theta=i|x)\)</span>与样本无关。因此估计<span class="math inline">\(p(\Theta=i|x)\)</span>的时候可以使用未标记样本,也就可以辅助预测<span class="math inline">\(f(x)\)</span>。</p></li>
<li><p>极大似然法求解过程：</p>
<ul>
<li><p><span class="math display">\[
  LL(D_l \cup D_u) =  \sum_{x} \ln(\sum_{i=1}^N p(y=j|\Theta=i,x) \cdot p(\Theta=i|x)) \\
  = \sum_{(x_j,y_j)\in D_l} \ln \sum_{i=1}^N \alpha_i  \cdot p(x|\mu_i,\Sigma_i) \cdot p(y=j|\Theta=i,x) + \\
      \sum_{x_j\in D_u} \ln \sum_{i=1}^N \alpha_i  \cdot p(x|\mu_i,\Sigma_i)
  \]</span></p>
<p>拆分为了与数据label有关的一项以及与数据label无关的一项。</p>
<p>与数据label有关的一项利用<span class="math inline">\(D_l\)</span>进行训练，与数据label无关的一项利用<span class="math inline">\(D_u\)</span>进行训练</p></li>
<li><p>接下来就需要用EM算法进行求解。</p></li>
</ul></li>
<li><p>但是这种方法存在一个缺陷就是只有在假设的数据分布与真实的数据分布吻合的情况下才有效，否则利用未标记数据会降低泛化性能，这也是现实中很难做到的事情。</p></li>
<li><p>(*)高斯混合分布：</p>
<ul>
<li><p><span class="math display">\[
  p_M(x) = \sum_{i=1}^k \alpha_i \cdot p(x|\mu_i,\Sigma_i)
  \]</span></p></li>
<li><p>k个混合成为组成，每个混合成分对应一个高斯分布。<span class="math inline">\(\alpha_i \ge 0,\sum_{i=1}^k \alpha_i = 1\)</span>是混合系数。</p></li>
<li><p>高斯分布:</p>
<ul>
<li><span class="math display">\[
  p(x) = \frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x-\mu)}
  \]</span></li>
</ul></li>
</ul></li>
<li></li>
<li><p>(*)EM算法 review:</p>
<ul>
<li><p>在西瓜书的推导中，将上述提到的<span class="math inline">\(p(\Theta=i|x)\)</span>记为<span class="math inline">\(\gamma_{i}\)</span> ,显然由最大化后验概率可知，对于样本<span class="math inline">\(x_j\)</span>的簇标记<span class="math inline">\(\lambda_j\)</span>由<span class="math inline">\(\arg\max \gamma_{ji}\)</span>确定。计算<span class="math inline">\(\gamma_{ji}\)</span>的过程,即计算样本<span class="math inline">\(x_j\)</span>属于各个高斯混合概率的概率就是E步。</p>
<p>下面根据计算的<span class="math inline">\(\gamma_{ji}\)</span>更新参数<span class="math inline">\(\alpha_i,\mu_i,\Sigma_i\)</span>,就是M步。</p></li>
<li><p>由极大似然可推知: <span class="math display">\[
  LL(D) = \ln(\prod_{j=1}^m p_M(x_j)) \\ 
       = \sum_{j=1}^m \ln (\sum_{i=1}^k \alpha_i \cdot p(x|\mu_i,\Sigma_i))
  \]</span> 我们要学习的是<span class="math inline">\(\alpha_i,\mu_i,\Sigma_i\)</span></p>
<p>由<span class="math inline">\(\frac{\partial LL(D)}{\partial \mu_i} = 0\)</span>可知 <span class="math display">\[
  \frac{d (\frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x-\mu)})}{d \mu_i} \sum_{j=1}^m \frac{\alpha_i \cdot p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^k \alpha_l \cdot p(x|\mu_l,\Sigma_l)} = 0 \ (1)\\ 
  \sum_{j=1}^m \frac{\alpha_i \cdot p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^k \alpha_l \cdot p(x|\mu_l,\Sigma_l)} (x_j - \mu_i) = 0 \ (2)\\
  \sum_{j=1}^m \gamma_{ji} (x_j - \mu_i) = 0 \\
  \mu_i = \frac{\sum_{j=1}^m \gamma_{ji} x_j}{\sum_{j=1}^m \gamma_{ji}}
  \]</span> 注意的是，(1)式的导数在约去所有大于零的部分以及参数部分就可以转换为(2)式。</p>
<p>同理，按照<span class="math inline">\(\frac{\partial LL(D)}{\partial \Sigma_i} = 0\)</span>可知 <span class="math display">\[
  \Sigma_i = \frac{\sum_{j=1}^m \gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^T}{\sum_{j=1}^m \gamma_{ji}}
  \]</span> 对于混合系数<span class="math inline">\(\alpha_i\)</span>,我们需要用拉格朗日乘子法， <span class="math display">\[
  LL(D) + \lambda(\sum_{i=1}^k \alpha_i -1)
  \]</span> 对上式进行对<span class="math inline">\(\alpha_i\)</span>进行求偏导为0 <span class="math display">\[
  \sum_{j=1}^m \gamma_{ji} + \alpha_i \lambda = 0 \\
  \sum_{i=1}^m(\sum_{j=1}^m \gamma_{ji} + \alpha_i \lambda) = m + \lambda = 0\\
  \lambda = -m \\
  \alpha_i = \frac{\sum_{j=1}^m \gamma_{ji}}{m}
  \]</span></p></li>
</ul></li>
</ul>
<h5 id="半监督svm">半监督SVM</h5>
<ul>
<li>半监督SVM是在SVM的基础上，将两类有标记样本切开并且倾向于穿过数据低密度区域。</li>
<li>半监督SVM最著名的TSVM,TSVM针对二分类，想法是尝试将每个未标记样本作为正例或者反例，在所有可能的结果中，找到一个在所有样本中间隔最大化的划分超平面。划分超平面确定，则样本的划分标签确定。</li>
<li></li>
</ul>
<h5 id="图半监督学习">图半监督学习</h5>
<ul>
<li><p>图半监督学习的想法是给定一个数据集，将其映射为一个图，每个样本对应于图中的一个结点，如果两个样本之间的相似度很高，那么两个节点之间不仅有边，并且边的权重也很高。</p>
<p>并且，对于有标记样本，我们将其建模为染过色的结点，未标记样本则是未染色的结点。那么，半监督学习的过程就是颜色在图上传播的过程。</p></li>
<li><p>要让图可训练，我们需要将其构建为一个矩阵，通常根据高斯函数定义affinity matrix(亲和矩阵): <span class="math display">\[
  (\mathbf{W})_{ij} = \left\{
  \begin{aligned}
  &amp; \exp (\frac{-||x_i - x_j||_2^2}{2\sigma^2}) , if\ i\not=j\\
  &amp; 0, otherwise
  \end{aligned} 
  \right.
  \]</span> 同时基于相似的样本有相似的输出可以定义能量函数： <span class="math display">\[
  E(f) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} (f(x_i) - f(x_j))^2 \\
  = \mathbf{f}^T (\mathbf{D} - \mathbf{W}) \mathbf{f} \\
  \mathbf{f} = (\mathbf{f}^T_l \mathbf{f}^T_u)^T \\
  \mathbf{D} = diag(d_1,d_2,...,d_{u+l}),d_i = \sum_{j=1}^{l+u} (\mathbf{W})_{ij}
  \]</span></p></li>
<li></li>
</ul>
<h5 id="基于分歧的方法">基于分歧的方法</h5>
<ul>
<li><p>基于分歧的方法常使用多个学习器，而学习器之间的分歧对于未标记数据的利用有很大的借鉴意义。</p></li>
<li><p>其重要代表是协同训练，它也是多视图学习的代表。</p>
<ul>
<li><p>其假设数据有两个充分且条件独立的视图。</p>
<ul>
<li>充分是指每个视图都包含足以产生最优学习器的信息。</li>
<li>条件独立是指在给定类别条件的情况下，两个视图独立。</li>
</ul></li>
<li><p>其想法是在每个视图上基于有标记样本分别训练出一个分类器，然后挑选每个分类器对未标记样本正确概率最大的样本作为另一个分类器的有标记样本加入下一轮训练。但是这种想法需要在每轮学习中都考察分类器在所有未标记样本上的置信度，计算开销大。这个问题可以借助未标记样本缓冲池（一开始选择s个<span class="math inline">\(D_u\)</span>中的样本加入，每轮循环结束后，再选一些加入）进行解决。</p></li>
<li><p>总过程伪代码：</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221451878.png" alt="image-20211222145112704" style="zoom:50%;" /></p></li>
<li><p>在数据有两个充分且条件独立的视图的条件下，弱学习器的泛化能力可以提升到任意高。</p></li>
</ul></li>
<li><p>在后续研究中发现，数据拥有两个视图的假设并不重要，只需要弱学习器之间有明显差异即可，算法、视图、不同采样方式、不同参数都可以产生差异。</p></li>
</ul>
<h5 id="半监督学习-1">半监督学习</h5>
<ul>
<li>在现实聚类的过程中，我们常常可以获得一些额外的监督信息，因此，我们可以使用半监督聚类来利用监督信息以获得更好的聚类效果。</li>
<li>监督信息大致有以下两类：
<ul>
<li>必连：样本必定属于同一个簇。</li>
<li>勿连：样本必定不属于同一个簇。</li>
<li>少量的有标记样本，即事先已经找到少量样本属于哪一个簇当中。</li>
</ul></li>
<li>典型代表是约束k均值算法：
<ul>
<li>其是k-mean是算法的拓展，是第一类监督信息的代表（勿连与必连）。需要保证勿连集合<span class="math inline">\(\mathcal{C}\)</span>以及必连集合<span class="math inline">\(\mathcal{M}\)</span>中的约束，即如果有<span class="math inline">\((x_i,x_j)\in\mathcal{M}\)</span>，那么<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>必定是同一类别的。</li>
<li>其实就是k-mean算法的基础上加上判断每次样本<span class="math inline">\(x\)</span>与哪个均值向量<span class="math inline">\(u_i\)</span>最近，在划入之前，检查是否满足约束，如果不满足，就将样本<span class="math inline">\(x\)</span>加入次近的簇当中，再进行检查。</li>
<li>如果始终找不到可以容纳样本<span class="math inline">\(x\)</span>的簇，则认为无法完成聚类，输出error。</li>
<li><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221527153.png" alt="image-20211222152740069" style="zoom:50%;" /></li>
</ul></li>
<li>利用第二类监督信息的代表是约束种子的k-means算法：
<ul>
<li>它的利用方式很容易理解，就是直接将已知簇标记的样本作为种子，初始化k-means的k个聚类中心，并且再聚类簇迭代的过程中始终不改变种子的标记。</li>
<li><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112221537526.png" alt="image-20211222153753446" style="zoom:50%;" /></li>
</ul></li>
</ul>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Chap.11 特征选择与稀疏学习</title>
    <url>/2021/12/22/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h4 id="第十一章特征选择与稀疏学习">第十一章：特征选择与稀疏学习</h4>
<h5 id="特征">特征</h5>
<ul>
<li><p>相关特征:与当前学习任务相关</p></li>
<li><p>无关特征:与当前学习任务无关</p></li>
<li><p>冗杂特征:可以从其他特征中提取出来</p></li>
<li><p>从特征集合选出相关特征的过程就是特征提取。</p></li>
<li><p>特征提取的过程可以大为减轻维度灾难的问题，同时提取出重要的特征也可以降低学习的难度。</p></li>
<li><p>特征提取的一般过程：</p>
<ul>
<li>产生初始特征候选子集 —— 子集搜索</li>
<li>评价候选子集的好坏 —— 子集评价</li>
<li>基于评价结果产生下一个候选子集</li>
</ul>
<p><span id="more"></span></p></li>
<li><p>其中的关键就是子集搜索以及子集评价:</p>
<ul>
<li><p>对于子集搜索:</p>
<ul>
<li>前向搜索：逐渐增加相关特征，如果增加特征后的子集优于上一轮的最优子集，那就继续，否则结束；</li>
<li>后向搜索：从完整的特征集合开始，逐渐减少特征；</li>
<li>双向搜索：每一轮逐渐增加相关特征，同时减少无关特征。</li>
</ul></li>
<li><p>对于子集评价:</p>
<ul>
<li><p>其思想在于如果特征子集划分的数据集与样本标记划分的数据集差异越小，就认为这种特征子集划分方法越好。</p></li>
<li><p>可以用信息熵以及信息增益进行评价：</p>
<ul>
<li><p>定义第i类样本在D数据集中所占比例为<span class="math inline">\(p_i\)</span></p></li>
<li><p><span class="math display">\[
  Gain(A) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|}Ent(D^v) \\
  Ent(D) = - \sum_{i=1}^{|\mathcal{Y}|} p_k \log_2 p_k 
  \]</span></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>常见的特征选择方法：</p>
<ul>
<li><p>过滤式(filter)：</p>
<ul>
<li><p>先对数据集进行特征选择，再训练学习器，也就是对数据集中的初始特征进行了一个过滤操作，学习器的性能表现不作为反馈。</p></li>
<li><p>相关方法——Relief:其想法是设计一个相关统计量向量，向量的一个维度都对应着样本的每一个特征，那么特征子集的重要性就是向量各维度的和。选择我们需要的特征子集的时候，只需要设定一个阈值<span class="math inline">\(t\)</span>,大于<span class="math inline">\(t\)</span>的都选即可。</p></li>
<li><p>相关统计量如何设定呢？Relief的做法是对于训练集中的每个样本<span class="math inline">\(x_i\)</span>，在其同类样本中找到最近邻<span class="math inline">\(x_{i,nh}\)</span>作为猜中近邻(near-hit)，从异类样本中找到最近邻<span class="math inline">\(x_{i,nm}\)</span>作为猜错近邻(near-miss),那么相关统计量在属性j(第j维)上的表现就是： <span class="math display">\[
  \delta^j = \sum_i -diff(x_i^j, x_{i,nh}^j)^2 + diff(x_i^j, x_{i,nm}^j)^2
  \]</span></p></li>
<li><p>根据属性j是否连续，<span class="math inline">\(diff(x_a^j,x_b^j)\)</span>的计算方式也有差别。连续：<span class="math inline">\(|x_a^j-x_b^j|\)</span>,离散：<span class="math inline">\(x_a^j==x_b^j? 0:1\)</span></p></li>
<li><p>并且，上述式子表明如果属性j使得样本与其猜中近邻的距离小于样本与其猜错近邻的距离，那么，属性j对于区分样本类别是有益，我们给定的<span class="math inline">\(\delta^j\)</span>的值就是正的</p></li>
<li><p>Relirf是线性时间的，速度快</p></li>
<li><p>Relief针对二分类，多分类是Relief-F,其相关统计量的式子为: <span class="math display">\[
  \delta^j = \sum_i -diff(x_i^j, x_{i,nh}^j)^2 + \sum_{l\not=k} p_l \times diff(x_i^j, x_{i,l,nm}^j)^2
  \]</span> 可以看出只是一个简单拓展而已，也是通过衡量属性j能否使得样本与其猜中近邻的距离小于样本与其猜错近邻的距离，来判断属性j是否有益。</p></li>
</ul></li>
<li><p>包裹式(wrapper)</p>
<ul>
<li>将学习器的性能作为特征子集的评价标准，因此，其目的在于给定学习器选择最有利于其性能的特征子集。</li>
<li>包裹式的性能常好于过滤式，但是由于需要训练多次，需要大量时间开销。</li>
<li>LVW是一个典型代表：
<ul>
<li><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112211632108.png" alt="image-20211221163215946" style="zoom:50%;" /></li>
<li>其过程可以概括为，每次选出一个随机的特征子集，训练返回loss，记录最小loss的特征子集，如果连续k次最小loss都不变，那么，认为我们选出了对于训练学习器而言最有效的特征子集。</li>
<li>由于随机性的加入，LVW要么不给解，要么给出一个满足条件的解；而MC方法，一定给出解，但合适不合适不一定。</li>
</ul></li>
</ul></li>
<li><p>嵌入式(embedding)</p>
<ul>
<li><p>嵌入式是将特征选择过程嵌入到学习器的训练过程当中，由学习器自动完成。</p></li>
<li><p>对于一个简单的线性回归模型，以MSE作为loss，如果引入L2范数作为正则项，那么这就是岭回归问题；如果引入L1范数作为正则项，那么就是LASSO问题。他们都可以显著降低过拟合风险。</p></li>
<li><p>对于引入L1范数作为正则项，其更容易产生稀疏解，求得的权重<span class="math inline">\(w\)</span>含有的非零项更少。观察下图L2范数与L1范数等值线的形状以及可能与平方误差等值线相交的点的位置可知,L1范数等值线与平方误差等值线相交的点更容易出现在坐标轴上。</p>
<p><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112211642656.png" alt="image-20211221164227586" style="zoom:33%;" /></p></li>
<li><p>因此，采用L1正则化进行训练，其实只用到了原始样本的一部分特征。L1正则化问题的求解可以使用近端梯度下降PGD。</p></li>
<li><p>PGD(近端梯度下降，利用了假设f(x)满足L-lipschitz条件):</p>
<ul>
<li><p>对于一个呈形如下的优化问题： <span class="math display">\[
  \min_x f(x)+\lambda ||x||_1
  \]</span> 如果<span class="math inline">\(f(x)\)</span>可导，并且<span class="math inline">\(\nabla f\)</span>满足<span class="math inline">\(L-Lipschitz\)</span>条件，即存在常数<span class="math inline">\(L&gt;0\)</span>对于<span class="math inline">\(\forall x,x^\prime\)</span>使得 <span class="math display">\[
  ||\nabla f(x^\prime) - \nabla f(x)||_2^2 \le L ||x^\prime - x||_2^2
  \]</span></p></li>
<li><p>将上式简单等价改写一下: <span class="math display">\[
  \frac{\partial^2 f(x_k)}{\partial x_k^2} =  \frac{||\nabla f(x^\prime) - \nabla f(x)||_2^2}{||x^\prime - x||_2^2} \le L
  \]</span></p></li>
<li><p>对于<span class="math inline">\(f(x)\)</span>,在<span class="math inline">\(x_k\)</span>附近展开二阶泰勒展开： <span class="math display">\[
  \hat{f}(x) = f(x_k) + &lt;\nabla f(x_k), x-x_k&gt; + \frac{1}{2}(x-x_k)^T \frac{\partial^2 f(x_k)}{\partial x_k^2} (x-x_k)\\ 
      \approx f(x_k) + &lt;\nabla f(x_k), x-x_k&gt; + \frac{L}{2} ||x-x_k||^2
  \]</span> 对上式将<span class="math inline">\(x-x_k\)</span>看成一个变量进行配方，我们就可以得到 <span class="math display">\[
  \hat{f}(x) = \frac{L}{2} [(x-x_k)^2 + \frac{2}{L}\nabla f(x_k)(x-x_k) ] + f(x_k) \\
      = \frac{L}{2} ||x - (x_k-\frac{1}{L}\nabla f(x_k))||^2 + const
  \]</span> 它的最小值在<span class="math inline">\(x_{k+1}\)</span>处取得 <span class="math display">\[
  x_{k+1} = x_k - \frac{1}{L}\nabla f(x_k)
  \]</span> 代入到原优化问题，我们可以得知原问题就是每一轮梯度下降迭代求解 <span class="math display">\[
  x_{k+1} = \arg \min_x \frac{L}{2} ||x - (x_k-\frac{1}{L}\nabla f(x_k))||^2 +\lambda ||x||_1
  \]</span></p></li>
<li><p>对于上式，我们可以先计算出<span class="math inline">\(z = x_k-\frac{1}{L}\nabla f(x_k)\)</span> ,然后我们就可以求解 <span class="math display">\[
  x_{k+1} = \arg \min_x \frac{L}{2} ||x - z||^2 +\lambda ||x||_1 (*)
  \]</span> 将<span class="math inline">\((*)\)</span>式按照分量展开,即按照<span class="math inline">\(x^i\)</span>(表示<span class="math inline">\(x\)</span>的第i个分量)展开，不存在<span class="math inline">\(\alpha_i^u \alpha_i^v(u\not=v)\)</span>这样的交叉项,也就是说<span class="math inline">\(x\)</span>的各分量之间是互不影响的。也就是说，对于每一个<span class="math inline">\(x\)</span>的一个分量而言<span class="math inline">\((*)\)</span>式就是一个二元一次方程，可以轻松求解argmin。那么，对于每个分量单独求解 <span class="math display">\[
  x_{k+1}^i = \left\{
  \begin{aligned}
    &amp; z^i - \frac{\lambda}{L}, \frac{\lambda}{L}  &lt; z^i \\
    &amp; 0, | z^i| \le \frac{\lambda}{L} \\
    &amp; z^i + \frac{\lambda}{L},z^i &lt; -\frac{\lambda}{L}
  \end{aligned}
  \right.
  \]</span></p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h5 id="稀疏表示">稀疏表示</h5>
<ul>
<li><p>一般来说，稀疏表示指的是矩阵中有很多零元素，且并非整行整列地出现，当然整行整列也行。</p></li>
<li><p>通过稀疏表示，在训练时，我们可以去除不必要的行或列，从而降低学习难度，并且稀疏表示降低了存储开销，文本数据可以线性可分。</p></li>
<li><p>那么，我们就需要探寻一种方式能够将普通稠密表达的样本变成稀疏表示。</p></li>
<li><p>而一种典型的方法就是字典学习：</p>
<ul>
<li><p>字典学习最普通的形式就是，对于一个k个词汇的字典可以用一个矩阵<span class="math inline">\(\mathbf{B} \in R^{d\times k}\)</span> 表示（这里将字典翻译的过程看成是乘以一个权重矩阵）,假设<span class="math inline">\(\alpha_i \in R^k\)</span>是样本<span class="math inline">\(x_i \in R^d\)</span> 的稀疏表示，那么问题可以转化为: <span class="math display">\[
  \min_{\mathbf{B},\alpha_i} \sum_{i=1}^m ||x_i - \mathbf{B}\alpha_i||_2^2 + \lambda \sum_{i=1}^m ||\alpha_i||_1 \ \ (*)
  \]</span> 第一项意为希望<span class="math inline">\(\alpha_i\)</span>可以重构<span class="math inline">\(x_i\)</span> ,第二项表示希望<span class="math inline">\(\alpha_i\)</span>尽量稀疏。</p></li>
<li><p>这个问题求解比LASSO更麻烦一些，需要学习<span class="math inline">\(\mathbf{B},\alpha_i\)</span>两个参数。</p></li>
<li><p>借鉴LASSO，可以采用变量交替优化的策略求解:</p>
<ul>
<li><p>第一步，固定字典<span class="math inline">\(\mathbf{B}\)</span>,将<span class="math inline">\((*)\)</span>式按照分量展开，不存在<span class="math inline">\(\alpha_i^u \alpha_i^v(u\not=v)\)</span>这样的交叉项，就可以参考LASSO的解法求解下式，为每个样本<span class="math inline">\(x_i\)</span>找到响应的<span class="math inline">\(\alpha_i\)</span> : <span class="math display">\[
  \min_{\alpha_i}  ||x_i - \mathbf{B}\alpha_i||_2^2 + \lambda  ||\alpha_i||_1
  \]</span></p></li>
<li><p>第二步，固定<span class="math inline">\(\alpha_i\)</span> 更新<span class="math inline">\(\mathbf{B}\)</span> ,此时的优化问题是 <span class="math display">\[
  \min_{\mathbf{B}} ||\mathbf{X} - \mathbf{B}\mathbf{A}||_F^2
  \]</span> F表示的是矩阵Frobenius范数。一种常用的求解上式的方式是基于逐列更新策略的<span class="math inline">\(KSVD\)</span> 。 <span class="math display">\[
  \min_{\mathbf{B}} ||\mathbf{X} - \mathbf{B}\mathbf{A}||_F^2 = \min_{\mathbf{b_i}} ||\mathbf{X} - \sum_{j=1}^k b_j \alpha^j||_F^2 \\
  =\min_{\mathbf{b_i}} ||(\mathbf{X} - \sum_{j\not=i} b_j \alpha^j) - b_i \alpha^i||_F^2 \\
  =\min_{\mathbf{b_i}} ||\mathbf{E}_i - b_i \alpha^i||_F^2
  \]</span> 在更新字典的第<span class="math inline">\(i\)</span>列的时候，其他各列都是固定的，因此<span class="math inline">\(\mathbf{E}_i = \sum_{j\not=i} b_j \alpha^j\)</span></p></li>
</ul></li>
</ul></li>
</ul>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Chap.12 计算学习理论</title>
    <url>/2021/12/22/%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<h4 id="计算学习理论">计算学习理论</h4>
<h5 id="基础知识">基础知识</h5>
<ul>
<li><p>对于一个数据集<span class="math inline">\(D = (\mathcal{X},\mathcal{Y})\)</span>,假设<span class="math inline">\(\mathcal{X}\)</span>中的所有样本都是从分布<span class="math inline">\(\mathcal{D}\)</span>当中采样出来的且服从独立同分布采样。</p></li>
<li><p><span class="math inline">\(h:\mathcal{X}-&gt;\mathcal{Y}\)</span>,定义泛化误差：</p>
<ul>
<li><p><span class="math display">\[
  E(h;\mathcal{D}) = P_{x&lt;-\mathcal{D}}(h(x)\not = y)
  \]</span></p>
<p>预测结果与label不一致。</p></li>
</ul></li>
<li><p>定义<span class="math inline">\(h\)</span>在<span class="math inline">\(D\)</span>上的经验误差：</p>
<ul>
<li><span class="math display">\[
  \hat{E}(h;D) = \frac{1}{m}\sum_{i=1}^m \mathbb{I}(h(x_i) \not= y_i)
  \]</span></li>
</ul></li>
<li><p>由于<span class="math inline">\(D\)</span>是分布<span class="math inline">\(\mathcal{D}\)</span>的独立同分布采样，因此，<span class="math inline">\(\mathbb{E}[\hat{E}(h;D)] = E(h;\mathcal{D})\)</span>,对于<span class="math inline">\(E(h;\mathcal{D})\)</span>有一个上限<span class="math inline">\(\epsilon\)</span> ,称其为误差参数。</p></li>
<li><p>如果经验误差为0，那么就认为<span class="math inline">\(h\)</span>与<span class="math inline">\(D\)</span>一致，否则不一致</p></li>
</ul>
<span id="more"></span>
<ul>
<li><p>常用不等式：</p>
<ul>
<li><p>Jensen不等式：</p>
<ul>
<li>对任意的凸函数<span class="math inline">\(f(x)\)</span>,有 <span class="math display">\[
  f(\mathbb{E(x)}) \le \mathbb{E}(f(x))
  \]</span></li>
</ul></li>
<li><p>Hoeffding不等式：</p>
<ul>
<li>若<span class="math inline">\(x_1,x_2,...,x_m\)</span>为m个独立随机变量，并且满足<span class="math inline">\(x_i \sub [0,1]\)</span>,则对任意的<span class="math inline">\(\epsilon &gt; 0\)</span>,有 <span class="math display">\[
  P(\frac{1}{m} \sum_{i=1}^m x_i - \frac{1}{m} \sum_{i=1}^m\mathbb{E}(x_i) \ge \epsilon) \le\exp(-2m\epsilon^2) \\
  P(|\frac{1}{m} \sum_{i=1}^m x_i - \frac{1}{m} \sum_{i=1}^m\mathbb{E}(x_i)| \ge \epsilon) \le 2 \exp(-2m\epsilon^2)
  \]</span></li>
</ul></li>
<li><p>McDiarmid不等式：</p>
<ul>
<li>若<span class="math inline">\(x_1,x_2,...,x_m\)</span>为m个独立随机变量，且对任意的<span class="math inline">\(i\sub[1,m]\)</span>,函数f满足 <span class="math display">\[
  \sup_{x_1,..,x_m,x_i^\prime} |f(x_1,..,x_m) - f(x_1,..,x_{i-1},x_i^\prime,x_{i+1},...,x_m)| \le c_i
  \]</span> 那么对于任意的<span class="math inline">\(\epsilon &gt; 0\)</span>,有 <span class="math display">\[
  P(f(x_1,..,x_m)-\mathbb{E}(f(x_1,..,x_m)) \ge \epsilon) \le \exp(\frac{-2\epsilon^2}{\sum_i c_i^2}) \\
  P(|f(x_1,..,x_m)-\mathbb{E}(f(x_1,..,x_m))| \ge \epsilon) \le 2\exp(\frac{-2\epsilon^2}{\sum_i c_i^2}) 
  \]</span></li>
</ul></li>
</ul></li>
</ul>
<h5 id="pac可学习">PAC可学习</h5>
<ul>
<li><p>概率近似正确学习理论（PCA）是计算学习理论中最基本的。</p></li>
<li><p>令<span class="math inline">\(c\)</span>表示一个概念，如果对于任意的<span class="math inline">\((x,y)\)</span>都有<span class="math inline">\(c(x) = y\)</span>成立，那么<span class="math inline">\(c\)</span>就是目标概念，所有目标概念所构成的集合就是概念类<span class="math inline">\(\mathcal{C}\)</span>。</p></li>
<li><p>给定任意一个学习算法<span class="math inline">\(\mathcal{L}\)</span>,它所考虑的所有可能概念的集合为假设空间<span class="math inline">\(\mathcal{H}\)</span>。如果目标概念<span class="math inline">\(c \in \mathcal{H}\)</span>,那么在当前假设空间中存在能把所有示例按照与真实label一致的方式分开，那么，我们称该问题对于学习算法<span class="math inline">\(\mathcal{L}\)</span>是可分的/一致的，反之，如果<span class="math inline">\(c\not\in \mathcal{H}\)</span>,那就不可分/不一致了。</p></li>
<li><p>而一般学习任务的目标就是近似这个目标概念<span class="math inline">\(c\)</span>,由于机器学习过程诸多因素的制约，我们无法希望<span class="math inline">\(h\)</span>可以精确地学到目标概念<span class="math inline">\(c\)</span>。</p></li>
<li><p>如果对于<span class="math inline">\(\epsilon &gt; 0, \delta &lt; 1\)</span>,存在一个学习算法<span class="math inline">\(\mathcal{L}\)</span>,其输出假设<span class="math inline">\(h\in\mathcal{H}\)</span>满足 <span class="math display">\[
  P(E(h)\le \epsilon) \ge 1 - \delta
  \]</span> 那么称该学习算法能够在假设空间中PAC辨识概念类<span class="math inline">\(\mathcal{C}\)</span>,他以<span class="math inline">\(1 - \delta\)</span>的概率学到了目标概念的一个近似（差一个<span class="math inline">\(\epsilon\)</span>）。</p></li>
<li><p>如果从分布<span class="math inline">\(\mathcal{D}\)</span>中采样得到m个样例，<span class="math inline">\(\epsilon &gt; 0, \delta &lt; 1\)</span>,对所有的分布<span class="math inline">\(\mathcal{D}\)</span>,如果存在学习算法<span class="math inline">\(\mathcal{L}\)</span>和多项式表示<span class="math inline">\(poly\)</span>,能够使得任意的<span class="math inline">\(m \ge poly(1/\epsilon, 1/\delta,size(x),size(c))\)</span>,学习算法能够在假设空间中PAC辨识概念类<span class="math inline">\(\mathcal{C}\)</span>,那么称概念类<span class="math inline">\(\mathcal{C}\)</span>对于假设空间<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的。</p>
<ul>
<li>如果满足上述条件的学习算法是多项式时间的，那么称概念类<span class="math inline">\(\mathcal{C}\)</span>是高效PAC可学习的，并且称学习算法<span class="math inline">\(\mathcal{L}\)</span>为概念类<span class="math inline">\(\mathcal{C}\)</span>的PAC学习算法。</li>
<li>满足上述条件的最小的<span class="math inline">\(m\)</span>称为学习算法<span class="math inline">\(\mathcal{L}\)</span>的样本复杂度。</li>
<li>PAC学习将复杂算法的时间复杂度的分析转换为了对于样本复杂度的分析，并且对于机器学习给出了一个抽象的理论框架。</li>
</ul></li>
<li><p>按照<span class="math inline">\(|\mathcal{H}|\)</span>是否有限，也可以将假设空间分为有限假设空间以及无限假设空间。</p>
<ul>
<li><p>有限假设空间都是PAC可学习的，输出<span class="math inline">\(h\)</span>的泛化误差随着样例数目的增多而逐渐收敛到0，证明如下：</p>
<ul>
<li><p><span class="math display">\[
  P(h(x)=y) = 1 - P(h(x)\not=y)\\
            = 1 - E(h) \\
            &lt; 1 - \epsilon
  \]</span></p></li>
<li><p>假设D中包含了从<span class="math inline">\(\mathcal{D}\)</span>分布中采样得到的<span class="math inline">\(m\)</span>个样例，因此，<span class="math inline">\(h\)</span>与D表现一致的概率为： <span class="math display">\[
  P(h(x_i)=y_i,\forall i\in [m]) &lt;(1-\epsilon)^m
  \]</span></p></li>
<li><p><span class="math inline">\(P(h\in \mathcal{H}:E(h)&gt;\epsilon\ and\ \hat{E}(h) =0) &lt; |\mathcal{H}|(1-\epsilon)^m &lt; |\mathcal{H}| e^{-m\epsilon}\)</span> <span class="math inline">\(|\mathcal{H}| e^{-m\epsilon}\le \delta\)</span></p>
<p><span class="math inline">\(m\ge \frac{1}{\epsilon}(\ln|\mathcal{H}| +\ln\frac{1}{\delta})\)</span></p></li>
</ul></li>
</ul></li>
<li><p>经验风险最小化（ERM）原则：</p>
<ul>
<li>对于学习算法的一个输出<span class="math inline">\(h\)</span>,如果满足： <span class="math display">\[
  \hat{E}(h) = \min_{h^\prime\in\mathcal{H}}\hat{E}(h^\prime)
  \]</span> 学习算法<span class="math inline">\(\mathcal{L}\)</span>为满足经验风险最小化（ERM）原则的算法。</li>
</ul></li>
<li><p>如果从分布<span class="math inline">\(\mathcal{D}\)</span>中采样得到m个样例，<span class="math inline">\(\epsilon &gt; 0, \delta &lt; 1\)</span>,对所有的分布<span class="math inline">\(\mathcal{D}\)</span>,如果存在学习算法<span class="math inline">\(\mathcal{L}\)</span>和多项式表示<span class="math inline">\(poly\)</span>,能够使得任意的<span class="math inline">\(m \ge poly(1/\epsilon, 1/\delta,size(x),size(c))\)</span>,且<span class="math inline">\(\mathcal{L}\)</span>的输出满足： <span class="math display">\[
  P(E(h)-\min_{h^\prime\in\mathcal{H}}E(h^\prime) \le \epsilon) \ge 1-\delta
  \]</span> 则称假设空间<span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的。</p></li>
</ul>
<h5 id="vc维">VC维</h5>
<ul>
<li><p>增长函数<span class="math inline">\(\Pi_{\mathcal{H}}(m)\)</span>是假设空间中对m个样例所能赋予label的最大可能结果数。</p></li>
<li><p>对于假设空间中对<span class="math inline">\(D\)</span>中<span class="math inline">\(m\)</span>个样例赋予的没中可能结果，我们都将其称为是对数据集<span class="math inline">\(D\)</span>的一个对分。</p></li>
<li><p>如果假设空间<span class="math inline">\(\mathcal{H}\)</span>能够实现<span class="math inline">\(D\)</span>中的所有对分,即<span class="math inline">\(\Pi_{\mathcal{H}}(m) = 2^m\)</span>,那么就称D能被假设空间<span class="math inline">\(\mathcal{H}\)</span>打散。</p></li>
<li><p>将能够将假设空间<span class="math inline">\(\mathcal{H}\)</span>打散的最大示例集的大小称为<span class="math inline">\(VC\)</span>维</p>
<ul>
<li><p>计算方式，存在大小为d的示例集能够被假设空间<span class="math inline">\(\mathcal{H}\)</span>打散,但是任意的大小为d+1的示例集都不能够被假设空间<span class="math inline">\(\mathcal{H}\)</span>打散,则<span class="math inline">\(VC\)</span>维为d。</p></li>
<li><p><span class="math inline">\(VC\)</span>维为d那么对任意的<span class="math inline">\(m\in\mathbb{N}\)</span>: <span class="math display">\[
  \Pi_{\mathcal{H}}(m) \le \sum_{i=1}^d C_m^i
  \]</span></p></li>
</ul></li>
<li></li>
<li><p>任何<span class="math inline">\(VC\)</span>维有限的假设空间<span class="math inline">\(\mathcal{H}\)</span>都是PAC可学习的。</p></li>
</ul>
<h5 id="rademacher复杂度">Rademacher复杂度</h5>
<h5 id="稳定性">稳定性</h5>
<ul>
<li><p>Rademacher复杂度与VC维都与具体的学习算法无关，对所有的学习算法都是适用的，则可以让我们脱离具体学习算法的本身来学习问题本身。而如果想要分析算法，稳定性是值得关注的。</p></li>
<li><p>稳定性指的是算法在输入发生变化的情况下，输出是否随之发生较大的变化。</p></li>
<li><p>两种示例集变化：</p>
<ul>
<li><span class="math inline">\(D^{i}\)</span></li>
</ul></li>
<li><p>泛化误差与经验误差：</p>
<ul>
<li>泛化误差:<span class="math inline">\(l(\mathcal{L},\mathcal{D})\)</span></li>
<li>经验误差：<span class="math inline">\(\hat{l}(\mathcal{L},\mathcal{D})\)</span></li>
</ul></li>
<li><p>均匀稳定性：</p>
<ul>
<li><p><span class="math display">\[
  |l(\mathcal{L}_D,z) - l(\mathcal{L}_D,z)| \le \beta
  \]</span></p>
<p>则称<span class="math inline">\(\mathcal{L}\)</span>关于损失函数<span class="math inline">\(l\)</span>满足<span class="math inline">\(\beta\)</span>-均匀稳定性。</p></li>
</ul></li>
<li><p>如果算法是ERM并且稳定的，那么假设空间可学习。</p></li>
</ul>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>chap.15 规则学习</title>
    <url>/2021/12/24/%E8%A7%84%E5%88%99%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h4 id="规则学习">规则学习</h4>
<h5 id="基本概念">基本概念</h5>
<ul>
<li><p>规则通常是指语义明确、能描述数据分布隐含的客观规律或领域概念。规则学习就是从训练数据中学习出一组能够对未见实例进行判别的规则。</p></li>
<li><p>规则都形如: <span class="math display">\[
  rule\ head \leftarrow f_1 \land f_2\land ... \land f_L (rule \ body)
  \]</span></p></li>
<li><p>如果一个示例被判别结果不同的多条规则覆盖了，那么我们认为发生了冲突，解决冲突的办法是冲突消解——采用投票、排序（优先级）、元规则（关于规则的规则——ex:冲突时，用长度最小的规则）。</p></li>
</ul>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>chap.10 降维与度量学习</title>
    <url>/2021/12/21/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h4 id="第十章.降维与度量学习">第十章.降维与度量学习</h4>
<h5 id="k近邻学习k-nearest-neighbor">k近邻学习(k-Nearest Neighbor)</h5>
<ul>
<li><p>是一种监督学习方法。</p></li>
<li><p>idea ： 给定样本，基于一种距离度量方案找出与其最接近的k个训练样本，然后基于这k个样本的信息进行预测。</p>
<ul>
<li><p>在分类问题中，对这k个样本采取投票法决定类别。</p></li>
<li><p>对于回归问题，用平均法作为预测结果。</p></li>
<li><p>一个示例图:</p>
<ul>
<li><img src="https://gitee.com/DespairL/typora-drawing-bed/raw/master/202112202057821.png" alt="image-20211220205713716" style="zoom:50%;" /></li>
</ul></li>
</ul></li>
<li><p>是一种懒惰学习算法。懒惰学习是指在训练阶段只把样本进行保存，训练时间的开销几乎为0，在获得测试样本时，再对训练样本进行处理；而与之对应的，如果在训练阶段就对样本进行处理的算法就是“急切学习”。</p>
<p><span id="more"></span></p></li>
<li><p>数学推导：</p>
<ul>
<li><p>给定测试样本<span class="math inline">\(x\)</span>,如果其最近邻样本为<span class="math inline">\(z\)</span>,那么分类错的概率就是<span class="math inline">\(x\)</span>,<span class="math inline">\(z\)</span>分了不同的类 <span class="math display">\[
  P(err) = 1 - \sum_{c\in\mathcal{Y}} P(c|x)P(c|z)\\
  \mathcal{Y}\ is\ the \ class\ set
  \]</span></p></li>
<li><p>对上述式子做一个强假设：样本独立同分布，且<span class="math inline">\(z\)</span>可以在<span class="math inline">\(x\)</span>的任意近的范围内找到。那么，如果令<span class="math inline">\(c^\star = \arg\max_{c\in\mathcal{Y}} P(c|x)\)</span> 即贝叶斯最优分类器的结果，可以推导出： <span class="math display">\[
  P(err) \approx 1 - \sum_{c\in\mathcal{Y}} P(c|x)^2 \\
      \le 1 - P^2(c^\star|x) \\
       = (1 - P(c^\star|x)) (1 + P(c^\star|x)) \\
       \le 2 \times [1 - P(c^\star|x)]
  \]</span> 因此，KNN分类器分错样本的概率比贝叶斯最优分类器的误差的两倍小。</p></li>
</ul></li>
<li><p>Review(贝叶斯最优分类器):</p>
<ul>
<li><p>将loss定义为将<span class="math inline">\(c_i\)</span>样本错分为<span class="math inline">\(c_j\)</span>样本的损失： <span class="math display">\[
  Loss(c_j|x) = \sum_{i=1}^N \lambda_{ji} P(c_i|x)
  \]</span></p></li>
<li><p>为了最小化总体loss，只需要选择能够让条件风险最小的类别<span class="math inline">\(c^\star\)</span>即可，就可以得到一个最优分类器： <span class="math display">\[
  c^\star = \arg\min_{c\in\mathcal{Y}} Loss(c|x) \\
          = \arg\max_{c\in\mathcal{Y}} P(c|x) 
  \]</span> 通俗的解释来说，想要让其他类错分为<span class="math inline">\(c^\star\)</span> 的造成的损失最小，那么就选择在所有类别中样本被划分的后验概率最大的那一个class <span class="math inline">\(c\)</span> 。</p></li>
</ul></li>
<li><p>但是，KNN分类器分错样本的概率比贝叶斯最优分类器的误差的两倍小这个结论能够推导出来的条件是<span class="math inline">\(z\)</span>可以在<span class="math inline">\(x\)</span>的任意近的范围内找到,也就是说在任意样本<span class="math inline">\(x\)</span>的任意<span class="math inline">\(\delta\)</span>范围都有另一个训练样本<span class="math inline">\(z\)</span>,则要求我们的采样密度足够大，这也就是“密采样”。</p></li>
<li><p>如果<span class="math inline">\(delta=0.001\)</span>,对于单个属性，就要求1000个样本分布在归一化属性范围内。如果属性维度为<span class="math inline">\(n\)</span>,那么采样数量就是<span class="math inline">\((1000)^n\)</span> 。在这种高纬度的数据情况下，会出现数据样本稀疏、距离计算困难等诸多障碍，也就是所谓的“维数灾难”。</p></li>
<li><p>要缓解维数灾难的重要方法就是降维——将原始高维子空间降为低维子空间,而要求就是这个低维子空间正是我们的学习任务密切相关的部分。在实际问题中，与问题密切相关的很多时候，恰恰就是一个低维子空间。</p></li>
</ul>
<h5 id="降维方法多维缩放mds">降维方法——多维缩放MDS</h5>
<ul>
<li><p>MDS要求原始空间中样本之间的距离在低维空间中得以保持。</p></li>
<li><p>将该问题建模。假设m个样本在原始空间的距离矩阵为<span class="math inline">\(\mathbf{D} = \{dist_{ij}\} \in R^{m\times m}\)</span>, <span class="math inline">\(dist_{ij}\)</span>为样本<span class="math inline">\(x_i\)</span>到样本<span class="math inline">\(x_j\)</span>的距离。我们的目标是找到低维空间的距离矩阵<span class="math inline">\(\mathbf{Z}\in R^{d^\prime \times m}\)</span> ,并且<span class="math inline">\(||z_j - z_i||_2^2 = dist_{ij} (1)\)</span> 。</p></li>
<li><p>令$ = ^T  = {b_{ij}} = z_i^T z_j $ ,是降维后样本的内积矩阵，由(1)式，我们可以推导得知， <span class="math display">\[
  dist_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^T z_j (2) \\
   = b_{ii} + b_{jj} - 2b_{ij}
  \]</span></p></li>
<li><p>为了方便，我们可以假设降维之后的样本<span class="math inline">\(\mathbf{Z}\)</span>被中心化，即<span class="math inline">\(\sum_{i=1}^m z_i = 0\)</span> 。</p></li>
<li><p>那么，<span class="math inline">\(\sum_{i=1}^m b_{ij} = z_j \times \sum_{i=1}^m z_i = 0\)</span>,<span class="math inline">\(\sum_{j=1}^m b_{ij} = z_i \times \sum_{j=1}^m z_j = 0\)</span>。</p></li>
<li><p>因此，<span class="math inline">\(tr(\mathbf{B})=||z_{i\ or \ j}||^2\)</span>,由(2)式推导得：</p>
<ul>
<li><p><span class="math display">\[
  \sum_{i=1}^m dist_{ij}^2 = \sum_{i=1}^m b_{ii} + m b_{jj} = tr(\mathbf{B}) + mb_{jj} \\
  \sum_{j=1}^m dist_{ij}^2 = \sum_{j=1}^m b_{jj} + m b_{ii} = tr(\mathbf{B}) + mb_{ii} \\
  \sum_{i=1}^m \sum_{j=1}^m dist_{ij}^2 = tr(\mathbf{B}) + \sum_{j=1}^m mb_{jj} = 2mtr(\mathbf{B})
  \]</span></p></li>
<li><p><span class="math display">\[
  dist_{i\cdot }^2  = \frac{1}{m} \sum_{j=1}^m dist_{ij}^2 = \frac{tr(\mathbf{B})}{m}  + b_{ii}\\
  dist_{\cdot j}^2  = \frac{1}{m} \sum_{i=1}^m dist_{ij}^2= \frac{tr(\mathbf{B})}{m}  + b_{jj}\\
  dist_{\cdot \cdot}^2  = \frac{1}{m^2} \sum_{j=1}^m \sum_{i=1}^m dist_{ij}^2 = 2\frac{tr(\mathbf{B})}{m}
  \]</span></p></li>
<li><p><span class="math display">\[
  b_{ij} = \frac{b_{ii} + b_{jj} - dist_{ij}^2}{2} \\
      = \frac{dist_{i\cdot }^2 - \frac{tr(\mathbf{B})}{m} + dist_{\cdot j }^2 - \frac{tr(\mathbf{B})}{m} - dist_{ij}^2 }{2} \\
      = \frac{dist_{i\cdot }^2  + dist_{\cdot j }^2  - dist_{ij}^2 - dist_{\cdot \cdot}^2}{2} 
  \]</span></p>
<p>至此，也就求出<span class="math inline">\(\mathbf{B}\)</span>矩阵了。</p></li>
</ul></li>
<li><p>而<span class="math inline">\(\mathbf{B}\)</span>是<span class="math inline">\(\mathbf{Z}\)</span>的内积矩阵，为了求得<span class="math inline">\(\mathbf{Z}\)</span>,我们需要找到一个矩阵，其内积为<span class="math inline">\(\mathbf{B}\)</span>。</p></li>
<li><p>一个想法是对<span class="math inline">\(\mathbf{B}\)</span>进行特征值分解，<span class="math inline">\(\mathbf{B} = \mathbf{V} \wedge \mathbf{V}^T\)</span> ,$<span class="math inline">\(是一个特征值构成的对角矩阵，\)</span><span class="math inline">\(是一个特征向量矩阵。如果假设有\)</span>d^*<span class="math inline">\(个非零特征值，其构成对角阵\)</span>_*$ ,那么<span class="math inline">\(\mathbf{Z}\)</span>就可以表示为<span class="math inline">\(\wedge_*^{1/2} \mathbf{V}_*^T\)</span> 。</p></li>
<li><p>而一般情况下，不要求降维后的距离严格不变，只要尽可能接近即可。此时，可以任意取<span class="math inline">\(\hat{d}\)</span>个非零特征值，其构成对角阵<span class="math inline">\(\hat{\wedge}_*\)</span> ,那么<span class="math inline">\(\mathbf{Z}\)</span>就可以表示为<span class="math inline">\(\hat{\wedge}_*^{1/2} \hat{\mathbf{V}_*}^T\)</span> 。</p></li>
</ul>
<h5 id="最一般的降维线性降维">最一般的降维——线性降维</h5>
<ul>
<li><p>一般，我们可以直接采取对<span class="math inline">\(\mathbf{X}\)</span>乘以一个变换矩阵<span class="math inline">\(\mathbf{W}\)</span>的形式得到<span class="math inline">\(\mathbf{Z}\)</span>,即<span class="math inline">\(\mathbf{Z} = \mathbf{W}^T \mathbf{X}\)</span></p>
<p>其中<span class="math inline">\(\mathbf{Z}\in R^{d^\prime \times m}\)</span>，<span class="math inline">\(\mathbf{X}\in R^{d \times m}\)</span>,<span class="math inline">\(\mathbf{W}\in R^{d \times d^\prime}\)</span> 。</p></li>
<li><p>其变换的含义在于对于每一个样本<span class="math inline">\(x_i\)</span>，<span class="math inline">\(z_i\)</span>都是<span class="math inline">\(x_i\)</span>与<span class="math inline">\(\mathbf{W}\)</span>中的<span class="math inline">\(d^\prime\)</span>个<span class="math inline">\(d\)</span>维向量内积的结果，即$z_i = ^T x_i $ 。如果基于<span class="math inline">\(z_i\)</span>来重构<span class="math inline">\(x_i\)</span>,那么<span class="math inline">\(x_i = \sum_{j=1}^{d^\prime} z_{ij} w_j\)</span>。原坐标系属性线性组合成了新坐标系中的属性。如果新坐标系是正交的，那么<span class="math inline">\(\mathbf{W}\)</span>就是一个正交变换。</p></li>
<li><p>PCA就是这种思想的典型代表。</p></li>
<li><p>PCA希望通过一个超平面对所有的样本点进行分隔。围绕这个目的，有两种等价的推导方式,在假设样本中心化以及<span class="math inline">\(\mathbf{W}\)</span>就是一个标准正交变换：</p>
<ul>
<li><p>最近重构性：所有的样本点到这个超平面距离都很近。</p>
<ul>
<li><p><span class="math display">\[
  ||\sum_{j=1}^{d^\prime} z_{ij} w_j - x_i||_2^2 \propto -tr(\mathbf{W}^T \mathbf{X}\mathbf{X}^T \mathbf{W})
  \]</span></p></li>
<li><p>优化目标：</p>
<ul>
<li><span class="math display">\[
  \min_{\mathbf{W}} -tr(\mathbf{W}^T \mathbf{X}\mathbf{X}^T \mathbf{W}) \\s.t. \mathbf{W}^T \mathbf{W} = \mathbf{I}
  \]</span></li>
</ul></li>
</ul></li>
<li><p>最大可分性：所有的样本点在这个超平面上的投影都尽可能分开。</p>
<ul>
<li><p>在这个超平面上的投影都尽可能分开-&gt;投影之后的方差最大化</p></li>
<li><p>投影也是一种线性变换，因此，投影后样本$ x_i<span class="math inline">\(变成\)</span>^T x_i<span class="math inline">\(,方差是\)</span>_i ^T x_i x_i^T  = tr(^T ^T )$ 。</p></li>
<li><p>优化目标：</p>
<ul>
<li><span class="math display">\[
  \max_{\mathbf{W}} tr(\mathbf{W}^T \mathbf{X}\mathbf{X}^T \mathbf{W}) \\
  s.t. \mathbf{W}^T \mathbf{W} = \mathbf{I}
  \]</span></li>
</ul></li>
</ul></li>
<li><p>对两个优化问题使用拉格朗日乘子法可以得到: <span class="math display">\[
  \mathbf{X}\mathbf{X}^T \mathbf{W} = \lambda \mathbf{W}
  \]</span></p></li>
<li><p>对协方差矩阵<span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span>进行特征值分解，就可以求得<span class="math inline">\(d\)</span>个特征值，按从大到小顺序取前<span class="math inline">\(d^\prime\)</span>个特征值即可得到PCA的解。</p></li>
<li><p>舍弃这<span class="math inline">\(d-d^\prime\)</span>个特征值，可以在采样相同样本数的情况下提升采样密度，并去除一部分噪声。</p></li>
<li><p><span class="math inline">\(d^\prime\)</span>一般由用户指定，也可以用其他分类器进行交叉验证，或者设定一个重构阈值<span class="math inline">\(t\)</span>,要求选取最小能够满足重构后的特征值大小<span class="math inline">\(\sum_{i=1}^{d^\prime} \lambda_i\)</span>与重构前的特征值大小<span class="math inline">\(\sum_{i=1}^{d} \lambda_i\)</span>的比值大于阈值$t $</p></li>
</ul></li>
</ul>
<h5 id="非线性降维核化线性降维">非线性降维——核化线性降维</h5>
<ul>
<li>上述介绍的PCA是线性降维方法，但是，事实上，我们通常需要一个非线性映射才能找到 恰当的低维嵌入,而非线性降维的常用方法是核化线性降维以及流形学习。这一节先介绍核化线性降维的想法。</li>
</ul>
<h5 id="流形学习">流形学习</h5>
<h5 id="距离度量学习">距离度量学习</h5>
]]></content>
      <tags>
        <tag>AML</tag>
        <tag>Note</tag>
      </tags>
  </entry>
</search>
