<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="manifest" href="/images/manifest.json">
  <meta name="msapplication-config" content="/images/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"despairl.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="YQBlog | HaveFun!">
<meta property="og:url" content="https://despairl.github.io/index.html">
<meta property="og:site_name" content="YQBlog | HaveFun!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yanquan Chen">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://despairl.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>YQBlog | HaveFun!</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">YQBlog | HaveFun!</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/04/05/Pattern_Recognition3.31/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/05/Pattern_Recognition3.31/" class="post-title-link" itemprop="url">2022.3.31 Pattern Recognition Note</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-04-05 23:37:42 / Modified: 23:38:52" itemprop="dateCreated datePublished" datetime="2022-04-05T23:37:42+08:00">2022-04-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-Note/" itemprop="url" rel="index"><span itemprop="name">ML Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="generative-models">Generative models</h3>
<p>估计联合概率 P(x,y) = P(x|y = i) P (y)</p>
<h3 id="discriminative判别-models">Discriminative(判别) models</h3>
<p>直接估计后验概率</p>
<h3 id="discriminant-function">Discriminant function</h3>
<p>直接求一个能够将各类都分开的边界</p>
<h3 id="svm形式化-简化的思路-关键">SVM形式化 简化的思路 关键</h3>
<p>简化 ： 线性、可分、二类 margin 计算 : <span class="math display">\[
设投影到超平面上的投影点为x_{\perp},那么对于任意的一个样本点x，其距离向量为x - x_{\perp}。\\
margin的方向为\frac{w}{||w||},可以设margin大小为r.\\
那么有 x = x_{\perp} + r \frac{w}{||w||}\\
w^Tx + b= w^Tx_{\perp} + b + r \frac{w^Tw}{||w||}\\
f(x) = f(x_{\perp}) + r||w||\\
因为x_{\perp}在超平面上，所以f(x_{\perp})=0\\
所以r = \frac{|w^Tx + b|}{||w||}
\]</span> SVM问题就是最大化margin的最小值 <span class="math display">\[
\arg\max_{w,b} (\min_i (\frac{|w^Tx + b|}{||w||}))
\]</span> 这个式子有四个难点</p>
<ul>
<li><div class="line-block">| 绝对值 -&gt; 简化为 <span class="math inline">\(y_i (w^Tx + b)\)</span></div></li>
<li><p>||||范数</p></li>
<li><p>x / 0 的问题 -&gt; 简化为 <span class="math inline">\(\arg\max_{w,b} \frac{1}{||w||} \min y_i (w^Tx + b)\)</span> why？ <span class="math inline">\(\frac{|w^Tx + b|}{||w||}\)</span>上下同阶,因此对于最优解<span class="math inline">\((w^*,b^*)\)</span>，<span class="math inline">\(a(w^*,b^*)\)</span> 仍为最优解</p></li>
<li><p>的问题 -&gt; 简化为 如果<span class="math inline">\(y_i (w^Tx + b) &gt; 0\)</span>就判定为正类，同时限制<span class="math inline">\(\min y_i (w^Tx + b)\)</span>为1</p></li>
<li><p>那么问题就转换为了<span class="math inline">\(\arg\min_{w,b} \frac{1}{2} w^Tw\ s.t.y_i (w^Tx + b) \ge 1,\forall i\)</span></p></li>
<li><p>利用拉格朗日乘子法、KKT条件求解。</p>
<ul>
<li>对偶形式 : <span class="math display">\[
  \arg\max_{a} \sum_{i=1}^n a_i - \frac{1}{2}  \sum_{i=1}^n \sum_{j=1}^n a_i  a_j y_i  y_j x_i^T x_j \\
  s.t.\ a_i \ge 0 \\
  \sum_{i=1}^n a_iy_i = 0
  \]</span></li>
</ul></li>
</ul>
<h3 id="奇异值分解">奇异值分解</h3>
<p><span class="math display">\[
A = U\Sigma V^T\\
其中， UU^T = VV^T = I,\Sigma除对角线上的元素为奇异值外，其余均0\\
\Sigma = diag(\sigma_1,\sigma_2,...,\sigma_{\min(m,n)}), as A\in m\times n
\]</span></p>
<p>也就是说，我们可以通过奇异值分解来解决矩阵某一维极大，而另外一维较小的情况下的降维且保留矩阵信息的问题。</p>
<h3 id="特征值分解">特征值分解</h3>
<p><span class="math display">\[
A = W \Sigma W^{-1} \\
W由n个特征向量组成，\Sigma除对角线上的元素为特征值外，其余均0
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/04/05/Pattern_Recognition3.24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/05/Pattern_Recognition3.24/" class="post-title-link" itemprop="url">2022.3.24 Pattern Recognition Note</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-04-05 23:37:42 / Modified: 23:38:38" itemprop="dateCreated datePublished" datetime="2022-04-05T23:37:42+08:00">2022-04-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-Note/" itemprop="url" rel="index"><span itemprop="name">ML Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="特征归一化">特征归一化</h3>
<h4 id="每维度归一化">每维度归一化</h4>
<ul>
<li>数值不在一个取值范围，很容易让数值小的一维完全没用</li>
<li>稀疏数据需要特别处理，<strong>0往往还要输成0</strong>。</li>
</ul>
<h4 id="l2l1归一化">l2,l1归一化</h4>
<ul>
<li>希望每一行的样本的范数保持一致</li>
<li>l1归一化适用于非负特征，如果<span class="math inline">\(x_i\)</span>是直方图，经常是最佳的</li>
</ul>
<h4 id="z-scaler">Z-scaler</h4>
<ul>
<li>相信数据是高斯分布的</li>
</ul>
<h4 id="测试集归一化">测试集归一化</h4>
<ul>
<li>测试集<strong>寻找最大值、最小值、均值</strong>，是<strong>绝对绝对错误</strong>的。</li>
<li><strong>除了测试的时候，永远不要使用测试数据</strong></li>
<li>测试集要与训练集用一样的归一化， 要用<strong>训练集的最大值、最小值、均值来归一化</strong>!
<ul>
<li><strong>保存训练集的参数</strong></li>
</ul></li>
<li><strong>交叉验证</strong>的时候也需要注意!</li>
</ul>
<h3 id="fisher-线性判别分析-fld">Fisher 线性判别分析 FLD</h3>
<ul>
<li><p>是LDA中较为著名的</p></li>
<li><p>idea : 在某一维度上做投影， 找到一个threshold来更好地分开不同的类别</p></li>
<li><p>对于二分类问题，求两个类别的均值，然后求投影以后的均值<span class="math inline">\(W^T \mu_1, W^T \mu_2\)</span></p>
<ul>
<li><p>我们需要一个标准来求分开的程度，max<span class="math inline">\((m_2-m_1)^2\)</span> ,这个值会无限大，所以我们要加限制条件<span class="math inline">\(W^TW = 1\)</span></p></li>
<li><p>一味地追求max<span class="math inline">\((m_2-m_1)^2\)</span>是不对的。</p></li>
<li><p><strong>Fisher准则</strong>是在要求<span class="math inline">\(|m_2-m_1|\)</span>尽量大的同时，让两类在投影过后尽量集中，不分散</p>
<ul>
<li><p><span class="math display">\[
  J(W) = \frac{(m_2-m_1)^2}{s_1^2 + s_2^2}
  \]</span></p></li>
<li><p>分散程度的度量是<strong>方差或者散度</strong>.这里用的是<strong>scatter 散度</strong></p></li>
<li><p><span class="math inline">\(s_1^2 + s_2^2\)</span>是<strong>总的类内散度</strong> <span class="math inline">\(s_1 = \sum_{y_i = k} (x_i - \mu_k)(x_i - \mu_k)^T\)</span></p>
<ul>
<li>类间散度<span class="math inline">\(S_B = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T\)</span></li>
</ul></li>
</ul></li>
<li><p>将Fisher准则矩阵化:</p>
<ul>
<li><p><span class="math display">\[
  \max J(w) = \frac{W^T S_B W}{W^T S_W W}
  \]</span></p>
<ul>
<li>广义瑞利商,最优的时候，必须满足 <span class="math inline">\(S_B W = \lambda S_W W\)</span></li>
<li>对于二分类问题，我们知道<span class="math inline">\(S_B W = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T W \propto (\mu_2 - \mu_1)\)</span>
<ul>
<li>可以将比例系数归入<span class="math inline">\(\lambda\)</span>,就可以得到<span class="math inline">\((\mu_2 - \mu_1) = = \lambda S_W W\)</span></li>
<li>而我们要确定的<span class="math inline">\(W\)</span>只是一个方向，要求其<span class="math inline">\(||W||=1\)</span>,因此<strong>乘数都是无关紧要的</strong>。</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>因此，FLD步骤:</p>
<ul>
<li>求解<span class="math inline">\(\mu_2,\mu_1,S_w\)</span>, 计算<span class="math inline">\(W = S_W^{-1} (\mu_2 - \mu_1)\)</span> ,然后<strong>对<span class="math inline">\(W\)</span>进行归一化</strong></li>
</ul></li>
</ul></li>
<li><p><span class="math inline">\(S_W\)</span>不可逆怎么办?</p>
<ul>
<li>数据很少或者维度很高的时候，<span class="math inline">\(S_W\)</span>很可能不可逆,用<strong>广义逆矩阵</strong>代替
<ul>
<li>如果<span class="math inline">\(S_W\)</span>是实对称的，至少是半正定的
<ul>
<li>则有$S_W = EE^T $</li>
</ul></li>
<li>Moore-Penrose 伪逆 pseudoiunverse
<ul>
<li>如果<span class="math inline">\(\lambda_{ii} &gt; 0,def\ \lambda_{ii}^+ = \frac{1}{\lambda_{ii}}\)</span>,否则<span class="math inline">\(\lambda_{ii}^+ = 0\)</span></li>
<li><span class="math inline">\(\Lambda^+ = diag(\lambda_{11}^+,\lambda_{22}^+,...,\lambda_{dd}^+)\)</span></li>
<li>则<span class="math inline">\(S_W\)</span>的伪逆位$S_W^+ = E<sup>+E</sup>T $</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>对于多类的问题</p>
<ul>
<li><p>总散布矩阵<span class="math inline">\(S_T = \sum_x (x-\mu)(x - \mu)^T,S_B = \sum_{i=1}^C N_i (\mu_i - \mu)(\mu_i - \mu)^T, S_T = S_W + S_B\)</span></p></li>
<li><p>需要更多的投影方向，最多能得到C-1的有效投影方向（求C-1个广义特征值）</p></li>
</ul></li>
</ul>
<h5 id="与pca等的区别">与PCA等的区别</h5>
<ul>
<li>PCA在数据为<strong>单个高斯分布</strong>的情况下是<strong>最佳</strong>的，<strong>无监督</strong>，<strong>没有用到样本的label</strong></li>
<li>分类问题中不同类别的分布<strong>不能相同</strong>，<strong>越不同越好</strong>
<ul>
<li>这导致 事实上，<strong>数据不可能是单个高斯分布</strong>，<strong>特征的分布一定要是不一样才好</strong></li>
</ul></li>
<li>对于分类问题需要一个最有利于分类的特征提取方法:
<ul>
<li>FLD是<strong>某些限制条件</strong>下的<strong>最优的线性特征提取</strong>方法</li>
</ul></li>
</ul>
<h4 id="张量">张量</h4>
<ul>
<li>高维数组</li>
<li></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/03/17/Pattern_Recognition3.17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/17/Pattern_Recognition3.17/" class="post-title-link" itemprop="url">2022.3.17 Pattern Recognition Note</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-03-17 14:24:24 / Modified: 14:25:04" itemprop="dateCreated datePublished" datetime="2022-03-17T14:24:24+08:00">2022-03-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-Note/" itemprop="url" rel="index"><span itemprop="name">ML Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="pca">PCA</h4>
<ul>
<li><p>PCA的<strong>数据要求</strong>：数据的<strong>内在维度是低于表面维度</strong>的，<strong>不是互相独立</strong>的。</p></li>
<li><p>PCA是用线性关系来降低维度的。</p></li>
<li><p><strong>零阶表示</strong>(Zero-dimensional representation, 一个PCA的引子) :</p>
<ul>
<li><p>寻找一个固定的constant m，使得 <span class="math display">\[
  J_1(m) = \min_m \sum_{i=1}^n ||x_i - m ||^2 \\
  m = \arg\min_m J_1(m) = \frac{1}{n} \sum_{i=1}^n  x_i
  \]</span></p></li>
<li><p>这样就不用任何维度，最佳表示了<span class="math inline">\(x\)</span></p></li>
</ul></li>
<li><p><strong>中心化</strong>: <span class="math inline">\(x^\prime = x - \overline{x}\)</span></p>
<ul>
<li><p>目的主要在于<strong>去除数据本身的bias</strong></p></li>
<li><p>之后需要选择一个方向，并在该方向上方差最大化。 <span class="math display">\[
  J_2(w) = \max_m \frac{1}{n}\sum_{i=1}^n ||w^T (x_i - \overline{x}) ||^2
  \]</span></p></li>
<li><p>而由于随着w的变化，<span class="math inline">\(J_2(w)\)</span>可能是无穷大或者无穷小</p></li>
<li><p>因此，我们需要加上限制条件 <span class="math inline">\(w^Tw=1\)</span> <span class="math display">\[
  \arg\max_m \frac{1}{n}\sum_{i=1}^n ||w^T (x_i - \overline{x}) ||^2 \\
  s.t. w^Tw = 1
  \]</span></p></li>
<li><p>简化: <span class="math display">\[
  ||w^T (x_i - \overline{x}) ||^2  = w^T \frac{1}{n}\sum_{i=1}^n (x_i - \overline{x}) (x_i - \overline{x})^T w \\
  = w^T Cov(x) w
  \]</span></p></li>
<li><p>用Lagrange乘子法转为无约束的优化问题。 <span class="math display">\[
  Cov(x) w = \lambda w\\
  w^Tw = 1
  \]</span> 也就是说，最优解的<span class="math inline">\(\lambda\)</span>都是<span class="math inline">\(Cov\)</span>矩阵的特征值,<span class="math inline">\(w\)</span>是其所对应的特征向量。</p></li>
<li><p>近似的<span class="math inline">\(x\)</span>表示为<span class="math inline">\(\overline{x} + w_1^T (x- \overline{x}) w_1\)</span> ,<span class="math inline">\(y = w_1^T (x - \overline{x})\)</span>,<span class="math inline">\(b = w_1^T \overline{x}\)</span></p>
<ul>
<li><span class="math inline">\(y\)</span>表示的是降维之后的向量</li>
<li>重建的关系为<span class="math inline">\(\hat{x_i} = \overline{x} + y_i \mathbf{w}_1\)</span></li>
<li>$J_2 <span class="math inline">\(与\)</span>J_1$等价</li>
</ul></li>
<li><p>有了一个投影方向获得其他的投影方向<span class="math inline">\(\mathbf{w_2}\)</span>，我们只需要加一个限制 <span class="math display">\[
  \mathbf{w_2} 垂直于 \mathbf{w_1}
  \]</span></p></li>
<li><p>如果<strong>用上所有的特征向量</strong>，我们重构是<strong>完全精确</strong>的，<span class="math inline">\(W = [w_1,w_2,...,w_n]\)</span>why?</p>
<ul>
<li><p><span class="math display">\[
  x = \overline{x} + WW^T (x - \overline{x}) \\
  WW^T = W^TW = I\\
  x = \overline{x} + (x - \overline{x})
  \]</span></p></li>
<li><p>而很多时候，有些投影方向是噪声，需要扔掉一些最小的那些特征值。</p></li>
<li><p>而我们要求去除之后的能量保持在90%以上,寻找第一个T,使得下式子成立: <span class="math display">\[
  \frac{\lambda_1 + \lambda_2 + ... + \lambda_T}{\lambda_1 + \lambda_2 + ... + \lambda_d} &gt; 0.9
  \]</span></p></li>
</ul></li>
<li></li>
<li><p>扔掉的维度的特征值就是每一维度上的误差。</p>
<ul>
<li><p><span class="math display">\[
  x - \hat{x} = \sum_{j={T+1}}^d w_j^T (x- \overline{x}) w_j = \sum_{j={T+1}}^d e_j \\
  e_j^T e_k = 0\\
  E(||x - \hat{x}||^2) = \sum_{j={T+1}}^d E(||e_j||^2) = \sum_{j={T+1}}^d \lambda_j
  \]</span></p></li>
<li></li>
</ul></li>
<li><p>如果数据比维数多，<span class="math inline">\(Cov(x)\)</span>可逆;反之，只有最大的几个维度是大于0的。</p></li>
</ul></li>
<li><p>PCA可以对任何数据<span class="math inline">\(x\)</span>进行降维。</p></li>
<li><p>以高斯分布为例:</p>
<ul>
<li>先<strong>平移</strong><span class="math inline">\(x- \mu\)</span>、后<strong>旋转</strong><span class="math inline">\(W^T(x- \mu)\)</span>
<ul>
<li>使其期望变为0，方差为<span class="math inline">\(\lambda\)</span>矩阵</li>
</ul></li>
<li>使得新特征的各个维度<strong>各不相关</strong>，对于高斯分布而言，各个维度就是<strong>独立</strong>的了。</li>
<li>进一步，我们可以用白化变换使变化后的高斯分布<span class="math inline">\(y - N(0,I)\)</span>
<ul>
<li>$y = (W <sup>{-})</sup>T (x - ) $</li>
<li>如果为0，直接0(伪逆矩阵)</li>
</ul></li>
</ul></li>
<li><p>优点:</p>
<ul>
<li>减少计算量、存储空间、可能去掉噪声、可能提高精度</li>
<li>对于高斯分布(单峰分布unifodal distribution)、带白噪声(各维度独立、均值为0、噪声幅度有限)非常有用,<strong>对于不是高斯分布</strong>的，PCA之后的结果仍然不是高斯分布，且<strong>各维度不独立</strong>.</li>
<li>实际上，只要<strong>特征值</strong>是<strong>指数递减</strong>的就可以。</li>
</ul></li>
<li><p>减均值</p></li>
<li><p>特征值分解</p></li>
</ul>
<h3 id="评估">评估</h3>
<ul>
<li>真实的函数<span class="math inline">\(F(x),y=F(x)\)</span>是没有误差的,唯一的随机变量是数据集<span class="math inline">\(D\)</span> ,用不同的数据集可以准确训练出不同的<span class="math inline">\(F(x)\)</span>。</li>
<li><span class="math inline">\(E[(f-F)^2](错误,error) = (F-Ef)^2 +E[(f-Ef)^2]\)</span> ,也就是西瓜书的泛化误差的定义，不过还没有考虑噪声。</li>
<li>要降低错误，就是两个方面-&gt;降低方差 or 降低偏差</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/03/16/torch_note1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/16/torch_note1/" class="post-title-link" itemprop="url">Pytorch Note One</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-03-16 19:23:11 / Modified: 19:24:33" itemprop="dateCreated datePublished" datetime="2022-03-16T19:23:11+08:00">2022-03-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/torch-Note/" itemprop="url" rel="index"><span itemprop="name">torch Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="pytorch">Pytorch</h1>
<h2 id="library">Library</h2>
<p>pprint 可以让打印效果更加好看一点.</p>
<p>pytorch 是更为灵活的一个Tensorflow的alternative之一</p>
<h2 id="tensor">Tensor</h2>
<p>我们可以从一个python list出发，构建一个tensor. data type 以及 dimensions 都会自动转换</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [ [0,1], [2,3] ]</span><br><span class="line">tensor_first = torch.tensor(data)</span><br><span class="line">pp.pprint(tensor_first) # result : tensor([ [0,1], [2,3] ])</span><br></pre></td></tr></table></figure>
<p>同样也可以用dtype指定数据类型，常用的为torch.bool torch.float torch.long</p>
<p>或者也可以使用 .float()等方法</p>
<p>还可以利用tensor.FloatTensor, tensor.LongTensor直接创建相应类型的tensor，则在NLP领域中非常有用</p>
<p>torch.tensor默认是float类型的</p>
<p>另外，当然也可以从numpy的array进行转换,利用torch.from_numpy()</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">ndarray = np.array(data)</span><br><span class="line">x_numpy = torch.from_numpy(ndarray)</span><br></pre></td></tr></table></figure>
<p>最后，也可以通过另一个tensor进行初始化tensor</p>
<p>有四种方法， torch.ones_like(), torch.zeros_like(), torch.rand_like() (均匀分布 0-1), torch.randn_like() (正态分布)</p>
<p>要达成相同的效果，可以指定一个shape，利用torch.zeros(), torch.ones(), torch.rand(), torch.randn()</p>
<p>example :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shape = (4, 2, 2)</span><br><span class="line">x_zeros = torch.zeros(shape)</span><br></pre></td></tr></table></figure>
<p>torch.arange(end) : 创建一个 从 0 到 end-1 的tensor</p>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/03/16/torch_note2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/16/torch_note2/" class="post-title-link" itemprop="url">Pytorch Note Two</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-03-16 19:23:11 / Modified: 19:24:58" itemprop="dateCreated datePublished" datetime="2022-03-16T19:23:11+08:00">2022-03-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/torch-Note/" itemprop="url" rel="index"><span itemprop="name">torch Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="pytorch-官方文档">1. Pytorch 官方文档</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">PyTorch documentation — PyTorch 1.9.1 documentation</a></p>
<h3 id="basic">1 .Basic</h3>
<ul>
<li><p>Optimizer 会根据计算好的梯度更新参数。</p>
<p>因此，在调用torch.optim中的方法之前需要进行梯度的计算。</p>
<p>基本的调用过程:</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss.backward() # 计算梯度</span><br><span class="line">optimizer.step() # 更新参数 例如 学习率，权重</span><br></pre></td></tr></table></figure></p>
<p>通常，需要在调用optimizer之前确定数据的存储位置。</p>
<p>这一步由torch.device完成:</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure></p>
<p>而pytorch的梯度会进行累计，因此通常在每一次epoch中都要进行梯度的清零，这一份工作交给了zero_grad:</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Backward and optimize</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></p>
<p>当我们需要对某一层的optimizer进行参数的特化:</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">      &#123;&#x27;params&#x27;: model.base.parameters()&#125;,</span><br><span class="line">      &#123;&#x27;params&#x27;: model.classifier.parameters(), &#x27;lr&#x27;: 1e-3&#125;</span><br><span class="line">            ], lr=1e-2, momentum=0.9)</span><br></pre></td></tr></table></figure></p>
<p>这意味着对于model.base用参数lr=1e-2,对于model.classifier自己用参数'lr': 1e-3。</p>
<p>但是，两个都采用参数momentum=0.9。</p>
<p>Optimizer的几个固有方法</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Optimizer.load_state_dict # 读取参数</span><br><span class="line">Optimizer.state_dict # 返回optimizer的一个参数</span><br><span class="line">Optimizer.step # 进行一步参数的更新</span><br><span class="line">Optimizer.zero_grad # 设定所有tensor的梯度为0</span><br></pre></td></tr></table></figure></p>
<p>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/03/07/%E9%9D%A2%E8%AF%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/07/%E9%9D%A2%E8%AF%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/" class="post-title-link" itemprop="url">面试学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-07 19:03:21" itemprop="dateCreated datePublished" datetime="2022-03-07T19:03:21+08:00">2022-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-22 16:14:10" itemprop="dateModified" datetime="2022-03-22T16:14:10+08:00">2022-03-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Work-Note/" itemprop="url" rel="index"><span itemprop="name">Work Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="算法题">算法题</h5>
<ul>
<li><p>简单的二分查找中需要考虑的<strong>细节问题</strong>:</p>
<ul>
<li><p>如果取mid = (left + right) / 2,当数据量很大的时候，left + right就可能超过int的上界，此时，需要改变公式为mid = left + (right - left) / 2,这样的话，(right - left) / 2 与 left + (right - left) / 2操作都不会超过int的上界(且不用使用longlong,对于c/c++来说)。</p></li>
<li><p>mid不是所求的数，那么，就要去找[left,mid-1],[mid+1，right]中是否有，mid已经找过了。</p></li>
<li><p>该算法的缺陷就在于，如果给的有序数组是[1,2,2,2,3],target是2，那么我们就只能得到index=3</p>
<ul>
<li>我们可以通过在判断num[mid] == target的时候，可以灵活调整left，right来求左侧索引或者右侧索引</li>
</ul></li>
<li><p>尽量用elif 展开不用else</p></li>
<li><p>另外当数据量比较小的时候，<strong>其实O(n)的线性查找会更快</strong>一些。</p></li>
<li><p>找右边界:</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while i &lt;= j:</span><br><span class="line">            m = (i + j) // 2</span><br><span class="line">            if nums[m] &lt;= target: i = m + 1 #只要相等i就会右移直到第一个不是target</span><br><span class="line">            else: j = m - 1</span><br><span class="line">        right = i</span><br></pre></td></tr></table></figure></p>
<p>找左边界:</p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while i &lt;= j:</span><br><span class="line">            m = (i + j) // 2</span><br><span class="line">            if nums[m] &lt; target: i = m + 1</span><br><span class="line">            else: j = m - 1 #只要相等j就会左移直到第一个不是target</span><br><span class="line">        left = j</span><br></pre></td></tr></table></figure></p></li>
<li><p>二分法专门用来解决排好序的数组中的搜索问题。打比方来说有一个缺失了一个元素的递增数组，我们要怎么搜索出来缺失元素? 二分,<strong>num[mid] == mid,说明在数组的左侧，i = mid + 1，反之，在右侧, j = mid + 1。</strong></p></li>
</ul></li>
</ul>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/03/07/ML-review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/07/ML-review/" class="post-title-link" itemprop="url">chap.1/2 ML基础概念 review</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-07 10:57:05" itemprop="dateCreated datePublished" datetime="2022-03-07T10:57:05+08:00">2022-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-22 16:08:48" itemprop="dateModified" datetime="2022-03-22T16:08:48+08:00">2022-03-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-Note/" itemprop="url" rel="index"><span itemprop="name">ML Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="chap1">Chap1</h3>
<ul>
<li>机器学习主要研究的就是从计算机的<strong>数据</strong>当中<strong>产生模型的算法</strong>,亦即<strong>学习算法</strong>。这是一个<strong>归纳</strong>的过程，不同于<strong>基于基本事实推导出结论的演绎</strong>过程。也可以将该学习过程看成是一个<strong>在假设空间中进行搜索</strong>与<strong>训练集fit</strong>的假设的过程,换句话说，对于<strong>每一个特征</strong>，学习过程都是<strong>假设一个值</strong>，组合特征维度形成一个<strong>特征向量</strong>/<strong>评判标准</strong>，看<strong>是否与训练集fit</strong>。</li>
<li>对于不同的学习任务，我们一般将预测对象按照值是否离散，区分<strong>分类(离散值)</strong>、<strong>回归(连续值)</strong>,它们同属于监督学习。</li>
<li>无监督、半监督、监督学习、测试、聚类(无监督)概念不赘述。</li>
<li>归纳偏好(Inductive Bias):指的是<strong>机器学习的算法或者模型</strong>在学习过程中<strong>对于某种类型假设的偏好</strong>。<strong>如果一个机器学习算法没有归纳偏好，那它就没法告诉我们什么是确定有效的</strong>。如果是随机产生，那么，每次训练预测得到的结果都不一样，这显然不是我们想看到的。</li>
<li><strong>Occam剃刀</strong>:如果有多个假设的结果一致，就选最简单的那一个</li>
<li><strong>天下没有免费的午餐(NFL)</strong>:所有学习算法的期望都是一样的。</li>
</ul>
<h3 id="chap2">Chap2</h3>
<ul>
<li><strong>过拟合</strong>:模型的复杂程度超过了数据的复杂程度
<ul>
<li>想要解决比解决欠拟合要复杂地多。</li>
</ul></li>
<li><strong>欠拟合</strong>:模型的复杂程度未超过了数据的复杂程度
<ul>
<li>克服? : 增加训练轮数即可。</li>
</ul></li>
<li>测试集的划分:
<ul>
<li><strong>hold-out 留出法</strong> : <strong>直接将数据集划分为训练集与测试集</strong>。比例把控不当会导致测试或者训练效果不佳。</li>
<li><strong>k-fold Cross Validation 交叉验证法</strong> :将数据集对等地<strong>划分为n折</strong>。 取其中<strong>一折作为测试集</strong>，其余的作为训练集。</li>
<li><strong>Bootstrap Sampling 自助法</strong>: 为了解决<strong>划分测试集导致的训练集复杂度降低</strong>的问题。该方法是<strong>在m个样本的数据集中随机有放回地采样m次</strong>构成新数据集<span class="math inline">\(D^\prime\)</span> 作为训练集，将<span class="math inline">\(D/D^\prime\)</span>作为测试集。在数学上可以证明，大概有36.8%的数据被划入了测试集。</li>
<li>总结: 一般来说，Bootstrap Sampling用于<strong>集成学习或者数据量较小</strong>的情况，另外两种可用于<strong>数据量较大</strong>的情况。</li>
</ul></li>
<li>错误率、精度、查准、查全、PR曲线、平衡点(BER)、F1度量</li>
<li>偏差-方差分解(<strong>bias-varience decompose</strong>):
<ul>
<li>定义泛化误差为<span class="math inline">\(E = bias^2(x) + var(x) + noise^2\)</span></li>
<li><span class="math inline">\(bias(x)\)</span> 描述期望预测与实际结果之间的矛盾，即<strong>模型本身的学习能力</strong></li>
<li><span class="math inline">\(var(x)\)</span> 描述<strong>数据扰动带来的影响</strong></li>
<li><span class="math inline">\(noise^2\)</span> 描述了学习问题泛化误差的下界，表示<strong>问题自身的复杂度</strong></li>
<li>偏差与方差存在冲突，训练前期，<strong>学习能力较差，不足以学习数据的扰动</strong>，就会<strong>由偏差主导泛化误差</strong>；训练后期，<strong>模型能够学习数据中的扰动</strong>，就会由<strong>方差主导泛化误差</strong>；当学习能力强到一定程度，<strong>极小的数据扰动带来的局部性质都被模型学习到了</strong>，就会出现<strong>过拟合</strong>。</li>
</ul></li>
</ul>
<h3 id="chap3">Chap3</h3>
<ul>
<li>线性模型:
<ul>
<li>尝试学习一个<strong>线性组合函数</strong>作为预测函数。</li>
<li>具有<strong>良好的可解释性</strong>，通过权重矩阵<span class="math inline">\(w\)</span>,可以直观地看出哪个属性更为重要。</li>
<li>通过引入<strong>层级结构或者高维结构</strong>就可以完成非线性。</li>
</ul></li>
<li>线性回归 ： 尝试学习一个线性组合函数作为预测函数, 预测标签。
<ul>
<li>采用<strong>最小二乘法（MSE最小化）</strong>的优化方法。</li>
</ul></li>
<li><strong>对数线性</strong>回归：将线性回归预测的<span class="math inline">\(y-&gt;\ln y\)</span></li>
<li><strong>对数几率</strong>回归：用Sigmoid进行预测，实际上是<strong>分类模型</strong>。
<ul>
<li><strong>Sigmoid任意阶可导</strong>，可以<strong>获得近似概率分布</strong>，<strong>有利于求最优解</strong>与<strong>辅助决策</strong>。</li>
<li><strong>不需要直接对数据分布做出假设</strong>，线性回归就会假设数据分布满足线性，但是对数几率回归<strong>对于分类的可能建模</strong>，<strong>数据不敏感</strong>。</li>
<li>用<strong>极大似然法</strong>进行优化处理,<strong>梯度下降</strong>求最优解。</li>
</ul></li>
<li>线性判别分析(LDA):
<ul>
<li><strong>同类</strong>尽可能接近（<strong>协方差小</strong>），<strong>异类</strong>尽可能远离（<strong>距离大</strong>）。</li>
</ul></li>
<li>多分类：
<ul>
<li><strong>将若干个类作为正例，若干个作为反例</strong>。</li>
<li>常见分类策略: <span class="math inline">\(OvO,OvR,MvM\)</span></li>
</ul></li>
<li>类别不平衡:
<ul>
<li>欠采样（<strong>反例</strong>）、过采样（<strong>正例</strong>）、阈值移动（在决策阶段使用<strong>等式转换</strong>目标）
<ul>
<li>通过转换判断<span class="math inline">\(\frac{y}{1-y} &gt; \frac{m^+}{m^-}\)</span>,来判断时候预测为正例。</li>
</ul></li>
<li>改变loss函数</li>
</ul></li>
</ul>
<h3 id="chap4">Chap4</h3>
<ul>
<li>决策树：
<ul>
<li>缓解过拟合:
<ul>
<li>剪枝：预剪枝（<strong>展开子树</strong>是否会带来泛化性能的提升来决定）、后剪枝(先训练出一棵完整的树再考虑)。
<ul>
<li>预剪枝：<strong>时间开销少、容易欠拟合</strong></li>
<li>后剪枝：<strong>时间开销大、不容易欠拟合</strong></li>
</ul></li>
</ul></li>
<li>决策树要实现斜向划分是通过<strong>分段近似</strong>(对每一个区间段<strong>拟合一个线性函数</strong>)实现的。</li>
</ul></li>
</ul>
<h3 id="chap5">Chap5</h3>
<ul>
<li>MP神经元</li>
<li>感知机Perceptron : 由两层神经元组成,输出层为MP神经元，需要激活函数处理。</li>
<li>非线性：多层前馈神经网络(multi-layer feedforwaed neural network)
<ul>
<li>通过BP算法训练</li>
<li><strong>累计BP算法（用所有样本）与标准BP算法(只用一个样本)</strong>的区别类似于<strong>随机梯度下降(只用一个样本)与梯度下降（用所有样本）</strong>之间的区别。</li>
<li>折中：小批量的随机梯度下降/...</li>
</ul></li>
<li>跳出局部最优:
<ul>
<li><strong>模拟退火</strong> : 每一步<strong>以一定概率接受更差</strong>的结果、<strong>随机梯度下降</strong>、<strong>遗传算法</strong></li>
<li>这些算法基本都是启发式的，没有理论保证。</li>
</ul></li>
<li>DNN:
<ul>
<li>进行<strong>深度学习</strong>也可以理解为进行<strong>特征学习</strong>或者<strong>表示学习</strong>。</li>
<li><strong>增加隐藏层数目比增加隐藏层神经元个数往往更有效</strong>。</li>
<li>多隐藏层的神经网络<strong>难以用BP进行训练</strong>，<strong>梯度会发散不收敛</strong>。</li>
<li><strong>无监督逐层训练</strong>是一个有效手段:
<ul>
<li>每次训练一层隐藏节点，将<strong>上一层输出作为输入，本层输出作为下一层输入</strong>。上述过程也被称为“<strong>预训练</strong>”。预训练结束之后，再对整个网络进行<strong>微调(fine-tuning)</strong>。这就相当于<strong>基于局部较优的结果组合寻找全局最优</strong>。</li>
</ul></li>
<li>在<strong>CNN</strong>中采用了<strong>权值共享</strong>的策略，让<strong>一组神经元有相同的连接权值</strong>。</li>
</ul></li>
</ul>
<h3 id="chap6">Chap6</h3>
<ul>
<li>SVM:
<ul>
<li><p>基于训练集，找到一个超平面将不同的样本分开。</p></li>
<li><p>通过将超平面方程中的<span class="math inline">\(x\)</span>替换为<span class="math inline">\(\phi(x)\)</span>就可以实现非线性。</p></li>
<li><p><strong>软间隔</strong>: 为了解决 <strong>难以判断线性可分的结果是不是过拟合</strong>造成的， 提出<strong>软间隔</strong>来缓解这一问题。</p>
<ul>
<li>软间隔将<strong>允许SVM在一些样本上出错</strong></li>
<li>优化目标就变成 在<strong>最大化间隔</strong>的基础上，<strong>最小化出错样本</strong>。</li>
<li>引入的<strong>松弛变量</strong>是为了表述<strong>样本不满足约束</strong>的程度,其实就是每个样本的<strong>loss</strong></li>
<li>引入软间隔之后，仍然超平面仍然<strong>仅与支持向量有关</strong>，<strong>保持了稀疏性</strong>。</li>
</ul></li>
<li><p>如果将软间隔的SVM的<strong>惩罚函数</strong>换为<span class="math inline">\(l_{log}\)</span> ,那么SVM（未经过拓展的情况下）就是一个<strong>二分类的对率回归分类器</strong>。 区别在于，<strong>对率回归可以输出概率</strong>，<strong>SVM不能</strong>；<strong>SVM的解具有稀疏性</strong>，<strong>对率回归依赖于样本</strong>，<strong>训练开销更大</strong>。</p></li>
<li><p>从SVM可以引出正则化问题：</p>
<ul>
<li><p><span class="math display">\[
  \min_f \Omega(f) + C \sum_{i=1}^m l(f(x_i),y_i)
  \]</span></p></li>
<li><p><span class="math inline">\(\Omega(f)\)</span>为<strong>正则化项</strong>，亦称<strong>结构风险</strong>，<span class="math inline">\(C\)</span>为<strong>正则化系数</strong>，<span class="math inline">\(\sum_{i=1}^m l(f(x_i),y_i)\)</span>也被称为<strong>经验风险</strong>。</p>
<ul>
<li>结构风险主要描述的是模型的特质</li>
<li>经验风险描述模型的拟合程度</li>
<li>正则化其实<strong>就是一种惩罚措施</strong>，对不希望出现的结果<strong>添加了一个惩罚项</strong>。从贝叶斯估计的角度来看，正则化给模型<strong>提供了先验概率</strong>。</li>
</ul></li>
<li><p>常用的正则化项为<span class="math inline">\(l_p\)</span>范数</p>
<ul>
<li><span class="math inline">\(l_2\)</span>范数倾向于<strong>权重的取值尽量均衡</strong>，<strong>非零分量尽量稠密</strong>，彼此的距离尽量小；<span class="math inline">\(l_1\)</span>范数倾向于<strong>非零分量尽可能少</strong></li>
</ul></li>
</ul></li>
</ul></li>
<li>表示定理、核方法、KLDA</li>
</ul>
<h3 id="chap7">Chap7</h3>
<ul>
<li><p>贝叶斯判定准则:</p>
<ul>
<li>最小化总体风险，只需在每个样本上选择能<strong>最小化期望损失的标签</strong>即可</li>
<li>由此得到<strong>贝叶斯最优分类器</strong></li>
<li>其优化目标等价于最大化 后验概率<span class="math inline">\(P(c|x)\)</span>。
<ul>
<li>由贝叶斯定理，可以转换为最大化<span class="math inline">\(\frac{P(c)P(x|c)}{P(x)}\)</span></li>
</ul></li>
</ul></li>
<li><p>而<strong>概率模型训练的过程就是参数估计</strong>的过程，一般采用极大似然估计的方法。</p></li>
<li><p>朴素贝叶斯分类器:</p>
<ul>
<li><p>假设所有属性独立产生影响。</p></li>
<li><p>那么对于一个样本，优化目标为</p>
<ul>
<li><span class="math display">\[
  \arg\max_x P(c) \prod_i P(x_i|c)
  \]</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="chap8">Chap8</h3>
<ul>
<li>Bagging :
<ul>
<li>目标在于<strong>减少方差</strong></li>
<li>Bagging使用<strong>Bootstrap随机有放回的抽样来获取数据子集</strong>训练基础学习器。通常<strong>分类</strong>任务使用<strong>投票</strong>的方式集成，而<strong>回归</strong>任务通过<strong>平均</strong>的方式集成。</li>
<li>Bootstrap剩下的36.8%的数据可以用来<strong>包外估计</strong>，<strong>辅助决策树剪枝</strong>，<strong>提前结束神经网络训练</strong>等。</li>
<li>适合用于<strong>决策树、神经网络等容易受到样本扰动</strong>的学习器上。</li>
</ul></li>
<li>Boosting
<ul>
<li>目标在于<strong>减少偏差</strong></li>
<li><strong>通过算法集合将弱学习器转换为强学习器</strong></li>
<li><strong>训练一系列的弱学习器</strong>，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是<strong>利用加权的数据</strong>。在训练的早期<strong>对于错分数据给予较大的权重</strong>。</li>
<li><strong>每一轮的训练集是不变的</strong>，<strong>改变的只是每一个样本的权重</strong>。</li>
<li><strong>样本权重</strong>：<strong>Bagging</strong>使用的是<strong>均匀取样</strong>，每个样本权重相等；<strong>Boosting根据错误率调整样本权重，错误率越大的样本权重越大</strong>。</li>
<li>因此，Boosting与Bagging的区分点在于 <strong>训练集以及样本权重</strong></li>
</ul></li>
<li><strong>串行集成方法</strong>: AdaBoost, 也就是说AdaBoost依次训练弱学习器，
<ul>
<li>学习器之间有强依赖。一般是Boosting。</li>
</ul></li>
<li><strong>并行集成方法</strong>: Random Forest
<ul>
<li>一般是Bagging、随机森林。</li>
</ul></li>
<li><strong>AdaBoost</strong>:
<ul>
<li>通过<strong>线性组合基学习器来最小化指数损失</strong>。</li>
<li>AdaBoost在经过一个时间步<span class="math inline">\(H_t\)</span>之后，<strong>样本分布将进行调整</strong>(<strong>序列采样</strong>)，<strong>使得下一轮基学习器能够纠正上一轮基学习器的错误</strong>。</li>
</ul></li>
<li><strong>随机森林</strong>:
<ul>
<li><strong>决策树作为基学习器</strong>，利用<strong>Bagging集成</strong>,引入了<strong>随机属性选择(只选择一个属性子集，与SGD/GD的区别类似)</strong>。</li>
<li><strong>训练效率、结果更好</strong>。</li>
</ul></li>
<li>集成的权重结合方法:
<ul>
<li>投票、平均、加权投票、加权平均、<strong>学习法（Stacking）</strong>。</li>
</ul></li>
<li><strong>Stacking:</strong>
<ul>
<li>它是一种<strong>“学习法”</strong>的代表，<strong>与Bagging,Boosting不在一条赛道</strong>上。</li>
<li>目标在于<strong>提升预测性能</strong></li>
<li><strong>异质集成,将多个不同类的学习器集合到一起</strong>。</li>
<li>先从<strong>初始数据集中训练出一个初级学习器</strong>，然后将<strong>初级学习器的输出融入特征</strong>作为<strong>次级学习器</strong>的输入。</li>
<li>其缺点在于<strong>次级</strong>学习器<strong>容易过拟合</strong>，因此常常使用<strong>交叉验证法</strong>或者<strong>留一法</strong>。</li>
</ul></li>
<li>多样性:
<ul>
<li>衡量<strong>不同学习器</strong>之间是否<strong>好而不同</strong>。</li>
<li>要增强多样性，可以在<strong>数据、参数、特征等</strong>不同角度进行调整。</li>
</ul></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/02/27/ICSOS%E8%A1%A5%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/27/ICSOS%E8%A1%A5%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">ICS and OS 杂项知识点</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-02-27 18:04:25 / Modified: 18:14:02" itemprop="dateCreated datePublished" datetime="2022-02-27T18:04:25+08:00">2022-02-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learn-ICS-and-OS/" itemprop="url" rel="index"><span itemprop="name">Learn ICS and OS</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="进程与线程">进程与线程</h5>
<ul>
<li>进程是并发执行的程序在执行过程中分配和管理资源的基本单位。
<ul>
<li>各个进程之间<strong>有独立的地址空间</strong>,而<strong>线程没有</strong>。</li>
</ul></li>
<li>线程是进程的一个执行单元,多个线程共享一个进程中的资源。</li>
<li>协程是由程序员自己控制的，主要是用户态执行的单元。</li>
</ul>
<h5 id="cdn">CDN</h5>
<ul>
<li>CDN的全称是Content Delivery Network，即<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/内容分发网络/4034265">内容分发网络</a>。CDN是构建<strong>在现有网络基础之上的智能虚拟网络</strong>，依靠部署在各地的边缘服务器，通过<strong>中心平台的负载均衡、内容分发、调度等功能模块</strong>，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有<strong>内容存储和分发技术</strong>。</li>
<li>主要用于:
<ul>
<li>节省骨干网带宽，减少带宽需求量</li>
<li>提供服务器端加速，解决由于用户访问量大造成的服务器过载问题</li>
<li>降低“通信风暴”的影响，提高网络访问的稳定性</li>
</ul></li>
</ul>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/02/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">推荐系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-27 18:02:29" itemprop="dateCreated datePublished" datetime="2022-02-27T18:02:29+08:00">2022-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-11 20:59:18" itemprop="dateModified" datetime="2022-03-11T20:59:18+08:00">2022-03-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learn-Recommand-System/" itemprop="url" rel="index"><span itemprop="name">Learn Recommand System</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="推荐系统">推荐系统</h5>
<h6 id="常用算法">常用算法</h6>
<ul>
<li>基于知识的推荐系统:
<ul>
<li>通过一定的工具，可能是NLP也可能是其他的，基于相似性生成推荐。</li>
</ul></li>
<li>基于内容的推荐系统:
<ul>
<li>通过用户过去的交互行为，比如最近购买的几个商品的属性相似度来进行推荐。但是有一定的范围限制。</li>
</ul></li>
<li><strong>协同过滤:</strong>
<ul>
<li>基于整个用户群的过去交互,因此会比之前的算法都要精确得多。</li>
<li>最常见的就是基于k阶近邻的接收到测试样本(<strong>一般都是用ANN找近似最近邻</strong>):
<ul>
<li>找出当前用户最近的k个邻居，利用这k+1个构成一个聚类，然后进行排序并推荐。</li>
<li>也可以产生两种变体:基于商品或者是基于用户的推荐方案。</li>
<li>k阶近邻的协同过滤算法是懒惰学习的一种，在接收到测试样本的时候，才能进行训练。</li>
</ul></li>
<li>还有一种方案是基于规则的学习，但是它每次接收到测试样本都要进行重新训练。</li>
<li>另一种是基于分解的的学习方法。</li>
</ul></li>
</ul>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/02/27/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/27/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/" class="post-title-link" itemprop="url">分布式架构</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-27 17:59:00" itemprop="dateCreated datePublished" datetime="2022-02-27T17:59:00+08:00">2022-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-28 23:54:58" itemprop="dateModified" datetime="2022-02-28T23:54:58+08:00">2022-02-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learn-Distributed-architecture/" itemprop="url" rel="index"><span itemprop="name">Learn Distributed architecture</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="分布式架构">分布式架构</h3>
<ul>
<li>基本内容:
<ul>
<li>分布式系统中的计算机不存在主从之分，可以任意分布。</li>
<li>并发访问导致对于并发性的处理至关重要</li>
<li>缺乏全局时钟</li>
<li>常常发生故障</li>
<li>分布式系统的请求会产生三态:成功、失败或者是延时。</li>
</ul></li>
<li><strong>CAP理论</strong>: 一个系统不可能同时满足一致性、可用性、分区容错性这三个性质，最多同时满足两个。
<ul>
<li><strong>一致性</strong>指的是一个系统在<strong>更新</strong>之后，系统的<strong>数据仍然处于一致</strong>的状态，亦即所有的用户都可以读取到最新的值。</li>
<li><strong>可用性</strong>指的是用户的<strong>每一个操作请求都可以在有限的时间内</strong>返回结果</li>
<li><strong>分区容错性</strong>指的是分布式系统<strong>遇到任何网络分区故障</strong>的时候，仍然需要能够<strong>保证对外提供满足一致性的可用性的服务</strong></li>
</ul></li>
<li><strong>BASE理论</strong>:BASE是指既然无法做到强一致性，那么我们可以让每个应用根据自身情况达成最终一致性。
<ul>
<li>这就涉及了基本可用以及弱状态这两个性质:
<ul>
<li><strong>基本可用</strong>指的是分布式系统在出现不可预知故障的时候，允许损失部分可用性,比如响应时间或者功能降级之类的。</li>
<li><strong>弱状态</strong>指的是允许存在达成一致性之前的中间状态。</li>
<li><strong>最终一致性</strong>指的是系统在一定时间的同步之后可以达成一致性。</li>
</ul></li>
</ul></li>
</ul>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yanquan Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Yanquan Chen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/DespairL" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DespairL" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Chen61723827" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Chen61723827" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yanquan Chen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
