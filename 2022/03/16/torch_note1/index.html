<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="manifest" href="/images/manifest.json">
  <meta name="msapplication-config" content="/images/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"despairl.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Pytorch Library pprint 可以让打印效果更加好看一点. pytorch 是更为灵活的一个Tensorflow的alternative之一 Tensor 我们可以从一个python list出发，构建一个tensor. data type 以及 dimensions 都会自动转换 123data &#x3D; [ [0,1], [2,3] ]tensor_first &#x3D; tor">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch Note One">
<meta property="og:url" content="https://despairl.github.io/2022/03/16/torch_note1/index.html">
<meta property="og:site_name" content="YQBlog | HaveFun!">
<meta property="og:description" content="Pytorch Library pprint 可以让打印效果更加好看一点. pytorch 是更为灵活的一个Tensorflow的alternative之一 Tensor 我们可以从一个python list出发，构建一个tensor. data type 以及 dimensions 都会自动转换 123data &#x3D; [ [0,1], [2,3] ]tensor_first &#x3D; tor">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-03-16T11:23:11.517Z">
<meta property="article:modified_time" content="2022-03-16T11:24:33.453Z">
<meta property="article:author" content="Yanquan Chen">
<meta property="article:tag" content="Note">
<meta property="article:tag" content="torch">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://despairl.github.io/2022/03/16/torch_note1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Pytorch Note One | YQBlog | HaveFun!</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">YQBlog | HaveFun!</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://despairl.github.io/2022/03/16/torch_note1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yanquan Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YQBlog | HaveFun!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch Note One
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-03-16 19:23:11 / Modified: 19:24:33" itemprop="dateCreated datePublished" datetime="2022-03-16T19:23:11+08:00">2022-03-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/torch-Note/" itemprop="url" rel="index"><span itemprop="name">torch Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="pytorch">Pytorch</h1>
<h2 id="library">Library</h2>
<p>pprint 可以让打印效果更加好看一点.</p>
<p>pytorch 是更为灵活的一个Tensorflow的alternative之一</p>
<h2 id="tensor">Tensor</h2>
<p>我们可以从一个python list出发，构建一个tensor. data type 以及 dimensions 都会自动转换</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [ [0,1], [2,3] ]</span><br><span class="line">tensor_first = torch.tensor(data)</span><br><span class="line">pp.pprint(tensor_first) # result : tensor([ [0,1], [2,3] ])</span><br></pre></td></tr></table></figure>
<p>同样也可以用dtype指定数据类型，常用的为torch.bool torch.float torch.long</p>
<p>或者也可以使用 .float()等方法</p>
<p>还可以利用tensor.FloatTensor, tensor.LongTensor直接创建相应类型的tensor，则在NLP领域中非常有用</p>
<p>torch.tensor默认是float类型的</p>
<p>另外，当然也可以从numpy的array进行转换,利用torch.from_numpy()</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">ndarray = np.array(data)</span><br><span class="line">x_numpy = torch.from_numpy(ndarray)</span><br></pre></td></tr></table></figure>
<p>最后，也可以通过另一个tensor进行初始化tensor</p>
<p>有四种方法， torch.ones_like(), torch.zeros_like(), torch.rand_like() (均匀分布 0-1), torch.randn_like() (正态分布)</p>
<p>要达成相同的效果，可以指定一个shape，利用torch.zeros(), torch.ones(), torch.rand(), torch.randn()</p>
<p>example :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shape = (4, 2, 2)</span><br><span class="line">x_zeros = torch.zeros(shape)</span><br></pre></td></tr></table></figure>
<p>torch.arange(end) : 创建一个 从 0 到 end-1 的tensor</p>
<span id="more"></span>
<h2 id="tensor-properties">Tensor Properties</h2>
<p>一个tensor x的data type 仍然可以通过 x.dtype 查看 data type</p>
<p>x.shape 查看 shape</p>
<p>x.size 查看 tensor 的dimension,通过x.size(0)可以查看batchsize</p>
<p>tensor 具有 view视图， 当我们改变其中一个状态的时候，会改变另外一个的状态. 常用来进行改变shape</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_view = x.view(2, 3) # x的shape就会变成(2, 3)</span><br><span class="line">x_view = x.view(3, -1) # 单单改变x的维度到三维,-1表示自适应</span><br></pre></td></tr></table></figure>
<p>这里类似的方法还有reshape，但是使用reshape与view有一定的区别，view要求将数据存放在连续内存当中,但也只是模拟的一种可以按序读取的方式，物理内存中的存放顺序并不会被改变</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_reshape = torch.reshape(x,(2,3))</span><br></pre></td></tr></table></figure>
<p>torch.unsqueeze (input, dim, out=None)方法会对torch对象添加一个维度，可以用torch.squeeze进行还原,更为常见的，如果参数为-1，那么就会作用到tensor的每一个维度。如果dim为负，则将会被转化dim+input.dim()+1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(10).reshape(5, 2)</span><br><span class="line">x = x.unsqueeze(1)</span><br><span class="line">x.shape # (5,1,2)</span><br><span class="line">x = x.squeeze()</span><br><span class="line">x.shape # (5,2)</span><br></pre></td></tr></table></figure>
<p>如果想要获取在一个tensor当中所有数据的总数，可以使用torch.numel方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 如果 tensor x : tensor([0,1],[2,3])</span><br><span class="line"># then:</span><br><span class="line">x.numel() </span><br><span class="line"># 4</span><br></pre></td></tr></table></figure>
<p>tensor x 的 device属性可以告知其数据的存储位置是CPU还是GPU,也可以通过x.to(device)进行转移</p>
<h2 id="index">Index</h2>
<p>tensor 的 index 下标以及切片与numpy类似, 对于每一个维度<span class="math inline">\(d_{i}\)</span>,都可以在[]切片中进行表述</p>
<p>但是tensor x还可以通过另一个tensor i进行取样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i = torch.tensor([0, 0, 1, 1])</span><br><span class="line">x[i] # x[i]会取出 x的0号维度两份 1号维度两份组成新的tensor</span><br><span class="line">i = torch.tensor([1, 2])</span><br><span class="line">j = torch.tensor([0])</span><br><span class="line">x[i, j] # x[i, j]会取出 x的1，2号维度上的0号元素</span><br></pre></td></tr></table></figure>
<p>如果要取出tensor中的一个元素可以使用 item()方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.item() # 就会取出标量 scalar</span><br></pre></td></tr></table></figure>
<h2 id="operation">Operation</h2>
<p>tensor的运算也可以像numpy一样进行传播，即对每一个元素都进行运算</p>
<p>如果想使用矩阵乘法，可以使用tensor.matmul(other_tensor)(也可以直接使用@,如 A @ B)以及.T进行Transpose运算</p>
<p>mean以及std运算要求tensor x的数据类型为float,运用x.mean(),x.std()进行运算</p>
<p>concrete与numpy类似的使用torch.cat进行</p>
<p>绝大多数pytorch运算并不是in place的，但是pytorch提供了一个_运算来进行in place的设置</p>
<h2 id="autograd">Autograd</h2>
<p>可以主动调用backward() 方法计算tensor x的gradient,gradient存储在x.grad属性当中</p>
<p>在创建一个tensor y的时候可以通过指定 requires_grad=True 来存储y的gradient</p>
<p>这里需要注意的是x.grad属性会累计所有iteration中计算出来的gradient，在训练过程中我们需要执行zero_grad()来进行一个清理,否则结果会出错</p>
<h2 id="neural-network-module----torch.nn">Neural Network module -- torch.nn</h2>
<p>''' import torch.nn as nn '''</p>
<h3 id="linear-layer">Linear Layer</h3>
<p>线性层 nn.Linear(in, out) 接受两个参数，旨在让维度为(N,*,in)的input转变为维度为(N,*,out)的output</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.ones(2,3,4)</span><br><span class="line">linear = nn.Linear(4, 2)</span><br><span class="line">linear_output = linear(input) # 此时 linear_output的维度变成了(2,3,2)</span><br></pre></td></tr></table></figure>
<p>如果不想让linear layer学习Bias, 可以设置bias为False</p>
<p>linear layer模拟的函数是 Ax+b</p>
<p>其他pytorch中含有的已经config过的网络模型有 nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm1d, nn.BatchNorm2d, nn.Upsample and nn.MaxPool2d</p>
<h3 id="activation-funcrion-layer">Activation Funcrion Layer</h3>
<p>常用的激活函数有 nn.sigmoid() nn.ReLU() nn.LeakyReLU()</p>
<p>这些激活函数保持激活函数shape不变</p>
<h3 id="put-layers-together">Put layers together</h3>
<p>我们可以使用 nn.Sequential()来把几个layer整合到一起</p>
<p>example:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">block = nn.Sequential(</span><br><span class="line">    nn.Linear(4, 2),</span><br><span class="line">    nn.Sigmoid()</span><br><span class="line">    # 这就是一个神经元传递并激活的过程了 先通过linear进行传递参数 再通过Sigmoid激活</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">input = torch.ones(2,3,4)</span><br><span class="line">output = block(input) #shape : (2,3,2)</span><br></pre></td></tr></table></figure>
<h3 id="build-module-on-our-own">build module on our own</h3>
<p>这需要使用到 nn.Module class</p>
<p>定义一个新的module，需要def <strong>init</strong>, 并def forward(x), 其中x是一个tensor</p>
<p>在__init__中可以先调用super class的__init__函数，即使用 super(ModuleName,self).__init__()</p>
<p>如果想要查看各个layer的参数，可以通过 list(model.named_parameteras()) 或者 list(model.parameteras())进行查看</p>
<h2 id="optimization">optimization</h2>
<p>该板块主要涉及的是optimizers 在pytorch中处于torch.optim板块</p>
<p>常用的optimizer为 optim.Adam,optim.SGD,通常以模型参数以及学习率作为hyperparameter</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 单轮训练的样例</span><br><span class="line"># Instantiate the model</span><br><span class="line">model = MultilayerPerceptron(5, 3)</span><br><span class="line"></span><br><span class="line"># Define the optimizer</span><br><span class="line">adam = optim.Adam(model.parameters(), lr=1e-1)</span><br><span class="line"></span><br><span class="line"># Define loss using a predefined loss function</span><br><span class="line">loss_function = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"># Calculate how our model is doing now</span><br><span class="line">y_pred = model(x)</span><br><span class="line">loss_function(y_pred, y).item()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 进行多轮训练时</span><br><span class="line"># Set the number of epoch, which determines the number of training iterations</span><br><span class="line">n_epoch = 10 </span><br><span class="line"></span><br><span class="line">for epoch in range(n_epoch):</span><br><span class="line">  # Set the gradients to 0</span><br><span class="line">  # 这一步是为了反之.grad进行了累加</span><br><span class="line">  adam.zero_grad() </span><br><span class="line"></span><br><span class="line">  # Get the model predictions</span><br><span class="line">  y_pred = model(x)</span><br><span class="line"></span><br><span class="line">  # Get the loss</span><br><span class="line">  loss = loss_function(y_pred, y)</span><br><span class="line"></span><br><span class="line">  # Print stats</span><br><span class="line">  print(f&quot;Epoch &#123;epoch&#125;: traing loss: &#123;loss&#125;&quot;)</span><br><span class="line"></span><br><span class="line">  # Compute the gradients</span><br><span class="line">  loss.backward()</span><br><span class="line"></span><br><span class="line">  # Take a step to optimize the weights</span><br><span class="line">  adam.step()</span><br></pre></td></tr></table></figure>
<h2 id="demo-word-window-classification">Demo : Word Window Classification</h2>
<p>demo.py 中进行一个demo试验</p>
<h4 id="data">Data</h4>
<p>NLP 领域的data通常都是一些.csv或者.txt文件</p>
<h4 id="preprocessing">preprocessing</h4>
<p>通常对于文本我们需要进行一些预处理: + tokenization : 把sentence全部拆分成为word + Lowercasing : 转为小写 + Noise removal : 去掉特殊字符 + Stop words removal : 去掉十分常见的字符</p>
<h4 id="convert-words-to-embeddings">convert words to embeddings</h4>
<p>在这一步中我们遵循的主要步骤就是: + Find the corresponding index i of the word in the embedding table: word-&gt;index. + Index into the embedding table and get the embedding: index-&gt;embedding.</p>
<p>我们建立一个table记录每一个word的index以及word所对应的embedding</p>
<p>通常我们需要先取出所有的vocabulary，并将不再vocabulary中的所有单词分配一个标签 例如"&lt;unk&gt;",并将其加入vocabulary</p>
<p>同时，由于window的定义需要围绕选定词的前后N个单词进行构建，而sentence中最开始的单词与最后面的单词无法形成符合定义的window</p>
<p>因此，我们还需要加入padding，即在sentence开头加入 n个"&lt;pad&gt;"，在末尾加入 n个"&lt;pad&gt;"</p>
<p>定义 in one pass : 一次对于forward()的call</p>
<h4 id="label">label</h4>
<p>我们需要给每一个在vocabulary中的word分配一个index</p>
<p>在这里要建立对应关系，我们可以使用torch.nn.Embedding(length_of_vocabulary,dimension)</p>
<p>想要取出其中某一个word的embedding,代码示例如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">embedding_dim = 5</span><br><span class="line">embeds = nn.Embedding(len(vocabulary), embedding_dim)</span><br><span class="line">print(list(embeds.parameters()))</span><br><span class="line"></span><br><span class="line"># 经过上面这一步之后，每一个word所对应的 embedding就构建好了</span><br><span class="line"># Get the embedding for the word Paris</span><br><span class="line">index = word_to_ix[&quot;paris&quot;]</span><br><span class="line">index_tensor = torch.tensor(index, dtype=torch.long) # dtype 必须是torch.long 这是一个 单独的index组成的一维tensor</span><br><span class="line">paris_embed = embeds(index_tensor)</span><br><span class="line"></span><br><span class="line"># 也可以一次性取出多个</span><br><span class="line"># We can also get multiple embeddings at once</span><br><span class="line">index_paris = word_to_ix[&quot;paris&quot;]</span><br><span class="line">index_ankara = word_to_ix[&quot;ankara&quot;]</span><br><span class="line">indices = [index_paris, index_ankara]</span><br><span class="line">indices_tensor = torch.tensor(indices, dtype=torch.long)</span><br><span class="line">embeddings = embeds(indices_tensor)</span><br></pre></td></tr></table></figure>
<h2 id="batching-sentences">Batching Sentences</h2>
<p>learn how to <strong>structure our data into batches using the torch.util.data.DataLoader class</strong>.</p>
<p>DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn):</p>
<p>shuffle : 控制是否打乱数据</p>
<p>batch_size : the number of examples per batch 每一批次的数据量</p>
<p>If provided, DataLoader passes the batches it prepares to the collate_fn. We can write a custom function(自定义函数) to pass to the collate_fn parameter in order to print stats(统计数据) about our batch or perform extra processing. In our case, we will use the collate_fn to: + Window pad our train sentences. 做window相关的padding预处理 + Convert the words in the training examples to indices. 转变words + Pad the training examples so that all the sentences and labels have the same length. Similarly, we also need to pad the labels. This creates an issue because when calculating the loss, we need to know <strong>the actual number of words</strong> in a given example. We will also keep track of this number in the function we pass to the collate_fn parameter.</p>
<p>Because our version of the collate_fn function will need to access to our word_to_ix dictionary (so that it can turn words into indices), we will make use of the partial function in Python, which passes the parameters we give to the function we pass it.</p>
<h2 id="window----unfold">window -- unfold</h2>
<p>由于我们每一次给model的输入都是one batch, model需要自行给每一个input都创建window</p>
<p>我们可以通过unfold(dimension, size, step)来提前完成这项工作</p>
<h2 id="model">Model</h2>
<p>用到 nn.Module ，在Stanford class中已经有所提及，这里贴出example</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">class WordWindowClassifier(nn.Module):</span><br><span class="line"></span><br><span class="line">  def __init__(self, hyperparameters, vocab_size, pad_ix=0):</span><br><span class="line">    super(WordWindowClassifier, self).__init__()</span><br><span class="line">    </span><br><span class="line">    &quot;&quot;&quot; Instance variables &quot;&quot;&quot;</span><br><span class="line">    self.window_size = hyperparameters[&quot;window_size&quot;]</span><br><span class="line">    self.embed_dim = hyperparameters[&quot;embed_dim&quot;]</span><br><span class="line">    self.hidden_dim = hyperparameters[&quot;hidden_dim&quot;]</span><br><span class="line">    self.freeze_embeddings = hyperparameters[&quot;freeze_embeddings&quot;]</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot; Embedding Layer </span><br><span class="line">    Takes in a tensor containing embedding indices, and returns the </span><br><span class="line">    corresponding embeddings. The output is of dim </span><br><span class="line">    (number_of_indices * embedding_dim).</span><br><span class="line"></span><br><span class="line">    If freeze_embeddings is True, set the embedding layer parameters to be</span><br><span class="line">    non-trainable. This is useful if we only want the parameters other than the</span><br><span class="line">    embeddings parameters to change. </span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)</span><br><span class="line">    if self.freeze_embeddings:</span><br><span class="line">      self.embed_layer.weight.requires_grad = False</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot; Hidden Layer</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    full_window_size = 2 * window_size + 1</span><br><span class="line">    self.hidden_layer = nn.Sequential(</span><br><span class="line">      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), </span><br><span class="line">      nn.Tanh()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot; Output Layer</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.output_layer = nn.Linear(self.hidden_dim, 1)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot; Probabilities </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.probabilities = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">  def forward(self, inputs):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Let B:= batch_size</span><br><span class="line">        L:= window-padded sentence length</span><br><span class="line">        D:= self.embed_dim</span><br><span class="line">        S:= self.window_size</span><br><span class="line">        H:= self.hidden_dim</span><br><span class="line">        </span><br><span class="line">    inputs: a (B, L) tensor of token indices</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    B, L = inputs.size()</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Reshaping.</span><br><span class="line">    Takes in a (B, L) LongTensor</span><br><span class="line">    Outputs a (B, L~, S) LongTensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Fist, get our word windows for each word in our input.</span><br><span class="line">    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)</span><br><span class="line">    _, adjusted_length, _ = token_windows.size()</span><br><span class="line"></span><br><span class="line">    # Good idea to do internal tensor-size sanity checks, at the least in comments!</span><br><span class="line">    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Embedding.</span><br><span class="line">    Takes in a torch.LongTensor of size (B, L~, S) </span><br><span class="line">    Outputs a (B, L~, S, D) FloatTensor.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    embedded_windows = self.embeds(token_windows)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Reshaping.</span><br><span class="line">    Takes in a (B, L~, S, D) FloatTensor.</span><br><span class="line">    Resizes it into a (B, L~, S*D) FloatTensor.</span><br><span class="line">    -1 argument &quot;infers&quot; what the last dimension should be based on leftover axes.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    embedded_windows = embedded_windows.view(B, adjusted_length, -1)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Layer 1.</span><br><span class="line">    Takes in a (B, L~, S*D) FloatTensor.</span><br><span class="line">    Resizes it into a (B, L~, H) FloatTensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    layer_1 = self.hidden_layer(embedded_windows)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Layer 2</span><br><span class="line">    Takes in a (B, L~, H) FloatTensor.</span><br><span class="line">    Resizes it into a (B, L~, 1) FloatTensor.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    output = self.output_layer(layer_1)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Softmax.</span><br><span class="line">    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.</span><br><span class="line">    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    output = self.probabilities(output)</span><br><span class="line">    output = output.view(B, -1)</span><br><span class="line"></span><br><span class="line">    return output</span><br></pre></td></tr></table></figure>
<h2 id="training">training</h2>
<h2 id="prediction">prediction</h2>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Note/" rel="tag"><i class="fa fa-tag"></i> Note</a>
              <a href="/tags/torch/" rel="tag"><i class="fa fa-tag"></i> torch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/16/torch_note2/" rel="prev" title="Pytorch Note Two">
      <i class="fa fa-chevron-left"></i> Pytorch Note Two
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/17/Pattern_Recognition3.17/" rel="next" title="2022.3.17 Pattern Recognition Note">
      2022.3.17 Pattern Recognition Note <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch"><span class="nav-number">1.</span> <span class="nav-text">Pytorch</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#library"><span class="nav-number">1.1.</span> <span class="nav-text">Library</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor"><span class="nav-number">1.2.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor-properties"><span class="nav-number">1.3.</span> <span class="nav-text">Tensor Properties</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#index"><span class="nav-number">1.4.</span> <span class="nav-text">Index</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#operation"><span class="nav-number">1.5.</span> <span class="nav-text">Operation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#autograd"><span class="nav-number">1.6.</span> <span class="nav-text">Autograd</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-network-module----torch.nn"><span class="nav-number">1.7.</span> <span class="nav-text">Neural Network module -- torch.nn</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-layer"><span class="nav-number">1.7.1.</span> <span class="nav-text">Linear Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#activation-funcrion-layer"><span class="nav-number">1.7.2.</span> <span class="nav-text">Activation Funcrion Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#put-layers-together"><span class="nav-number">1.7.3.</span> <span class="nav-text">Put layers together</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#build-module-on-our-own"><span class="nav-number">1.7.4.</span> <span class="nav-text">build module on our own</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization"><span class="nav-number">1.8.</span> <span class="nav-text">optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#demo-word-window-classification"><span class="nav-number">1.9.</span> <span class="nav-text">Demo : Word Window Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#data"><span class="nav-number">1.9.0.1.</span> <span class="nav-text">Data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#preprocessing"><span class="nav-number">1.9.0.2.</span> <span class="nav-text">preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#convert-words-to-embeddings"><span class="nav-number">1.9.0.3.</span> <span class="nav-text">convert words to embeddings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#label"><span class="nav-number">1.9.0.4.</span> <span class="nav-text">label</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batching-sentences"><span class="nav-number">1.10.</span> <span class="nav-text">Batching Sentences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#window----unfold"><span class="nav-number">1.11.</span> <span class="nav-text">window -- unfold</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model"><span class="nav-number">1.12.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training"><span class="nav-number">1.13.</span> <span class="nav-text">training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prediction"><span class="nav-number">1.14.</span> <span class="nav-text">prediction</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yanquan Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Yanquan Chen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/DespairL" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DespairL" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Chen61723827" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Chen61723827" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yanquan Chen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
